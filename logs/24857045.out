Experiment arguments: {'mode': 'train', 'epochs': 500, 'beta': 2.0, 'neg_factor': 2.0, 'device': None, 'hidden_dim': 64, 'latent_dim': 32, 'num_rounds': 5, 'gnn_layers': 1, 'decoder': 'gnn', 'checkpoint': './experiments/gnn_hd64_ld32_nr5_ep500_b2.0_nf2.0/model.pt', 'log_dir': './experiments/gnn_hd64_ld32_nr5_ep500_b2.0_nf2.0/logs', 'fig_dir': './experiments/gnn_hd64_ld32_nr5_ep500_b2.0_nf2.0/figures'}
Loaded 171 training hashes.
Epoch 0001 â”‚ loss=3.3148 â”‚ lr=1.0e-03
Epoch 0005 â”‚ loss=2.9128 â”‚ lr=1.0e-03
Epoch 0010 â”‚ loss=2.5882 â”‚ lr=1.0e-03
Epoch 0015 â”‚ loss=2.3510 â”‚ lr=1.0e-03
Epoch 0020 â”‚ loss=2.2803 â”‚ lr=1.0e-03
Epoch 0025 â”‚ loss=2.2393 â”‚ lr=1.0e-03
Epoch 0030 â”‚ loss=2.2418 â”‚ lr=1.0e-03
Epoch 0035 â”‚ loss=2.2299 â”‚ lr=1.0e-03
Epoch 0040 â”‚ loss=2.2232 â”‚ lr=1.0e-03
Epoch 0045 â”‚ loss=2.2327 â”‚ lr=1.0e-03
Epoch 0050 â”‚ loss=2.2077 â”‚ lr=1.0e-03
Epoch 0055 â”‚ loss=2.2183 â”‚ lr=1.0e-03
Epoch 0060 â”‚ loss=2.1999 â”‚ lr=1.0e-03
Epoch 0065 â”‚ loss=2.2312 â”‚ lr=1.0e-03
Epoch 0070 â”‚ loss=2.2001 â”‚ lr=1.0e-03
Epoch 0075 â”‚ loss=2.2288 â”‚ lr=1.0e-03
Epoch 0080 â”‚ loss=2.1679 â”‚ lr=1.0e-03
Epoch 0085 â”‚ loss=2.2231 â”‚ lr=1.0e-03
Epoch 0090 â”‚ loss=2.1785 â”‚ lr=1.0e-03
Epoch 0095 â”‚ loss=2.1520 â”‚ lr=1.0e-03
Epoch 0100 â”‚ loss=2.1783 â”‚ lr=5.0e-04
Epoch 0105 â”‚ loss=2.1245 â”‚ lr=5.0e-04
Epoch 0110 â”‚ loss=2.1500 â”‚ lr=5.0e-04
Epoch 0115 â”‚ loss=2.1427 â”‚ lr=5.0e-04
Epoch 0120 â”‚ loss=2.1261 â”‚ lr=5.0e-04
Epoch 0125 â”‚ loss=2.1604 â”‚ lr=5.0e-04
Epoch 0130 â”‚ loss=2.1413 â”‚ lr=5.0e-04
Epoch 0135 â”‚ loss=2.1199 â”‚ lr=5.0e-04
Epoch 0140 â”‚ loss=2.1337 â”‚ lr=5.0e-04
Epoch 0145 â”‚ loss=2.1349 â”‚ lr=5.0e-04
Epoch 0150 â”‚ loss=2.1045 â”‚ lr=5.0e-04
Epoch 0155 â”‚ loss=2.1251 â”‚ lr=5.0e-04
Epoch 0160 â”‚ loss=2.0933 â”‚ lr=5.0e-04
Epoch 0165 â”‚ loss=2.1136 â”‚ lr=5.0e-04
Epoch 0170 â”‚ loss=2.1337 â”‚ lr=5.0e-04
Epoch 0175 â”‚ loss=2.1214 â”‚ lr=5.0e-04
Epoch 0180 â”‚ loss=2.0936 â”‚ lr=5.0e-04
Epoch 0185 â”‚ loss=2.0823 â”‚ lr=5.0e-04
Epoch 0190 â”‚ loss=2.1240 â”‚ lr=5.0e-04
Epoch 0195 â”‚ loss=2.0941 â”‚ lr=5.0e-04
Epoch 0200 â”‚ loss=2.0968 â”‚ lr=2.5e-04
Epoch 0205 â”‚ loss=2.0804 â”‚ lr=2.5e-04
Epoch 0210 â”‚ loss=2.0857 â”‚ lr=2.5e-04
Epoch 0215 â”‚ loss=2.0775 â”‚ lr=2.5e-04
Epoch 0220 â”‚ loss=2.1105 â”‚ lr=2.5e-04
Epoch 0225 â”‚ loss=2.0858 â”‚ lr=2.5e-04
Epoch 0230 â”‚ loss=2.0945 â”‚ lr=2.5e-04
Epoch 0235 â”‚ loss=2.0877 â”‚ lr=2.5e-04
Epoch 0240 â”‚ loss=2.0794 â”‚ lr=2.5e-04
Epoch 0245 â”‚ loss=2.0811 â”‚ lr=2.5e-04
Epoch 0250 â”‚ loss=2.0703 â”‚ lr=2.5e-04
Epoch 0255 â”‚ loss=2.0677 â”‚ lr=2.5e-04
Epoch 0260 â”‚ loss=2.1041 â”‚ lr=2.5e-04
Epoch 0265 â”‚ loss=2.0680 â”‚ lr=2.5e-04
Epoch 0270 â”‚ loss=2.0732 â”‚ lr=2.5e-04
Epoch 0275 â”‚ loss=2.0449 â”‚ lr=2.5e-04
Epoch 0280 â”‚ loss=2.0961 â”‚ lr=2.5e-04
Epoch 0285 â”‚ loss=2.0901 â”‚ lr=2.5e-04
Epoch 0290 â”‚ loss=2.0656 â”‚ lr=2.5e-04
Epoch 0295 â”‚ loss=2.1036 â”‚ lr=2.5e-04
Epoch 0300 â”‚ loss=2.1033 â”‚ lr=1.3e-04
Epoch 0305 â”‚ loss=2.0375 â”‚ lr=1.3e-04
Epoch 0310 â”‚ loss=2.1006 â”‚ lr=1.3e-04
Epoch 0315 â”‚ loss=2.1060 â”‚ lr=1.3e-04
Epoch 0320 â”‚ loss=2.0853 â”‚ lr=1.3e-04
Epoch 0325 â”‚ loss=2.0695 â”‚ lr=1.3e-04
Epoch 0330 â”‚ loss=2.0960 â”‚ lr=1.3e-04
Epoch 0335 â”‚ loss=2.0706 â”‚ lr=1.3e-04
Epoch 0340 â”‚ loss=2.0800 â”‚ lr=1.3e-04
Epoch 0345 â”‚ loss=2.0746 â”‚ lr=1.3e-04
Epoch 0350 â”‚ loss=2.0300 â”‚ lr=1.3e-04
Epoch 0355 â”‚ loss=2.0788 â”‚ lr=1.3e-04
Epoch 0360 â”‚ loss=2.0941 â”‚ lr=1.3e-04
Epoch 0365 â”‚ loss=2.0649 â”‚ lr=1.3e-04
Epoch 0370 â”‚ loss=2.0770 â”‚ lr=1.3e-04
Epoch 0375 â”‚ loss=2.0613 â”‚ lr=1.3e-04
Epoch 0380 â”‚ loss=2.0733 â”‚ lr=1.3e-04
Epoch 0385 â”‚ loss=2.0746 â”‚ lr=1.3e-04
Epoch 0390 â”‚ loss=2.0694 â”‚ lr=1.3e-04
Epoch 0395 â”‚ loss=2.0762 â”‚ lr=1.3e-04
Epoch 0400 â”‚ loss=2.0680 â”‚ lr=6.3e-05
Epoch 0405 â”‚ loss=2.0862 â”‚ lr=6.3e-05
Epoch 0410 â”‚ loss=2.0498 â”‚ lr=6.3e-05
Epoch 0415 â”‚ loss=2.0699 â”‚ lr=6.3e-05
Epoch 0420 â”‚ loss=2.0882 â”‚ lr=6.3e-05
Epoch 0425 â”‚ loss=2.0595 â”‚ lr=6.3e-05
Epoch 0430 â”‚ loss=2.0615 â”‚ lr=6.3e-05
Epoch 0435 â”‚ loss=2.0549 â”‚ lr=6.3e-05
Epoch 0440 â”‚ loss=2.0352 â”‚ lr=6.3e-05
Epoch 0445 â”‚ loss=2.0415 â”‚ lr=6.3e-05
Epoch 0450 â”‚ loss=2.0707 â”‚ lr=6.3e-05
Epoch 0455 â”‚ loss=2.0560 â”‚ lr=6.3e-05
Epoch 0460 â”‚ loss=2.0589 â”‚ lr=6.3e-05
Epoch 0465 â”‚ loss=2.0682 â”‚ lr=6.3e-05
Epoch 0470 â”‚ loss=2.0692 â”‚ lr=6.3e-05
Epoch 0475 â”‚ loss=2.0472 â”‚ lr=6.3e-05
Epoch 0480 â”‚ loss=2.0688 â”‚ lr=6.3e-05
Epoch 0485 â”‚ loss=2.0547 â”‚ lr=6.3e-05
Epoch 0490 â”‚ loss=2.0694 â”‚ lr=6.3e-05
Epoch 0495 â”‚ loss=2.0497 â”‚ lr=6.3e-05
Epoch 0500 â”‚ loss=2.0669 â”‚ lr=3.1e-05
âœ…  Model saved to ./experiments/gnn_hd64_ld32_nr5_ep500_b2.0_nf2.0/model.pt
ðŸ“ˆ  Loss curve saved to vgae_loss.png
Novel: 100.00% (Baseline), 100.00% (VGAE)
Unique: 100.00% (Baseline), 100.00% (VGAE)
Novel and Unique: 100.00% (Baseline), 100.00% (VGAE)
Computing node-level statistics...
Computing node-level statistics...
Computing node-level statistics...
