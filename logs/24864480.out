Experiment arguments: {'mode': 'train', 'epochs': 500, 'lr': 0.0005, 'beta': 5, 'neg_factor': 2.0, 'device': 'cuda', 'hidden_dim': 128, 'latent_dim': 64, 'num_enc_MP_rounds': 5, 'dec_layers': 2, 'heads': 4, 'decoder': 'gat', 'checkpoint': './experiments/gat_hd128_ld64_nr5_ep500_nf2.0_dl2/model.pt', 'log_dir': './experiments/gat_hd128_ld64_nr5_ep500_nf2.0_dl2/logs', 'fig_dir': './experiments/gat_hd128_ld64_nr5_ep500_nf2.0_dl2/figures'}
Loaded 171 training hashes.
cuda
Device: cuda
Training VGAE model...
Epoch 0001 │ loss=4.9568 │ recon=1.458 │ kl=5.194 │ β=0.02 │ lr=5.0e-04
Epoch 0002 │ loss=1.6120 │ recon=1.462 │ kl=4.958 │ β=0.04 │ lr=5.0e-04
Epoch 0003 │ loss=1.6936 │ recon=1.428 │ kl=4.388 │ β=0.06 │ lr=5.0e-04
Epoch 0004 │ loss=1.7090 │ recon=1.527 │ kl=4.177 │ β=0.08 │ lr=5.0e-04
Epoch 0005 │ loss=1.6969 │ recon=1.296 │ kl=3.999 │ β=0.10 │ lr=5.0e-04
Epoch 0006 │ loss=1.7074 │ recon=1.453 │ kl=3.608 │ β=0.12 │ lr=5.0e-04
Epoch 0007 │ loss=1.7592 │ recon=1.305 │ kl=3.319 │ β=0.14 │ lr=5.0e-04
Epoch 0008 │ loss=1.8155 │ recon=1.409 │ kl=3.368 │ β=0.16 │ lr=5.0e-04
Epoch 0009 │ loss=1.8449 │ recon=1.460 │ kl=2.776 │ β=0.18 │ lr=5.0e-04
Epoch 0010 │ loss=1.8575 │ recon=1.407 │ kl=2.920 │ β=0.20 │ lr=5.0e-04
Epoch 0011 │ loss=1.8581 │ recon=1.033 │ kl=3.016 │ β=0.22 │ lr=5.0e-04
Epoch 0012 │ loss=1.8104 │ recon=1.209 │ kl=2.764 │ β=0.24 │ lr=5.0e-04
Epoch 0013 │ loss=1.8273 │ recon=1.426 │ kl=2.498 │ β=0.26 │ lr=5.0e-04
Epoch 0014 │ loss=1.8519 │ recon=1.434 │ kl=2.542 │ β=0.28 │ lr=5.0e-04
Epoch 0015 │ loss=1.8722 │ recon=1.262 │ kl=2.343 │ β=0.30 │ lr=5.0e-04
Epoch 0016 │ loss=1.8786 │ recon=1.116 │ kl=2.386 │ β=0.32 │ lr=5.0e-04
Epoch 0017 │ loss=1.9237 │ recon=0.877 │ kl=2.188 │ β=0.34 │ lr=5.0e-04
Epoch 0018 │ loss=1.9493 │ recon=1.109 │ kl=2.249 │ β=0.36 │ lr=5.0e-04
Epoch 0019 │ loss=1.9608 │ recon=1.172 │ kl=2.090 │ β=0.38 │ lr=5.0e-04
Epoch 0020 │ loss=2.0005 │ recon=1.663 │ kl=2.091 │ β=0.40 │ lr=5.0e-04
Epoch 0021 │ loss=2.0298 │ recon=1.226 │ kl=1.886 │ β=0.42 │ lr=5.0e-04
Epoch 0022 │ loss=2.0059 │ recon=0.956 │ kl=2.099 │ β=0.44 │ lr=5.0e-04
Epoch 0023 │ loss=2.0826 │ recon=1.313 │ kl=1.892 │ β=0.46 │ lr=5.0e-04
Epoch 0024 │ loss=2.0741 │ recon=0.666 │ kl=1.883 │ β=0.48 │ lr=5.0e-04
Epoch 0025 │ loss=2.0757 │ recon=1.108 │ kl=1.822 │ β=0.50 │ lr=5.0e-04
Epoch 0026 │ loss=2.1168 │ recon=0.958 │ kl=1.719 │ β=0.52 │ lr=5.0e-04
Epoch 0027 │ loss=2.1230 │ recon=0.965 │ kl=1.685 │ β=0.54 │ lr=5.0e-04
Epoch 0028 │ loss=2.1511 │ recon=1.480 │ kl=1.677 │ β=0.56 │ lr=5.0e-04
Epoch 0029 │ loss=2.1191 │ recon=1.312 │ kl=1.538 │ β=0.58 │ lr=5.0e-04
Epoch 0030 │ loss=2.1750 │ recon=1.389 │ kl=1.351 │ β=0.60 │ lr=5.0e-04
Epoch 0031 │ loss=2.1740 │ recon=1.260 │ kl=1.555 │ β=0.62 │ lr=5.0e-04
Epoch 0032 │ loss=2.1752 │ recon=1.763 │ kl=1.349 │ β=0.64 │ lr=5.0e-04
Epoch 0033 │ loss=2.2509 │ recon=0.904 │ kl=1.543 │ β=0.66 │ lr=5.0e-04
Epoch 0034 │ loss=2.2485 │ recon=1.543 │ kl=1.559 │ β=0.68 │ lr=5.0e-04
Epoch 0035 │ loss=2.2829 │ recon=2.067 │ kl=1.550 │ β=0.70 │ lr=5.0e-04
Epoch 0036 │ loss=2.2184 │ recon=1.345 │ kl=1.393 │ β=0.72 │ lr=5.0e-04
Epoch 0037 │ loss=2.2899 │ recon=1.461 │ kl=1.337 │ β=0.74 │ lr=5.0e-04
Epoch 0038 │ loss=2.2529 │ recon=0.674 │ kl=1.381 │ β=0.76 │ lr=5.0e-04
Epoch 0039 │ loss=2.3259 │ recon=1.898 │ kl=1.270 │ β=0.78 │ lr=5.0e-04
Epoch 0040 │ loss=2.3033 │ recon=1.111 │ kl=1.222 │ β=0.80 │ lr=5.0e-04
Epoch 0041 │ loss=2.3623 │ recon=1.626 │ kl=1.320 │ β=0.82 │ lr=5.0e-04
Epoch 0042 │ loss=2.3286 │ recon=0.938 │ kl=1.129 │ β=0.84 │ lr=5.0e-04
Epoch 0043 │ loss=2.3538 │ recon=1.434 │ kl=1.195 │ β=0.86 │ lr=5.0e-04
Epoch 0044 │ loss=2.3864 │ recon=1.998 │ kl=1.195 │ β=0.88 │ lr=5.0e-04
Epoch 0045 │ loss=2.3853 │ recon=1.643 │ kl=1.106 │ β=0.90 │ lr=5.0e-04
Epoch 0046 │ loss=2.4525 │ recon=1.240 │ kl=1.159 │ β=0.92 │ lr=5.0e-04
Epoch 0047 │ loss=2.4568 │ recon=1.048 │ kl=1.157 │ β=0.94 │ lr=5.0e-04
Epoch 0048 │ loss=2.4734 │ recon=1.542 │ kl=1.188 │ β=0.96 │ lr=5.0e-04
Epoch 0049 │ loss=2.4658 │ recon=0.945 │ kl=1.019 │ β=0.98 │ lr=5.0e-04
Epoch 0050 │ loss=2.4726 │ recon=0.972 │ kl=1.137 │ β=1.00 │ lr=5.0e-04
Epoch 0051 │ loss=2.4798 │ recon=1.622 │ kl=1.203 │ β=1.00 │ lr=5.0e-04
Epoch 0052 │ loss=2.4927 │ recon=0.849 │ kl=1.129 │ β=1.00 │ lr=5.0e-04
Epoch 0053 │ loss=2.4801 │ recon=1.652 │ kl=1.168 │ β=1.00 │ lr=2.5e-04
Epoch 0054 │ loss=2.4684 │ recon=1.104 │ kl=0.986 │ β=1.00 │ lr=2.5e-04
Epoch 0055 │ loss=2.4116 │ recon=1.526 │ kl=1.074 │ β=1.00 │ lr=2.5e-04
Epoch 0056 │ loss=2.4183 │ recon=1.615 │ kl=1.067 │ β=1.00 │ lr=2.5e-04
Epoch 0057 │ loss=2.4956 │ recon=1.190 │ kl=1.139 │ β=1.00 │ lr=2.5e-04
Epoch 0058 │ loss=2.4547 │ recon=0.984 │ kl=1.106 │ β=1.00 │ lr=2.5e-04
Epoch 0059 │ loss=2.4175 │ recon=1.961 │ kl=1.086 │ β=1.00 │ lr=2.5e-04
Epoch 0060 │ loss=2.4414 │ recon=0.928 │ kl=1.077 │ β=1.00 │ lr=2.5e-04
Epoch 0061 │ loss=2.4338 │ recon=1.149 │ kl=1.102 │ β=1.00 │ lr=2.5e-04
Epoch 0062 │ loss=2.4348 │ recon=1.366 │ kl=1.043 │ β=1.00 │ lr=2.5e-04
Epoch 0063 │ loss=2.4565 │ recon=1.599 │ kl=1.066 │ β=1.00 │ lr=2.5e-04
Epoch 0064 │ loss=2.3936 │ recon=1.469 │ kl=1.004 │ β=1.00 │ lr=2.5e-04
Epoch 0065 │ loss=2.4026 │ recon=1.234 │ kl=1.048 │ β=1.00 │ lr=2.5e-04
Epoch 0066 │ loss=2.4363 │ recon=1.572 │ kl=1.048 │ β=1.00 │ lr=2.5e-04
Epoch 0067 │ loss=2.4242 │ recon=1.460 │ kl=1.078 │ β=1.00 │ lr=2.5e-04
Epoch 0068 │ loss=2.3819 │ recon=1.873 │ kl=1.017 │ β=1.00 │ lr=2.5e-04
Epoch 0069 │ loss=2.4297 │ recon=1.567 │ kl=1.000 │ β=1.00 │ lr=2.5e-04
Epoch 0070 │ loss=2.3856 │ recon=1.518 │ kl=1.060 │ β=1.00 │ lr=2.5e-04
Epoch 0071 │ loss=2.4192 │ recon=1.852 │ kl=1.104 │ β=1.00 │ lr=2.5e-04
Epoch 0072 │ loss=2.3574 │ recon=1.510 │ kl=1.114 │ β=1.00 │ lr=2.5e-04
Epoch 0073 │ loss=2.4300 │ recon=0.973 │ kl=0.993 │ β=1.00 │ lr=2.5e-04
Epoch 0074 │ loss=2.3859 │ recon=0.662 │ kl=1.056 │ β=1.00 │ lr=2.5e-04
Epoch 0075 │ loss=2.3666 │ recon=0.889 │ kl=1.056 │ β=1.00 │ lr=2.5e-04
Epoch 0076 │ loss=2.4061 │ recon=0.879 │ kl=1.073 │ β=1.00 │ lr=2.5e-04
Epoch 0077 │ loss=2.3969 │ recon=2.175 │ kl=1.032 │ β=1.00 │ lr=2.5e-04
Epoch 0078 │ loss=2.3999 │ recon=1.432 │ kl=1.078 │ β=1.00 │ lr=2.5e-04
Epoch 0079 │ loss=2.3617 │ recon=1.477 │ kl=1.028 │ β=1.00 │ lr=2.5e-04
Epoch 0080 │ loss=2.3547 │ recon=1.431 │ kl=1.065 │ β=1.00 │ lr=2.5e-04
Epoch 0081 │ loss=2.3716 │ recon=1.256 │ kl=1.066 │ β=1.00 │ lr=2.5e-04
Epoch 0082 │ loss=2.3803 │ recon=0.954 │ kl=1.016 │ β=1.00 │ lr=2.5e-04
Epoch 0083 │ loss=2.3403 │ recon=1.032 │ kl=1.005 │ β=1.00 │ lr=2.5e-04
Epoch 0084 │ loss=2.3947 │ recon=1.303 │ kl=1.061 │ β=1.00 │ lr=2.5e-04
Epoch 0085 │ loss=2.4005 │ recon=1.236 │ kl=1.064 │ β=1.00 │ lr=2.5e-04
Epoch 0086 │ loss=2.3393 │ recon=1.063 │ kl=1.020 │ β=1.00 │ lr=2.5e-04
Epoch 0087 │ loss=2.4133 │ recon=1.500 │ kl=1.101 │ β=1.00 │ lr=2.5e-04
Epoch 0088 │ loss=2.4190 │ recon=1.546 │ kl=1.109 │ β=1.00 │ lr=2.5e-04
Epoch 0089 │ loss=2.3592 │ recon=0.525 │ kl=1.033 │ β=1.00 │ lr=2.5e-04
Epoch 0090 │ loss=2.3437 │ recon=0.735 │ kl=0.999 │ β=1.00 │ lr=2.5e-04
Epoch 0091 │ loss=2.3931 │ recon=1.116 │ kl=1.068 │ β=1.00 │ lr=2.5e-04
Epoch 0092 │ loss=2.3663 │ recon=1.568 │ kl=0.995 │ β=1.00 │ lr=2.5e-04
Epoch 0093 │ loss=2.3651 │ recon=1.143 │ kl=1.017 │ β=1.00 │ lr=2.5e-04
Epoch 0094 │ loss=2.3445 │ recon=1.721 │ kl=1.016 │ β=1.00 │ lr=2.5e-04
Epoch 0095 │ loss=2.3044 │ recon=0.979 │ kl=0.956 │ β=1.00 │ lr=2.5e-04
Epoch 0096 │ loss=2.3428 │ recon=1.273 │ kl=0.999 │ β=1.00 │ lr=2.5e-04
Epoch 0097 │ loss=2.4050 │ recon=1.449 │ kl=1.158 │ β=1.00 │ lr=2.5e-04
Epoch 0098 │ loss=2.3482 │ recon=1.148 │ kl=1.066 │ β=1.00 │ lr=2.5e-04
Epoch 0099 │ loss=2.3678 │ recon=1.638 │ kl=1.035 │ β=1.00 │ lr=2.5e-04
Epoch 0100 │ loss=2.3724 │ recon=0.787 │ kl=0.990 │ β=1.00 │ lr=2.5e-04
Epoch 0101 │ loss=2.3359 │ recon=1.432 │ kl=1.079 │ β=1.00 │ lr=2.5e-04
Epoch 0102 │ loss=2.3992 │ recon=1.129 │ kl=1.060 │ β=1.00 │ lr=2.5e-04
⏹️  Early stopping at epoch 102 (no improvement for 100 epochs).
✅  Model saved to ./experiments/gat_hd128_ld64_nr5_ep500_nf2.0_dl2/model.pt
📈  Loss curve saved to vgae_loss.png
Novel: 100.00% (Baseline), 100.00% (VGAE)
Unique: 100.00% (Baseline), 99.40% (VGAE)
Novel and Unique: 100.00% (Baseline), 99.40% (VGAE)
Computing node-level statistics...
Computing node-level statistics...
Computing node-level statistics...

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 24864480: <graph_generations> in cluster <dcc> Done

Job <graph_generations> was submitted from host <n-62-20-9> by user <s185927> in cluster <dcc> at Fri May  2 16:39:37 2025
Job was executed on host(s) <8*n-62-20-2>, in queue <gpuv100>, as user <s185927> in cluster <dcc> at Fri May  2 16:41:49 2025
</zhome/e3/3/139772> was used as the home directory.
</zhome/e3/3/139772/Desktop/AML/AML/Module_3/AML-Project-3> was used as the working directory.
Started at Fri May  2 16:41:49 2025
Terminated at Fri May  2 16:47:23 2025
Results reported at Fri May  2 16:47:23 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J graph_generations
#BSUB -q gpuv100
#BSUB -n 8
#BSUB -o logs/%J.out
#BSUB -e logs/%J.err
#BSUB -gpu "num=1:mode=exclusive_process"
#BSUB -W 5:00
#BSUB -R "span[hosts=1]"
#BSUB -R "rusage[mem=5GB]"
# end of BSUB options

module load cuda/11.8

source ~/Desktop/AML/aml_new/bin/activate

python -u src/main.py --mode 'train' --epochs 500 --lr 5e-4 --hidden_dim 128 --latent_dim 64 --num_enc_MP_rounds 5 --decoder gat  --neg_factor 2 --dec_layers 2 --heads 4 



------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   328.68 sec.
    Max Memory :                                 685 MB
    Average Memory :                             684.20 MB
    Total Requested Memory :                     40960.00 MB
    Delta Memory :                               40275.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   334 sec.
    Turnaround time :                            466 sec.

The output (if any) is above this job summary.



PS:

Read file <logs/24864480.err> for stderr output of this job.

