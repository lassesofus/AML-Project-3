Experiment arguments: {'mode': 'train', 'epochs': 500, 'lr': 0.0005, 'beta': 5, 'neg_factor': 2.0, 'device': 'cuda', 'hidden_dim': 128, 'latent_dim': 64, 'num_enc_MP_rounds': 5, 'dec_layers': 2, 'heads': 4, 'decoder': 'gat', 'checkpoint': './experiments/gat_hd128_ld64_nr5_ep500_nf2.0_dl2/model.pt', 'log_dir': './experiments/gat_hd128_ld64_nr5_ep500_nf2.0_dl2/logs', 'fig_dir': './experiments/gat_hd128_ld64_nr5_ep500_nf2.0_dl2/figures'}
Loaded 171 training hashes.
cuda
Device: cuda
Training VGAE model...
Epoch 0001 â”‚ loss=4.9568 â”‚ recon=1.458 â”‚ kl=5.194 â”‚ Î²=0.02 â”‚ lr=5.0e-04
Epoch 0002 â”‚ loss=1.6120 â”‚ recon=1.462 â”‚ kl=4.958 â”‚ Î²=0.04 â”‚ lr=5.0e-04
Epoch 0003 â”‚ loss=1.6936 â”‚ recon=1.428 â”‚ kl=4.388 â”‚ Î²=0.06 â”‚ lr=5.0e-04
Epoch 0004 â”‚ loss=1.7090 â”‚ recon=1.527 â”‚ kl=4.177 â”‚ Î²=0.08 â”‚ lr=5.0e-04
Epoch 0005 â”‚ loss=1.6969 â”‚ recon=1.296 â”‚ kl=3.999 â”‚ Î²=0.10 â”‚ lr=5.0e-04
Epoch 0006 â”‚ loss=1.7074 â”‚ recon=1.453 â”‚ kl=3.608 â”‚ Î²=0.12 â”‚ lr=5.0e-04
Epoch 0007 â”‚ loss=1.7592 â”‚ recon=1.305 â”‚ kl=3.319 â”‚ Î²=0.14 â”‚ lr=5.0e-04
Epoch 0008 â”‚ loss=1.8155 â”‚ recon=1.409 â”‚ kl=3.368 â”‚ Î²=0.16 â”‚ lr=5.0e-04
Epoch 0009 â”‚ loss=1.8449 â”‚ recon=1.460 â”‚ kl=2.776 â”‚ Î²=0.18 â”‚ lr=5.0e-04
Epoch 0010 â”‚ loss=1.8575 â”‚ recon=1.407 â”‚ kl=2.920 â”‚ Î²=0.20 â”‚ lr=5.0e-04
Epoch 0011 â”‚ loss=1.8581 â”‚ recon=1.033 â”‚ kl=3.016 â”‚ Î²=0.22 â”‚ lr=5.0e-04
Epoch 0012 â”‚ loss=1.8104 â”‚ recon=1.209 â”‚ kl=2.764 â”‚ Î²=0.24 â”‚ lr=5.0e-04
Epoch 0013 â”‚ loss=1.8273 â”‚ recon=1.426 â”‚ kl=2.498 â”‚ Î²=0.26 â”‚ lr=5.0e-04
Epoch 0014 â”‚ loss=1.8519 â”‚ recon=1.434 â”‚ kl=2.542 â”‚ Î²=0.28 â”‚ lr=5.0e-04
Epoch 0015 â”‚ loss=1.8722 â”‚ recon=1.262 â”‚ kl=2.343 â”‚ Î²=0.30 â”‚ lr=5.0e-04
Epoch 0016 â”‚ loss=1.8786 â”‚ recon=1.116 â”‚ kl=2.386 â”‚ Î²=0.32 â”‚ lr=5.0e-04
Epoch 0017 â”‚ loss=1.9237 â”‚ recon=0.877 â”‚ kl=2.188 â”‚ Î²=0.34 â”‚ lr=5.0e-04
Epoch 0018 â”‚ loss=1.9493 â”‚ recon=1.109 â”‚ kl=2.249 â”‚ Î²=0.36 â”‚ lr=5.0e-04
Epoch 0019 â”‚ loss=1.9608 â”‚ recon=1.172 â”‚ kl=2.090 â”‚ Î²=0.38 â”‚ lr=5.0e-04
Epoch 0020 â”‚ loss=2.0005 â”‚ recon=1.663 â”‚ kl=2.091 â”‚ Î²=0.40 â”‚ lr=5.0e-04
Epoch 0021 â”‚ loss=2.0298 â”‚ recon=1.226 â”‚ kl=1.886 â”‚ Î²=0.42 â”‚ lr=5.0e-04
Epoch 0022 â”‚ loss=2.0059 â”‚ recon=0.956 â”‚ kl=2.099 â”‚ Î²=0.44 â”‚ lr=5.0e-04
Epoch 0023 â”‚ loss=2.0826 â”‚ recon=1.313 â”‚ kl=1.892 â”‚ Î²=0.46 â”‚ lr=5.0e-04
Epoch 0024 â”‚ loss=2.0741 â”‚ recon=0.666 â”‚ kl=1.883 â”‚ Î²=0.48 â”‚ lr=5.0e-04
Epoch 0025 â”‚ loss=2.0757 â”‚ recon=1.108 â”‚ kl=1.822 â”‚ Î²=0.50 â”‚ lr=5.0e-04
Epoch 0026 â”‚ loss=2.1168 â”‚ recon=0.958 â”‚ kl=1.719 â”‚ Î²=0.52 â”‚ lr=5.0e-04
Epoch 0027 â”‚ loss=2.1230 â”‚ recon=0.965 â”‚ kl=1.685 â”‚ Î²=0.54 â”‚ lr=5.0e-04
Epoch 0028 â”‚ loss=2.1511 â”‚ recon=1.480 â”‚ kl=1.677 â”‚ Î²=0.56 â”‚ lr=5.0e-04
Epoch 0029 â”‚ loss=2.1191 â”‚ recon=1.312 â”‚ kl=1.538 â”‚ Î²=0.58 â”‚ lr=5.0e-04
Epoch 0030 â”‚ loss=2.1750 â”‚ recon=1.389 â”‚ kl=1.351 â”‚ Î²=0.60 â”‚ lr=5.0e-04
Epoch 0031 â”‚ loss=2.1740 â”‚ recon=1.260 â”‚ kl=1.555 â”‚ Î²=0.62 â”‚ lr=5.0e-04
Epoch 0032 â”‚ loss=2.1752 â”‚ recon=1.763 â”‚ kl=1.349 â”‚ Î²=0.64 â”‚ lr=5.0e-04
Epoch 0033 â”‚ loss=2.2509 â”‚ recon=0.904 â”‚ kl=1.543 â”‚ Î²=0.66 â”‚ lr=5.0e-04
Epoch 0034 â”‚ loss=2.2485 â”‚ recon=1.543 â”‚ kl=1.559 â”‚ Î²=0.68 â”‚ lr=5.0e-04
Epoch 0035 â”‚ loss=2.2829 â”‚ recon=2.067 â”‚ kl=1.550 â”‚ Î²=0.70 â”‚ lr=5.0e-04
Epoch 0036 â”‚ loss=2.2184 â”‚ recon=1.345 â”‚ kl=1.393 â”‚ Î²=0.72 â”‚ lr=5.0e-04
Epoch 0037 â”‚ loss=2.2899 â”‚ recon=1.461 â”‚ kl=1.337 â”‚ Î²=0.74 â”‚ lr=5.0e-04
Epoch 0038 â”‚ loss=2.2529 â”‚ recon=0.674 â”‚ kl=1.381 â”‚ Î²=0.76 â”‚ lr=5.0e-04
Epoch 0039 â”‚ loss=2.3259 â”‚ recon=1.898 â”‚ kl=1.270 â”‚ Î²=0.78 â”‚ lr=5.0e-04
Epoch 0040 â”‚ loss=2.3033 â”‚ recon=1.111 â”‚ kl=1.222 â”‚ Î²=0.80 â”‚ lr=5.0e-04
Epoch 0041 â”‚ loss=2.3623 â”‚ recon=1.626 â”‚ kl=1.320 â”‚ Î²=0.82 â”‚ lr=5.0e-04
Epoch 0042 â”‚ loss=2.3286 â”‚ recon=0.938 â”‚ kl=1.129 â”‚ Î²=0.84 â”‚ lr=5.0e-04
Epoch 0043 â”‚ loss=2.3538 â”‚ recon=1.434 â”‚ kl=1.195 â”‚ Î²=0.86 â”‚ lr=5.0e-04
Epoch 0044 â”‚ loss=2.3864 â”‚ recon=1.998 â”‚ kl=1.195 â”‚ Î²=0.88 â”‚ lr=5.0e-04
Epoch 0045 â”‚ loss=2.3853 â”‚ recon=1.643 â”‚ kl=1.106 â”‚ Î²=0.90 â”‚ lr=5.0e-04
Epoch 0046 â”‚ loss=2.4525 â”‚ recon=1.240 â”‚ kl=1.159 â”‚ Î²=0.92 â”‚ lr=5.0e-04
Epoch 0047 â”‚ loss=2.4568 â”‚ recon=1.048 â”‚ kl=1.157 â”‚ Î²=0.94 â”‚ lr=5.0e-04
Epoch 0048 â”‚ loss=2.4734 â”‚ recon=1.542 â”‚ kl=1.188 â”‚ Î²=0.96 â”‚ lr=5.0e-04
Epoch 0049 â”‚ loss=2.4658 â”‚ recon=0.945 â”‚ kl=1.019 â”‚ Î²=0.98 â”‚ lr=5.0e-04
Epoch 0050 â”‚ loss=2.4726 â”‚ recon=0.972 â”‚ kl=1.137 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0051 â”‚ loss=2.4798 â”‚ recon=1.622 â”‚ kl=1.203 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0052 â”‚ loss=2.4927 â”‚ recon=0.849 â”‚ kl=1.129 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0053 â”‚ loss=2.4801 â”‚ recon=1.652 â”‚ kl=1.168 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0054 â”‚ loss=2.4684 â”‚ recon=1.104 â”‚ kl=0.986 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0055 â”‚ loss=2.4116 â”‚ recon=1.526 â”‚ kl=1.074 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0056 â”‚ loss=2.4183 â”‚ recon=1.615 â”‚ kl=1.067 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0057 â”‚ loss=2.4956 â”‚ recon=1.190 â”‚ kl=1.139 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0058 â”‚ loss=2.4547 â”‚ recon=0.984 â”‚ kl=1.106 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0059 â”‚ loss=2.4175 â”‚ recon=1.961 â”‚ kl=1.086 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0060 â”‚ loss=2.4414 â”‚ recon=0.928 â”‚ kl=1.077 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0061 â”‚ loss=2.4338 â”‚ recon=1.149 â”‚ kl=1.102 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0062 â”‚ loss=2.4348 â”‚ recon=1.366 â”‚ kl=1.043 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0063 â”‚ loss=2.4565 â”‚ recon=1.599 â”‚ kl=1.066 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0064 â”‚ loss=2.3936 â”‚ recon=1.469 â”‚ kl=1.004 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0065 â”‚ loss=2.4026 â”‚ recon=1.234 â”‚ kl=1.048 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0066 â”‚ loss=2.4363 â”‚ recon=1.572 â”‚ kl=1.048 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0067 â”‚ loss=2.4242 â”‚ recon=1.460 â”‚ kl=1.078 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0068 â”‚ loss=2.3819 â”‚ recon=1.873 â”‚ kl=1.017 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0069 â”‚ loss=2.4297 â”‚ recon=1.567 â”‚ kl=1.000 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0070 â”‚ loss=2.3856 â”‚ recon=1.518 â”‚ kl=1.060 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0071 â”‚ loss=2.4192 â”‚ recon=1.852 â”‚ kl=1.104 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0072 â”‚ loss=2.3574 â”‚ recon=1.510 â”‚ kl=1.114 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0073 â”‚ loss=2.4300 â”‚ recon=0.973 â”‚ kl=0.993 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0074 â”‚ loss=2.3859 â”‚ recon=0.662 â”‚ kl=1.056 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0075 â”‚ loss=2.3666 â”‚ recon=0.889 â”‚ kl=1.056 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0076 â”‚ loss=2.4061 â”‚ recon=0.879 â”‚ kl=1.073 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0077 â”‚ loss=2.3969 â”‚ recon=2.175 â”‚ kl=1.032 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0078 â”‚ loss=2.3999 â”‚ recon=1.432 â”‚ kl=1.078 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0079 â”‚ loss=2.3617 â”‚ recon=1.477 â”‚ kl=1.028 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0080 â”‚ loss=2.3547 â”‚ recon=1.431 â”‚ kl=1.065 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0081 â”‚ loss=2.3716 â”‚ recon=1.256 â”‚ kl=1.066 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0082 â”‚ loss=2.3803 â”‚ recon=0.954 â”‚ kl=1.016 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0083 â”‚ loss=2.3403 â”‚ recon=1.032 â”‚ kl=1.005 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0084 â”‚ loss=2.3947 â”‚ recon=1.303 â”‚ kl=1.061 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0085 â”‚ loss=2.4005 â”‚ recon=1.236 â”‚ kl=1.064 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0086 â”‚ loss=2.3393 â”‚ recon=1.063 â”‚ kl=1.020 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0087 â”‚ loss=2.4133 â”‚ recon=1.500 â”‚ kl=1.101 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0088 â”‚ loss=2.4190 â”‚ recon=1.546 â”‚ kl=1.109 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0089 â”‚ loss=2.3592 â”‚ recon=0.525 â”‚ kl=1.033 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0090 â”‚ loss=2.3437 â”‚ recon=0.735 â”‚ kl=0.999 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0091 â”‚ loss=2.3931 â”‚ recon=1.116 â”‚ kl=1.068 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0092 â”‚ loss=2.3663 â”‚ recon=1.568 â”‚ kl=0.995 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0093 â”‚ loss=2.3651 â”‚ recon=1.143 â”‚ kl=1.017 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0094 â”‚ loss=2.3445 â”‚ recon=1.721 â”‚ kl=1.016 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0095 â”‚ loss=2.3044 â”‚ recon=0.979 â”‚ kl=0.956 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0096 â”‚ loss=2.3428 â”‚ recon=1.273 â”‚ kl=0.999 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0097 â”‚ loss=2.4050 â”‚ recon=1.449 â”‚ kl=1.158 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0098 â”‚ loss=2.3482 â”‚ recon=1.148 â”‚ kl=1.066 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0099 â”‚ loss=2.3678 â”‚ recon=1.638 â”‚ kl=1.035 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0100 â”‚ loss=2.3724 â”‚ recon=0.787 â”‚ kl=0.990 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0101 â”‚ loss=2.3359 â”‚ recon=1.432 â”‚ kl=1.079 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0102 â”‚ loss=2.3992 â”‚ recon=1.129 â”‚ kl=1.060 â”‚ Î²=1.00 â”‚ lr=2.5e-04
â¹ï¸  Early stopping at epoch 102 (no improvement for 100 epochs).
âœ…  Model saved to ./experiments/gat_hd128_ld64_nr5_ep500_nf2.0_dl2/model.pt
ğŸ“ˆ  Loss curve saved to vgae_loss.png
Novel: 100.00% (Baseline), 100.00% (VGAE)
Unique: 100.00% (Baseline), 99.40% (VGAE)
Novel and Unique: 100.00% (Baseline), 99.40% (VGAE)
Computing node-level statistics...
Computing node-level statistics...
Computing node-level statistics...

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 24864480: <graph_generations> in cluster <dcc> Done

Job <graph_generations> was submitted from host <n-62-20-9> by user <s185927> in cluster <dcc> at Fri May  2 16:39:37 2025
Job was executed on host(s) <8*n-62-20-2>, in queue <gpuv100>, as user <s185927> in cluster <dcc> at Fri May  2 16:41:49 2025
</zhome/e3/3/139772> was used as the home directory.
</zhome/e3/3/139772/Desktop/AML/AML/Module_3/AML-Project-3> was used as the working directory.
Started at Fri May  2 16:41:49 2025
Terminated at Fri May  2 16:47:23 2025
Results reported at Fri May  2 16:47:23 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J graph_generations
#BSUB -q gpuv100
#BSUB -n 8
#BSUB -o logs/%J.out
#BSUB -e logs/%J.err
#BSUB -gpu "num=1:mode=exclusive_process"
#BSUB -W 5:00
#BSUB -R "span[hosts=1]"
#BSUB -R "rusage[mem=5GB]"
# end of BSUB options

module load cuda/11.8

source ~/Desktop/AML/aml_new/bin/activate

python -u src/main.py --mode 'train' --epochs 500 --lr 5e-4 --hidden_dim 128 --latent_dim 64 --num_enc_MP_rounds 5 --decoder gat  --neg_factor 2 --dec_layers 2 --heads 4 



------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   328.68 sec.
    Max Memory :                                 685 MB
    Average Memory :                             684.20 MB
    Total Requested Memory :                     40960.00 MB
    Delta Memory :                               40275.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   334 sec.
    Turnaround time :                            466 sec.

The output (if any) is above this job summary.



PS:

Read file <logs/24864480.err> for stderr output of this job.

