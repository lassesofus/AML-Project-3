Experiment arguments: {'mode': 'train', 'epochs': 500, 'lr': 0.0005, 'beta': 5, 'neg_factor': 3.0, 'device': 'cuda', 'hidden_dim': 64, 'latent_dim': 32, 'num_enc_MP_rounds': 3, 'dec_layers': 1, 'heads': 4, 'decoder': 'gat', 'checkpoint': './experiments/gat_hd64_ld32_nr3_ep500_nf3.0_dl1/model.pt', 'log_dir': './experiments/gat_hd64_ld32_nr3_ep500_nf3.0_dl1/logs', 'fig_dir': './experiments/gat_hd64_ld32_nr3_ep500_nf3.0_dl1/figures'}
Loaded 171 training hashes.
cuda
Device: cuda
Training VGAE model...
Epoch 0001 â”‚ loss=2.9420 â”‚ recon=1.371 â”‚ kl=2.113 â”‚ Î²=0.02 â”‚ lr=5.0e-04
Epoch 0002 â”‚ loss=1.4951 â”‚ recon=1.414 â”‚ kl=2.331 â”‚ Î²=0.04 â”‚ lr=5.0e-04
Epoch 0003 â”‚ loss=1.5044 â”‚ recon=1.264 â”‚ kl=2.262 â”‚ Î²=0.06 â”‚ lr=5.0e-04
Epoch 0004 â”‚ loss=1.4240 â”‚ recon=1.221 â”‚ kl=2.225 â”‚ Î²=0.08 â”‚ lr=5.0e-04
Epoch 0005 â”‚ loss=1.4162 â”‚ recon=1.325 â”‚ kl=2.038 â”‚ Î²=0.10 â”‚ lr=5.0e-04
Epoch 0006 â”‚ loss=1.4317 â”‚ recon=1.283 â”‚ kl=1.816 â”‚ Î²=0.12 â”‚ lr=5.0e-04
Epoch 0007 â”‚ loss=1.4533 â”‚ recon=1.108 â”‚ kl=1.760 â”‚ Î²=0.14 â”‚ lr=5.0e-04
Epoch 0008 â”‚ loss=1.4769 â”‚ recon=1.142 â”‚ kl=1.728 â”‚ Î²=0.16 â”‚ lr=5.0e-04
Epoch 0009 â”‚ loss=1.4727 â”‚ recon=1.112 â”‚ kl=1.683 â”‚ Î²=0.18 â”‚ lr=5.0e-04
Epoch 0010 â”‚ loss=1.4742 â”‚ recon=1.211 â”‚ kl=1.562 â”‚ Î²=0.20 â”‚ lr=5.0e-04
Epoch 0011 â”‚ loss=1.4675 â”‚ recon=1.152 â”‚ kl=1.603 â”‚ Î²=0.22 â”‚ lr=5.0e-04
Epoch 0012 â”‚ loss=1.4696 â”‚ recon=1.173 â”‚ kl=1.477 â”‚ Î²=0.24 â”‚ lr=5.0e-04
Epoch 0013 â”‚ loss=1.4843 â”‚ recon=1.192 â”‚ kl=1.475 â”‚ Î²=0.26 â”‚ lr=5.0e-04
Epoch 0014 â”‚ loss=1.4899 â”‚ recon=1.179 â”‚ kl=1.374 â”‚ Î²=0.28 â”‚ lr=5.0e-04
Epoch 0015 â”‚ loss=1.5271 â”‚ recon=1.137 â”‚ kl=1.316 â”‚ Î²=0.30 â”‚ lr=5.0e-04
Epoch 0016 â”‚ loss=1.5190 â”‚ recon=1.102 â”‚ kl=1.325 â”‚ Î²=0.32 â”‚ lr=5.0e-04
Epoch 0017 â”‚ loss=1.5575 â”‚ recon=1.141 â”‚ kl=1.341 â”‚ Î²=0.34 â”‚ lr=5.0e-04
Epoch 0018 â”‚ loss=1.5557 â”‚ recon=1.387 â”‚ kl=1.271 â”‚ Î²=0.36 â”‚ lr=5.0e-04
Epoch 0019 â”‚ loss=1.5575 â”‚ recon=1.145 â”‚ kl=1.176 â”‚ Î²=0.38 â”‚ lr=5.0e-04
Epoch 0020 â”‚ loss=1.5712 â”‚ recon=1.166 â”‚ kl=1.206 â”‚ Î²=0.40 â”‚ lr=5.0e-04
Epoch 0021 â”‚ loss=1.5920 â”‚ recon=1.190 â”‚ kl=1.169 â”‚ Î²=0.42 â”‚ lr=5.0e-04
Epoch 0022 â”‚ loss=1.6228 â”‚ recon=1.288 â”‚ kl=1.175 â”‚ Î²=0.44 â”‚ lr=5.0e-04
Epoch 0023 â”‚ loss=1.6285 â”‚ recon=1.048 â”‚ kl=1.044 â”‚ Î²=0.46 â”‚ lr=5.0e-04
Epoch 0024 â”‚ loss=1.6215 â”‚ recon=1.029 â”‚ kl=1.067 â”‚ Î²=0.48 â”‚ lr=5.0e-04
Epoch 0025 â”‚ loss=1.6182 â”‚ recon=1.073 â”‚ kl=0.986 â”‚ Î²=0.50 â”‚ lr=5.0e-04
Epoch 0026 â”‚ loss=1.6418 â”‚ recon=1.076 â”‚ kl=1.038 â”‚ Î²=0.52 â”‚ lr=5.0e-04
Epoch 0027 â”‚ loss=1.6692 â”‚ recon=1.026 â”‚ kl=1.026 â”‚ Î²=0.54 â”‚ lr=5.0e-04
Epoch 0028 â”‚ loss=1.6709 â”‚ recon=1.277 â”‚ kl=0.958 â”‚ Î²=0.56 â”‚ lr=5.0e-04
Epoch 0029 â”‚ loss=1.6809 â”‚ recon=1.183 â”‚ kl=0.964 â”‚ Î²=0.58 â”‚ lr=5.0e-04
Epoch 0030 â”‚ loss=1.7073 â”‚ recon=0.946 â”‚ kl=0.944 â”‚ Î²=0.60 â”‚ lr=5.0e-04
Epoch 0031 â”‚ loss=1.7179 â”‚ recon=1.231 â”‚ kl=0.898 â”‚ Î²=0.62 â”‚ lr=5.0e-04
Epoch 0032 â”‚ loss=1.7058 â”‚ recon=1.034 â”‚ kl=0.888 â”‚ Î²=0.64 â”‚ lr=5.0e-04
Epoch 0033 â”‚ loss=1.7360 â”‚ recon=1.241 â”‚ kl=0.906 â”‚ Î²=0.66 â”‚ lr=5.0e-04
Epoch 0034 â”‚ loss=1.7285 â”‚ recon=0.952 â”‚ kl=0.880 â”‚ Î²=0.68 â”‚ lr=5.0e-04
Epoch 0035 â”‚ loss=1.7364 â”‚ recon=1.021 â”‚ kl=0.858 â”‚ Î²=0.70 â”‚ lr=5.0e-04
Epoch 0036 â”‚ loss=1.7526 â”‚ recon=1.248 â”‚ kl=0.863 â”‚ Î²=0.72 â”‚ lr=5.0e-04
Epoch 0037 â”‚ loss=1.7658 â”‚ recon=1.077 â”‚ kl=0.872 â”‚ Î²=0.74 â”‚ lr=5.0e-04
Epoch 0038 â”‚ loss=1.7865 â”‚ recon=0.791 â”‚ kl=0.855 â”‚ Î²=0.76 â”‚ lr=5.0e-04
Epoch 0039 â”‚ loss=1.8145 â”‚ recon=1.155 â”‚ kl=0.830 â”‚ Î²=0.78 â”‚ lr=5.0e-04
Epoch 0040 â”‚ loss=1.8235 â”‚ recon=1.221 â”‚ kl=0.796 â”‚ Î²=0.80 â”‚ lr=5.0e-04
Epoch 0041 â”‚ loss=1.8132 â”‚ recon=1.130 â”‚ kl=0.813 â”‚ Î²=0.82 â”‚ lr=5.0e-04
Epoch 0042 â”‚ loss=1.8323 â”‚ recon=0.822 â”‚ kl=0.840 â”‚ Î²=0.84 â”‚ lr=5.0e-04
Epoch 0043 â”‚ loss=1.8346 â”‚ recon=1.437 â”‚ kl=0.831 â”‚ Î²=0.86 â”‚ lr=5.0e-04
Epoch 0044 â”‚ loss=1.8383 â”‚ recon=1.150 â”‚ kl=0.794 â”‚ Î²=0.88 â”‚ lr=5.0e-04
Epoch 0045 â”‚ loss=1.8721 â”‚ recon=1.702 â”‚ kl=0.751 â”‚ Î²=0.90 â”‚ lr=5.0e-04
Epoch 0046 â”‚ loss=1.8427 â”‚ recon=1.048 â”‚ kl=0.747 â”‚ Î²=0.92 â”‚ lr=5.0e-04
Epoch 0047 â”‚ loss=1.8948 â”‚ recon=0.986 â”‚ kl=0.769 â”‚ Î²=0.94 â”‚ lr=5.0e-04
Epoch 0048 â”‚ loss=1.8940 â”‚ recon=1.393 â”‚ kl=0.728 â”‚ Î²=0.96 â”‚ lr=5.0e-04
Epoch 0049 â”‚ loss=1.8921 â”‚ recon=1.030 â”‚ kl=0.721 â”‚ Î²=0.98 â”‚ lr=5.0e-04
Epoch 0050 â”‚ loss=1.9032 â”‚ recon=1.083 â”‚ kl=0.747 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0051 â”‚ loss=1.8813 â”‚ recon=1.259 â”‚ kl=0.750 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0052 â”‚ loss=1.8909 â”‚ recon=1.099 â”‚ kl=0.720 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0053 â”‚ loss=1.9372 â”‚ recon=1.069 â”‚ kl=0.739 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0054 â”‚ loss=1.8952 â”‚ recon=1.238 â”‚ kl=0.701 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0055 â”‚ loss=1.8958 â”‚ recon=1.524 â”‚ kl=0.690 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0056 â”‚ loss=1.8768 â”‚ recon=1.543 â”‚ kl=0.694 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0057 â”‚ loss=1.8884 â”‚ recon=1.143 â”‚ kl=0.694 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0058 â”‚ loss=1.8573 â”‚ recon=1.394 â”‚ kl=0.718 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0059 â”‚ loss=1.8724 â”‚ recon=0.988 â”‚ kl=0.697 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0060 â”‚ loss=1.8680 â”‚ recon=1.517 â”‚ kl=0.684 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0061 â”‚ loss=1.8532 â”‚ recon=1.101 â”‚ kl=0.688 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0062 â”‚ loss=1.8542 â”‚ recon=1.698 â”‚ kl=0.698 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0063 â”‚ loss=1.8754 â”‚ recon=1.481 â”‚ kl=0.686 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0064 â”‚ loss=1.8851 â”‚ recon=0.914 â”‚ kl=0.698 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0065 â”‚ loss=1.8874 â”‚ recon=0.966 â”‚ kl=0.695 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0066 â”‚ loss=1.8490 â”‚ recon=0.954 â”‚ kl=0.702 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0067 â”‚ loss=1.8547 â”‚ recon=0.936 â”‚ kl=0.697 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0068 â”‚ loss=1.8668 â”‚ recon=1.234 â”‚ kl=0.716 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0069 â”‚ loss=1.8769 â”‚ recon=1.185 â”‚ kl=0.719 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0070 â”‚ loss=1.8298 â”‚ recon=1.193 â”‚ kl=0.656 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0071 â”‚ loss=1.8295 â”‚ recon=0.808 â”‚ kl=0.687 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0072 â”‚ loss=1.8566 â”‚ recon=1.040 â”‚ kl=0.711 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0073 â”‚ loss=1.8212 â”‚ recon=1.408 â”‚ kl=0.675 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0074 â”‚ loss=1.8609 â”‚ recon=0.987 â”‚ kl=0.698 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0075 â”‚ loss=1.8337 â”‚ recon=1.229 â”‚ kl=0.679 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0076 â”‚ loss=1.8336 â”‚ recon=1.202 â”‚ kl=0.697 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0077 â”‚ loss=1.8387 â”‚ recon=1.255 â”‚ kl=0.668 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0078 â”‚ loss=1.8236 â”‚ recon=1.063 â”‚ kl=0.701 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0079 â”‚ loss=1.8430 â”‚ recon=1.197 â”‚ kl=0.664 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0080 â”‚ loss=1.8773 â”‚ recon=1.252 â”‚ kl=0.733 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0081 â”‚ loss=1.8566 â”‚ recon=0.937 â”‚ kl=0.748 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0082 â”‚ loss=1.8295 â”‚ recon=1.227 â”‚ kl=0.687 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0083 â”‚ loss=1.8273 â”‚ recon=1.160 â”‚ kl=0.656 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0084 â”‚ loss=1.8217 â”‚ recon=0.857 â”‚ kl=0.662 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0085 â”‚ loss=1.8350 â”‚ recon=0.933 â”‚ kl=0.683 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0086 â”‚ loss=1.8032 â”‚ recon=1.155 â”‚ kl=0.684 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0087 â”‚ loss=1.8178 â”‚ recon=0.993 â”‚ kl=0.637 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0088 â”‚ loss=1.8150 â”‚ recon=1.081 â”‚ kl=0.696 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0089 â”‚ loss=1.8193 â”‚ recon=0.920 â”‚ kl=0.683 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0090 â”‚ loss=1.8201 â”‚ recon=1.084 â”‚ kl=0.661 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0091 â”‚ loss=1.8242 â”‚ recon=1.189 â”‚ kl=0.692 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0092 â”‚ loss=1.8125 â”‚ recon=1.107 â”‚ kl=0.673 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0093 â”‚ loss=1.8261 â”‚ recon=1.162 â”‚ kl=0.666 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0094 â”‚ loss=1.8208 â”‚ recon=0.978 â”‚ kl=0.668 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0095 â”‚ loss=1.8175 â”‚ recon=1.459 â”‚ kl=0.657 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0096 â”‚ loss=1.8462 â”‚ recon=1.189 â”‚ kl=0.694 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0097 â”‚ loss=1.8266 â”‚ recon=1.081 â”‚ kl=0.691 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0098 â”‚ loss=1.8243 â”‚ recon=0.706 â”‚ kl=0.693 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0099 â”‚ loss=1.8171 â”‚ recon=1.373 â”‚ kl=0.694 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0100 â”‚ loss=1.8259 â”‚ recon=0.821 â”‚ kl=0.659 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0101 â”‚ loss=1.8068 â”‚ recon=1.202 â”‚ kl=0.683 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0102 â”‚ loss=1.7968 â”‚ recon=1.334 â”‚ kl=0.673 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0103 â”‚ loss=1.8147 â”‚ recon=1.206 â”‚ kl=0.664 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0104 â”‚ loss=1.7976 â”‚ recon=1.024 â”‚ kl=0.674 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0105 â”‚ loss=1.8053 â”‚ recon=1.321 â”‚ kl=0.691 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0106 â”‚ loss=1.7977 â”‚ recon=0.759 â”‚ kl=0.670 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0107 â”‚ loss=1.8029 â”‚ recon=1.503 â”‚ kl=0.683 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0108 â”‚ loss=1.8006 â”‚ recon=1.181 â”‚ kl=0.655 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0109 â”‚ loss=1.8170 â”‚ recon=0.906 â”‚ kl=0.701 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0110 â”‚ loss=1.8149 â”‚ recon=1.255 â”‚ kl=0.689 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0111 â”‚ loss=1.7956 â”‚ recon=1.645 â”‚ kl=0.673 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0112 â”‚ loss=1.7988 â”‚ recon=1.716 â”‚ kl=0.666 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0113 â”‚ loss=1.7957 â”‚ recon=1.015 â”‚ kl=0.693 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0114 â”‚ loss=1.7860 â”‚ recon=1.097 â”‚ kl=0.683 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0115 â”‚ loss=1.7893 â”‚ recon=0.812 â”‚ kl=0.682 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0116 â”‚ loss=1.7826 â”‚ recon=1.033 â”‚ kl=0.666 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0117 â”‚ loss=1.8030 â”‚ recon=1.123 â”‚ kl=0.669 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0118 â”‚ loss=1.7953 â”‚ recon=1.346 â”‚ kl=0.667 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0119 â”‚ loss=1.7822 â”‚ recon=1.066 â”‚ kl=0.654 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0120 â”‚ loss=1.7974 â”‚ recon=1.124 â”‚ kl=0.665 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0121 â”‚ loss=1.8054 â”‚ recon=1.213 â”‚ kl=0.674 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0122 â”‚ loss=1.8088 â”‚ recon=1.272 â”‚ kl=0.684 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0123 â”‚ loss=1.8095 â”‚ recon=0.999 â”‚ kl=0.683 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0124 â”‚ loss=1.7927 â”‚ recon=1.249 â”‚ kl=0.674 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0125 â”‚ loss=1.8049 â”‚ recon=1.114 â”‚ kl=0.690 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0126 â”‚ loss=1.7698 â”‚ recon=1.428 â”‚ kl=0.665 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0127 â”‚ loss=1.8060 â”‚ recon=0.982 â”‚ kl=0.681 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0128 â”‚ loss=1.8375 â”‚ recon=1.334 â”‚ kl=0.681 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0129 â”‚ loss=1.8089 â”‚ recon=1.151 â”‚ kl=0.689 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0130 â”‚ loss=1.8183 â”‚ recon=0.937 â”‚ kl=0.679 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0131 â”‚ loss=1.8310 â”‚ recon=1.228 â”‚ kl=0.706 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0132 â”‚ loss=1.7862 â”‚ recon=1.299 â”‚ kl=0.678 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0133 â”‚ loss=1.7891 â”‚ recon=1.068 â”‚ kl=0.699 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0134 â”‚ loss=1.8244 â”‚ recon=1.271 â”‚ kl=0.703 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0135 â”‚ loss=1.7732 â”‚ recon=1.213 â”‚ kl=0.668 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0136 â”‚ loss=1.8037 â”‚ recon=1.110 â”‚ kl=0.674 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0137 â”‚ loss=1.7755 â”‚ recon=0.960 â”‚ kl=0.672 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0138 â”‚ loss=1.7941 â”‚ recon=1.349 â”‚ kl=0.691 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0139 â”‚ loss=1.8178 â”‚ recon=0.908 â”‚ kl=0.695 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0140 â”‚ loss=1.7794 â”‚ recon=1.117 â”‚ kl=0.684 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0141 â”‚ loss=1.8064 â”‚ recon=1.529 â”‚ kl=0.709 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0142 â”‚ loss=1.7681 â”‚ recon=1.192 â”‚ kl=0.695 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0143 â”‚ loss=1.7669 â”‚ recon=1.114 â”‚ kl=0.669 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0144 â”‚ loss=1.7822 â”‚ recon=1.449 â”‚ kl=0.676 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0145 â”‚ loss=1.7705 â”‚ recon=0.957 â”‚ kl=0.667 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0146 â”‚ loss=1.7696 â”‚ recon=1.256 â”‚ kl=0.676 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0147 â”‚ loss=1.7880 â”‚ recon=1.223 â”‚ kl=0.683 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0148 â”‚ loss=1.7816 â”‚ recon=1.137 â”‚ kl=0.673 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0149 â”‚ loss=1.8105 â”‚ recon=0.900 â”‚ kl=0.692 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0150 â”‚ loss=1.7793 â”‚ recon=0.676 â”‚ kl=0.667 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0151 â”‚ loss=1.7756 â”‚ recon=1.635 â”‚ kl=0.677 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0152 â”‚ loss=1.7975 â”‚ recon=1.232 â”‚ kl=0.678 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0153 â”‚ loss=1.8067 â”‚ recon=1.370 â”‚ kl=0.701 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0154 â”‚ loss=1.7820 â”‚ recon=0.773 â”‚ kl=0.677 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0155 â”‚ loss=1.7870 â”‚ recon=1.357 â”‚ kl=0.695 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0156 â”‚ loss=1.8093 â”‚ recon=1.220 â”‚ kl=0.666 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0157 â”‚ loss=1.7793 â”‚ recon=1.188 â”‚ kl=0.685 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0158 â”‚ loss=1.7683 â”‚ recon=1.172 â”‚ kl=0.691 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0159 â”‚ loss=1.7726 â”‚ recon=1.308 â”‚ kl=0.668 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0160 â”‚ loss=1.7885 â”‚ recon=0.835 â”‚ kl=0.661 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0161 â”‚ loss=1.7738 â”‚ recon=0.849 â”‚ kl=0.693 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0162 â”‚ loss=1.7663 â”‚ recon=1.380 â”‚ kl=0.669 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0163 â”‚ loss=1.7868 â”‚ recon=0.952 â”‚ kl=0.674 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0164 â”‚ loss=1.7692 â”‚ recon=0.818 â”‚ kl=0.671 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0165 â”‚ loss=1.7671 â”‚ recon=0.847 â”‚ kl=0.673 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0166 â”‚ loss=1.7633 â”‚ recon=1.395 â”‚ kl=0.666 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0167 â”‚ loss=1.7664 â”‚ recon=1.137 â”‚ kl=0.655 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0168 â”‚ loss=1.7707 â”‚ recon=1.339 â”‚ kl=0.683 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0169 â”‚ loss=1.7807 â”‚ recon=0.924 â”‚ kl=0.675 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0170 â”‚ loss=1.7761 â”‚ recon=0.908 â”‚ kl=0.662 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0171 â”‚ loss=1.7461 â”‚ recon=0.879 â”‚ kl=0.656 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0172 â”‚ loss=1.7806 â”‚ recon=1.712 â”‚ kl=0.670 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0173 â”‚ loss=1.7856 â”‚ recon=1.358 â”‚ kl=0.688 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0174 â”‚ loss=1.7893 â”‚ recon=1.331 â”‚ kl=0.687 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0175 â”‚ loss=1.7626 â”‚ recon=0.976 â”‚ kl=0.677 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0176 â”‚ loss=1.7881 â”‚ recon=0.988 â”‚ kl=0.674 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0177 â”‚ loss=1.7733 â”‚ recon=1.334 â”‚ kl=0.671 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0178 â”‚ loss=1.7541 â”‚ recon=1.011 â”‚ kl=0.675 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0179 â”‚ loss=1.7686 â”‚ recon=1.441 â”‚ kl=0.682 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0180 â”‚ loss=1.7747 â”‚ recon=1.036 â”‚ kl=0.677 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0181 â”‚ loss=1.7737 â”‚ recon=0.748 â”‚ kl=0.672 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0182 â”‚ loss=1.7666 â”‚ recon=1.202 â”‚ kl=0.673 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0183 â”‚ loss=1.7672 â”‚ recon=1.172 â”‚ kl=0.661 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0184 â”‚ loss=1.7759 â”‚ recon=0.981 â”‚ kl=0.682 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0185 â”‚ loss=1.7773 â”‚ recon=1.303 â”‚ kl=0.664 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0186 â”‚ loss=1.7591 â”‚ recon=0.952 â”‚ kl=0.672 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0187 â”‚ loss=1.8016 â”‚ recon=0.994 â”‚ kl=0.689 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0188 â”‚ loss=1.7559 â”‚ recon=1.029 â”‚ kl=0.674 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0189 â”‚ loss=1.7719 â”‚ recon=1.497 â”‚ kl=0.671 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0190 â”‚ loss=1.7580 â”‚ recon=1.198 â”‚ kl=0.665 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0191 â”‚ loss=1.7557 â”‚ recon=0.810 â”‚ kl=0.660 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0192 â”‚ loss=1.7769 â”‚ recon=0.945 â”‚ kl=0.670 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0193 â”‚ loss=1.7817 â”‚ recon=0.971 â”‚ kl=0.667 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0194 â”‚ loss=1.7777 â”‚ recon=1.264 â”‚ kl=0.683 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0195 â”‚ loss=1.7468 â”‚ recon=1.245 â”‚ kl=0.672 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0196 â”‚ loss=1.7611 â”‚ recon=1.145 â”‚ kl=0.674 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0197 â”‚ loss=1.7825 â”‚ recon=1.233 â”‚ kl=0.670 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0198 â”‚ loss=1.7607 â”‚ recon=1.011 â”‚ kl=0.677 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0199 â”‚ loss=1.7940 â”‚ recon=0.894 â”‚ kl=0.679 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0200 â”‚ loss=1.7782 â”‚ recon=1.120 â”‚ kl=0.676 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0201 â”‚ loss=1.7614 â”‚ recon=1.296 â”‚ kl=0.670 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0202 â”‚ loss=1.7625 â”‚ recon=1.008 â”‚ kl=0.671 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0203 â”‚ loss=1.7674 â”‚ recon=1.191 â”‚ kl=0.658 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0204 â”‚ loss=1.7776 â”‚ recon=0.929 â”‚ kl=0.674 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0205 â”‚ loss=1.7824 â”‚ recon=0.924 â”‚ kl=0.689 â”‚ Î²=1.00 â”‚ lr=6.3e-05
â¹ï¸  Early stopping at epoch 205 (no improvement for 200 epochs).
âœ…  Model saved to ./experiments/gat_hd64_ld32_nr3_ep500_nf3.0_dl1/model.pt
ğŸ“ˆ  Loss curve saved to vgae_loss.png
Novel: 100.00% (Baseline), 100.00% (VGAE)
Unique: 100.00% (Baseline), 98.80% (VGAE)
Novel and Unique: 100.00% (Baseline), 98.80% (VGAE)
Computing node-level statistics...
Computing node-level statistics...
Computing node-level statistics...

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 24882722: <graph_generations> in cluster <dcc> Done

Job <graph_generations> was submitted from host <n-62-20-1> by user <s185927> in cluster <dcc> at Sun May  4 07:26:48 2025
Job was executed on host(s) <8*n-62-20-10>, in queue <gpuv100>, as user <s185927> in cluster <dcc> at Sun May  4 07:26:49 2025
</zhome/e3/3/139772> was used as the home directory.
</zhome/e3/3/139772/Desktop/AML/AML/Module_3/AML-Project-3> was used as the working directory.
Started at Sun May  4 07:26:49 2025
Terminated at Sun May  4 07:35:56 2025
Results reported at Sun May  4 07:35:56 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J graph_generations
#BSUB -q gpuv100
#BSUB -n 8
#BSUB -o logs/%J.out
#BSUB -e logs/%J.err
#BSUB -gpu "num=1:mode=exclusive_process"
#BSUB -W 5:00
#BSUB -R "span[hosts=1]"
#BSUB -R "rusage[mem=5GB]"
# end of BSUB options

module load cuda/11.8

source ~/Desktop/AML/aml_new/bin/activate

python -u src/main.py --mode 'train' --epochs 500 --lr 5e-4 --hidden_dim 64 --latent_dim 32 --num_enc_MP_rounds 3 --decoder gat  --neg_factor 3 --dec_layers 1 --heads 4 



------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   525.00 sec.
    Max Memory :                                 704 MB
    Average Memory :                             628.00 MB
    Total Requested Memory :                     40960.00 MB
    Delta Memory :                               40256.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                11
    Run time :                                   616 sec.
    Turnaround time :                            548 sec.

The output (if any) is above this job summary.



PS:

Read file <logs/24882722.err> for stderr output of this job.

