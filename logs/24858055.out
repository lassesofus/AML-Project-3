Experiment arguments: {'mode': 'train', 'epochs': 500, 'lr': 0.0005, 'beta': 2.0, 'neg_factor': 5.0, 'device': 'cuda', 'hidden_dim': 64, 'latent_dim': 32, 'num_rounds': 3, 'gnn_layers': 1, 'decoder': 'gnn', 'checkpoint': './experiments/gnn_hd64_ld32_nr3_ep500_b2.0_nf5.0/model.pt', 'log_dir': './experiments/gnn_hd64_ld32_nr3_ep500_b2.0_nf5.0/logs', 'fig_dir': './experiments/gnn_hd64_ld32_nr3_ep500_b2.0_nf5.0/figures'}
Loaded 171 training hashes.
Epoch 0001 â”‚ loss=2.5771 â”‚ recon=1.348 â”‚ kl=2.386 â”‚ Î²=0.02 â”‚ lr=5.0e-04
Epoch 0005 â”‚ loss=1.3467 â”‚ recon=1.192 â”‚ kl=2.014 â”‚ Î²=0.10 â”‚ lr=5.0e-04
Epoch 0010 â”‚ loss=1.3234 â”‚ recon=1.098 â”‚ kl=1.509 â”‚ Î²=0.20 â”‚ lr=5.0e-04
Epoch 0015 â”‚ loss=1.4016 â”‚ recon=1.056 â”‚ kl=1.323 â”‚ Î²=0.30 â”‚ lr=5.0e-04
Epoch 0020 â”‚ loss=1.4323 â”‚ recon=0.870 â”‚ kl=1.182 â”‚ Î²=0.40 â”‚ lr=5.0e-04
Epoch 0025 â”‚ loss=1.4782 â”‚ recon=0.899 â”‚ kl=1.041 â”‚ Î²=0.50 â”‚ lr=5.0e-04
Epoch 0030 â”‚ loss=1.5463 â”‚ recon=0.768 â”‚ kl=0.919 â”‚ Î²=0.60 â”‚ lr=5.0e-04
Epoch 0035 â”‚ loss=1.5709 â”‚ recon=0.883 â”‚ kl=0.860 â”‚ Î²=0.70 â”‚ lr=5.0e-04
Epoch 0040 â”‚ loss=1.6405 â”‚ recon=0.648 â”‚ kl=0.838 â”‚ Î²=0.80 â”‚ lr=5.0e-04
Epoch 0045 â”‚ loss=1.6864 â”‚ recon=1.264 â”‚ kl=0.742 â”‚ Î²=0.90 â”‚ lr=5.0e-04
Epoch 0050 â”‚ loss=1.7417 â”‚ recon=1.058 â”‚ kl=0.750 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0055 â”‚ loss=1.7340 â”‚ recon=1.300 â”‚ kl=0.714 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0060 â”‚ loss=1.7489 â”‚ recon=1.860 â”‚ kl=0.711 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0065 â”‚ loss=1.7288 â”‚ recon=0.819 â”‚ kl=0.717 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0070 â”‚ loss=1.6893 â”‚ recon=0.913 â”‚ kl=0.688 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0075 â”‚ loss=1.6942 â”‚ recon=0.834 â”‚ kl=0.670 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0080 â”‚ loss=1.7081 â”‚ recon=0.857 â”‚ kl=0.752 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0085 â”‚ loss=1.6931 â”‚ recon=0.796 â”‚ kl=0.699 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0090 â”‚ loss=1.6764 â”‚ recon=1.106 â”‚ kl=0.654 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0095 â”‚ loss=1.6684 â”‚ recon=1.198 â”‚ kl=0.700 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0100 â”‚ loss=1.6770 â”‚ recon=0.842 â”‚ kl=0.694 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0105 â”‚ loss=1.6265 â”‚ recon=1.106 â”‚ kl=0.677 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0110 â”‚ loss=1.6621 â”‚ recon=0.809 â”‚ kl=0.710 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0115 â”‚ loss=1.6418 â”‚ recon=0.583 â”‚ kl=0.691 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0120 â”‚ loss=1.6467 â”‚ recon=0.814 â”‚ kl=0.683 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0125 â”‚ loss=1.6324 â”‚ recon=0.819 â”‚ kl=0.691 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0130 â”‚ loss=1.6380 â”‚ recon=0.827 â”‚ kl=0.699 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0135 â”‚ loss=1.6310 â”‚ recon=1.245 â”‚ kl=0.687 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0140 â”‚ loss=1.6233 â”‚ recon=0.736 â”‚ kl=0.732 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0145 â”‚ loss=1.6346 â”‚ recon=0.967 â”‚ kl=0.724 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0150 â”‚ loss=1.6334 â”‚ recon=0.732 â”‚ kl=0.720 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0155 â”‚ loss=1.6217 â”‚ recon=0.789 â”‚ kl=0.727 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0160 â”‚ loss=1.6344 â”‚ recon=0.865 â”‚ kl=0.703 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0165 â”‚ loss=1.6054 â”‚ recon=0.854 â”‚ kl=0.668 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0170 â”‚ loss=1.6306 â”‚ recon=0.714 â”‚ kl=0.696 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0175 â”‚ loss=1.6111 â”‚ recon=0.892 â”‚ kl=0.699 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0180 â”‚ loss=1.6114 â”‚ recon=1.118 â”‚ kl=0.682 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0185 â”‚ loss=1.6173 â”‚ recon=0.741 â”‚ kl=0.701 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0190 â”‚ loss=1.5901 â”‚ recon=1.133 â”‚ kl=0.702 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0195 â”‚ loss=1.5956 â”‚ recon=1.046 â”‚ kl=0.698 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0200 â”‚ loss=1.5927 â”‚ recon=0.919 â”‚ kl=0.719 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0205 â”‚ loss=1.6145 â”‚ recon=0.878 â”‚ kl=0.724 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0210 â”‚ loss=1.5983 â”‚ recon=1.069 â”‚ kl=0.684 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0215 â”‚ loss=1.5996 â”‚ recon=0.917 â”‚ kl=0.694 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0220 â”‚ loss=1.5888 â”‚ recon=0.774 â”‚ kl=0.694 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0225 â”‚ loss=1.6013 â”‚ recon=1.213 â”‚ kl=0.705 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0230 â”‚ loss=1.5839 â”‚ recon=0.813 â”‚ kl=0.711 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0235 â”‚ loss=1.5994 â”‚ recon=0.981 â”‚ kl=0.735 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0240 â”‚ loss=1.5914 â”‚ recon=1.199 â”‚ kl=0.718 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0245 â”‚ loss=1.5974 â”‚ recon=1.170 â”‚ kl=0.719 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0250 â”‚ loss=1.5915 â”‚ recon=0.692 â”‚ kl=0.722 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0255 â”‚ loss=1.5959 â”‚ recon=0.836 â”‚ kl=0.732 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0260 â”‚ loss=1.5963 â”‚ recon=0.861 â”‚ kl=0.687 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0265 â”‚ loss=1.6021 â”‚ recon=1.017 â”‚ kl=0.712 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0270 â”‚ loss=1.5738 â”‚ recon=0.883 â”‚ kl=0.731 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0275 â”‚ loss=1.5717 â”‚ recon=0.896 â”‚ kl=0.719 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0280 â”‚ loss=1.5722 â”‚ recon=0.695 â”‚ kl=0.676 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0285 â”‚ loss=1.5676 â”‚ recon=0.753 â”‚ kl=0.698 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0290 â”‚ loss=1.5790 â”‚ recon=0.786 â”‚ kl=0.705 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0295 â”‚ loss=1.5705 â”‚ recon=0.713 â”‚ kl=0.706 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0300 â”‚ loss=1.5982 â”‚ recon=1.004 â”‚ kl=0.695 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0305 â”‚ loss=1.5645 â”‚ recon=1.104 â”‚ kl=0.720 â”‚ Î²=1.00 â”‚ lr=1.3e-04
Epoch 0310 â”‚ loss=1.5776 â”‚ recon=0.546 â”‚ kl=0.700 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0315 â”‚ loss=1.5656 â”‚ recon=0.936 â”‚ kl=0.715 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0320 â”‚ loss=1.5719 â”‚ recon=0.726 â”‚ kl=0.693 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0325 â”‚ loss=1.5603 â”‚ recon=0.922 â”‚ kl=0.714 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0330 â”‚ loss=1.5887 â”‚ recon=0.868 â”‚ kl=0.696 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0335 â”‚ loss=1.5721 â”‚ recon=0.859 â”‚ kl=0.710 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0340 â”‚ loss=1.5832 â”‚ recon=0.791 â”‚ kl=0.720 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0345 â”‚ loss=1.5780 â”‚ recon=1.188 â”‚ kl=0.711 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0350 â”‚ loss=1.5886 â”‚ recon=0.850 â”‚ kl=0.724 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0355 â”‚ loss=1.5746 â”‚ recon=0.595 â”‚ kl=0.698 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0360 â”‚ loss=1.5938 â”‚ recon=0.868 â”‚ kl=0.721 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0365 â”‚ loss=1.5643 â”‚ recon=0.674 â”‚ kl=0.700 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0370 â”‚ loss=1.5624 â”‚ recon=0.999 â”‚ kl=0.692 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0375 â”‚ loss=1.5647 â”‚ recon=1.360 â”‚ kl=0.677 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0380 â”‚ loss=1.5733 â”‚ recon=0.880 â”‚ kl=0.730 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0385 â”‚ loss=1.5692 â”‚ recon=0.998 â”‚ kl=0.703 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0390 â”‚ loss=1.5671 â”‚ recon=0.821 â”‚ kl=0.706 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0395 â”‚ loss=1.5650 â”‚ recon=0.558 â”‚ kl=0.716 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0400 â”‚ loss=1.5729 â”‚ recon=0.951 â”‚ kl=0.712 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0405 â”‚ loss=1.5439 â”‚ recon=1.295 â”‚ kl=0.670 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0410 â”‚ loss=1.5741 â”‚ recon=0.928 â”‚ kl=0.720 â”‚ Î²=1.00 â”‚ lr=6.3e-05
Epoch 0415 â”‚ loss=1.5654 â”‚ recon=0.928 â”‚ kl=0.711 â”‚ Î²=1.00 â”‚ lr=5.0e-05
Epoch 0420 â”‚ loss=1.5807 â”‚ recon=0.872 â”‚ kl=0.700 â”‚ Î²=1.00 â”‚ lr=5.0e-05
Epoch 0425 â”‚ loss=1.5835 â”‚ recon=0.857 â”‚ kl=0.702 â”‚ Î²=1.00 â”‚ lr=5.0e-05
Epoch 0430 â”‚ loss=1.5664 â”‚ recon=0.864 â”‚ kl=0.703 â”‚ Î²=1.00 â”‚ lr=5.0e-05
Epoch 0435 â”‚ loss=1.5609 â”‚ recon=0.884 â”‚ kl=0.704 â”‚ Î²=1.00 â”‚ lr=5.0e-05
Epoch 0440 â”‚ loss=1.5869 â”‚ recon=0.985 â”‚ kl=0.673 â”‚ Î²=1.00 â”‚ lr=5.0e-05
Epoch 0445 â”‚ loss=1.5646 â”‚ recon=0.995 â”‚ kl=0.718 â”‚ Î²=1.00 â”‚ lr=5.0e-05
Epoch 0450 â”‚ loss=1.5568 â”‚ recon=1.078 â”‚ kl=0.693 â”‚ Î²=1.00 â”‚ lr=5.0e-05
Epoch 0455 â”‚ loss=1.5600 â”‚ recon=0.929 â”‚ kl=0.704 â”‚ Î²=1.00 â”‚ lr=5.0e-05
Epoch 0460 â”‚ loss=1.5677 â”‚ recon=1.092 â”‚ kl=0.702 â”‚ Î²=1.00 â”‚ lr=5.0e-05
Epoch 0465 â”‚ loss=1.5646 â”‚ recon=0.824 â”‚ kl=0.692 â”‚ Î²=1.00 â”‚ lr=5.0e-05
Epoch 0470 â”‚ loss=1.5550 â”‚ recon=0.802 â”‚ kl=0.725 â”‚ Î²=1.00 â”‚ lr=5.0e-05
Epoch 0475 â”‚ loss=1.5740 â”‚ recon=0.921 â”‚ kl=0.713 â”‚ Î²=1.00 â”‚ lr=5.0e-05
Epoch 0480 â”‚ loss=1.5560 â”‚ recon=0.891 â”‚ kl=0.717 â”‚ Î²=1.00 â”‚ lr=5.0e-05
Epoch 0485 â”‚ loss=1.5734 â”‚ recon=0.692 â”‚ kl=0.729 â”‚ Î²=1.00 â”‚ lr=5.0e-05
Epoch 0490 â”‚ loss=1.5584 â”‚ recon=1.072 â”‚ kl=0.711 â”‚ Î²=1.00 â”‚ lr=5.0e-05
Epoch 0495 â”‚ loss=1.5535 â”‚ recon=0.620 â”‚ kl=0.714 â”‚ Î²=1.00 â”‚ lr=5.0e-05
Epoch 0500 â”‚ loss=1.5793 â”‚ recon=0.866 â”‚ kl=0.708 â”‚ Î²=1.00 â”‚ lr=5.0e-05
âœ…  Model saved to ./experiments/gnn_hd64_ld32_nr3_ep500_b2.0_nf5.0/model.pt
ðŸ“ˆ  Loss curve saved to vgae_loss.png
Novel: 100.00% (Baseline), 100.00% (VGAE)
Unique: 100.00% (Baseline), 100.00% (VGAE)
Novel and Unique: 100.00% (Baseline), 100.00% (VGAE)
Computing node-level statistics...
Computing node-level statistics...
Computing node-level statistics...
