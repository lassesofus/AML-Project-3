Experiment arguments: {'mode': 'train', 'epochs': 500, 'beta': 2.0, 'neg_factor': 2.0, 'device': None, 'hidden_dim': 64, 'latent_dim': 32, 'num_rounds': 5, 'decoder': 'gnn', 'checkpoint': './experiments/gnn_hd64_ld32_nr5_ep500/model.pt', 'log_dir': './experiments/gnn_hd64_ld32_nr5_ep500/logs', 'fig_dir': './experiments/gnn_hd64_ld32_nr5_ep500/figures'}
Loaded 171 training hashes.
Epoch 0001 â”‚ loss=2.6043 â”‚ lr=1.0e-03
Epoch 0005 â”‚ loss=1.5917 â”‚ lr=1.0e-03
Epoch 0010 â”‚ loss=1.5142 â”‚ lr=1.0e-03
Epoch 0015 â”‚ loss=1.4726 â”‚ lr=1.0e-03
Epoch 0020 â”‚ loss=1.4576 â”‚ lr=1.0e-03
Epoch 0025 â”‚ loss=1.4420 â”‚ lr=1.0e-03
Epoch 0030 â”‚ loss=1.4296 â”‚ lr=1.0e-03
Epoch 0035 â”‚ loss=1.4272 â”‚ lr=1.0e-03
Epoch 0040 â”‚ loss=1.4240 â”‚ lr=1.0e-03
Epoch 0045 â”‚ loss=1.4132 â”‚ lr=1.0e-03
Epoch 0050 â”‚ loss=1.4117 â”‚ lr=1.0e-03
Epoch 0055 â”‚ loss=1.4052 â”‚ lr=1.0e-03
Epoch 0060 â”‚ loss=1.4074 â”‚ lr=1.0e-03
Epoch 0065 â”‚ loss=1.4071 â”‚ lr=1.0e-03
Epoch 0070 â”‚ loss=1.4023 â”‚ lr=1.0e-03
Epoch 0075 â”‚ loss=1.3984 â”‚ lr=1.0e-03
Epoch 0080 â”‚ loss=1.3998 â”‚ lr=1.0e-03
Epoch 0085 â”‚ loss=1.3980 â”‚ lr=1.0e-03
Epoch 0090 â”‚ loss=1.3964 â”‚ lr=1.0e-03
Epoch 0095 â”‚ loss=1.3935 â”‚ lr=1.0e-03
Epoch 0100 â”‚ loss=1.3987 â”‚ lr=5.0e-04
Epoch 0105 â”‚ loss=1.3942 â”‚ lr=5.0e-04
Epoch 0110 â”‚ loss=1.3923 â”‚ lr=5.0e-04
Epoch 0115 â”‚ loss=1.3964 â”‚ lr=5.0e-04
Epoch 0120 â”‚ loss=1.3936 â”‚ lr=5.0e-04
Epoch 0125 â”‚ loss=1.3904 â”‚ lr=5.0e-04
Epoch 0130 â”‚ loss=1.3920 â”‚ lr=5.0e-04
Epoch 0135 â”‚ loss=1.3928 â”‚ lr=5.0e-04
Epoch 0140 â”‚ loss=1.3905 â”‚ lr=5.0e-04
Epoch 0145 â”‚ loss=1.3923 â”‚ lr=5.0e-04
Epoch 0150 â”‚ loss=1.3888 â”‚ lr=5.0e-04
Epoch 0155 â”‚ loss=1.3913 â”‚ lr=5.0e-04
Epoch 0160 â”‚ loss=1.3884 â”‚ lr=5.0e-04
Epoch 0165 â”‚ loss=1.3883 â”‚ lr=5.0e-04
Epoch 0170 â”‚ loss=1.3891 â”‚ lr=5.0e-04
Epoch 0175 â”‚ loss=1.3898 â”‚ lr=5.0e-04
Epoch 0180 â”‚ loss=1.3882 â”‚ lr=5.0e-04
Epoch 0185 â”‚ loss=1.3885 â”‚ lr=5.0e-04
Epoch 0190 â”‚ loss=1.3877 â”‚ lr=5.0e-04
Epoch 0195 â”‚ loss=1.3886 â”‚ lr=5.0e-04
Epoch 0200 â”‚ loss=1.3883 â”‚ lr=2.5e-04
Epoch 0205 â”‚ loss=1.3877 â”‚ lr=2.5e-04
Epoch 0210 â”‚ loss=1.3886 â”‚ lr=2.5e-04
Epoch 0215 â”‚ loss=1.3883 â”‚ lr=2.5e-04
Epoch 0220 â”‚ loss=1.3875 â”‚ lr=2.5e-04
Epoch 0225 â”‚ loss=1.3873 â”‚ lr=2.5e-04
Epoch 0230 â”‚ loss=1.3888 â”‚ lr=2.5e-04
Epoch 0235 â”‚ loss=1.3883 â”‚ lr=2.5e-04
Epoch 0240 â”‚ loss=1.3876 â”‚ lr=2.5e-04
Epoch 0245 â”‚ loss=1.3878 â”‚ lr=2.5e-04
Epoch 0250 â”‚ loss=1.3863 â”‚ lr=2.5e-04
Epoch 0255 â”‚ loss=1.3865 â”‚ lr=2.5e-04
Epoch 0260 â”‚ loss=1.3866 â”‚ lr=2.5e-04
Epoch 0265 â”‚ loss=1.3873 â”‚ lr=2.5e-04
Epoch 0270 â”‚ loss=1.3870 â”‚ lr=2.5e-04
Epoch 0275 â”‚ loss=1.3873 â”‚ lr=2.5e-04
Epoch 0280 â”‚ loss=1.3880 â”‚ lr=2.5e-04
Epoch 0285 â”‚ loss=1.3881 â”‚ lr=2.5e-04
Epoch 0290 â”‚ loss=1.3874 â”‚ lr=2.5e-04
Epoch 0295 â”‚ loss=1.3878 â”‚ lr=2.5e-04
Epoch 0300 â”‚ loss=1.3863 â”‚ lr=1.3e-04
Epoch 0305 â”‚ loss=1.3885 â”‚ lr=1.3e-04
Epoch 0310 â”‚ loss=1.3873 â”‚ lr=1.3e-04
Epoch 0315 â”‚ loss=1.3869 â”‚ lr=1.3e-04
Epoch 0320 â”‚ loss=1.3863 â”‚ lr=1.3e-04
Epoch 0325 â”‚ loss=1.3872 â”‚ lr=1.3e-04
Epoch 0330 â”‚ loss=1.3869 â”‚ lr=1.3e-04
Epoch 0335 â”‚ loss=1.3874 â”‚ lr=1.3e-04
Epoch 0340 â”‚ loss=1.3868 â”‚ lr=1.3e-04
Epoch 0345 â”‚ loss=1.3866 â”‚ lr=1.3e-04
Epoch 0350 â”‚ loss=1.3876 â”‚ lr=1.3e-04
Epoch 0355 â”‚ loss=1.3867 â”‚ lr=1.3e-04
Epoch 0360 â”‚ loss=1.3873 â”‚ lr=1.3e-04
Epoch 0365 â”‚ loss=1.3877 â”‚ lr=1.3e-04
Epoch 0370 â”‚ loss=1.3872 â”‚ lr=1.3e-04
Epoch 0375 â”‚ loss=1.3868 â”‚ lr=1.3e-04
Epoch 0380 â”‚ loss=1.3865 â”‚ lr=1.3e-04
Epoch 0385 â”‚ loss=1.3867 â”‚ lr=1.3e-04
Epoch 0390 â”‚ loss=1.3873 â”‚ lr=1.3e-04
Epoch 0395 â”‚ loss=1.3869 â”‚ lr=1.3e-04
Epoch 0400 â”‚ loss=1.3873 â”‚ lr=6.3e-05
Epoch 0405 â”‚ loss=1.3873 â”‚ lr=6.3e-05
Epoch 0410 â”‚ loss=1.3864 â”‚ lr=6.3e-05
Epoch 0415 â”‚ loss=1.3868 â”‚ lr=6.3e-05
Epoch 0420 â”‚ loss=1.3869 â”‚ lr=6.3e-05
Epoch 0425 â”‚ loss=1.3869 â”‚ lr=6.3e-05
Epoch 0430 â”‚ loss=1.3872 â”‚ lr=6.3e-05
Epoch 0435 â”‚ loss=1.3860 â”‚ lr=6.3e-05
Epoch 0440 â”‚ loss=1.3873 â”‚ lr=6.3e-05
Epoch 0445 â”‚ loss=1.3870 â”‚ lr=6.3e-05
Epoch 0450 â”‚ loss=1.3859 â”‚ lr=6.3e-05
Epoch 0455 â”‚ loss=1.3872 â”‚ lr=6.3e-05
Epoch 0460 â”‚ loss=1.3871 â”‚ lr=6.3e-05
Epoch 0465 â”‚ loss=1.3867 â”‚ lr=6.3e-05
Epoch 0470 â”‚ loss=1.3870 â”‚ lr=6.3e-05
Epoch 0475 â”‚ loss=1.3869 â”‚ lr=6.3e-05
Epoch 0480 â”‚ loss=1.3869 â”‚ lr=6.3e-05
Epoch 0485 â”‚ loss=1.3871 â”‚ lr=6.3e-05
Epoch 0490 â”‚ loss=1.3865 â”‚ lr=6.3e-05
Epoch 0495 â”‚ loss=1.3870 â”‚ lr=6.3e-05
Epoch 0500 â”‚ loss=1.3874 â”‚ lr=3.1e-05
âœ…  Model saved to ./experiments/gnn_hd64_ld32_nr5_ep500/model.pt
ðŸ“ˆ  Loss curve saved to vgae_loss.png
Novel: 100.00% (Baseline), 100.00% (VGAE)
Unique: 100.00% (Baseline), 100.00% (VGAE)
Novel and Unique: 100.00% (Baseline), 100.00% (VGAE)
Computing node-level statistics...
Computing node-level statistics...
Computing node-level statistics...
