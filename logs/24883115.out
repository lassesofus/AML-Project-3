Experiment arguments: {'mode': 'train', 'epochs': 500, 'lr': 0.0005, 'beta': 5, 'neg_factor': 3.0, 'device': 'cuda', 'hidden_dim': 64, 'latent_dim': 32, 'num_enc_MP_rounds': 3, 'dec_layers': 1, 'heads': 4, 'decoder': 'gat', 'checkpoint': './experiments/gat_hd64_ld32_nr3_ep500_nf3.0_dl1/model.pt', 'log_dir': './experiments/gat_hd64_ld32_nr3_ep500_nf3.0_dl1/logs', 'fig_dir': './experiments/gat_hd64_ld32_nr3_ep500_nf3.0_dl1/figures'}
Loaded 171 training hashes.
cuda
Device: cuda
Training VGAE model...
Epoch 0001 â”‚ loss=3.6670 â”‚ recon=1.625 â”‚ kl=0.813 â”‚ Î²=0.02 â”‚ lr=5.0e-04
Epoch 0010 â”‚ loss=2.2463 â”‚ recon=1.782 â”‚ kl=0.697 â”‚ Î²=0.20 â”‚ lr=5.0e-04
Epoch 0020 â”‚ loss=2.1189 â”‚ recon=1.519 â”‚ kl=0.749 â”‚ Î²=0.40 â”‚ lr=5.0e-04
Epoch 0030 â”‚ loss=2.0548 â”‚ recon=1.109 â”‚ kl=0.689 â”‚ Î²=0.60 â”‚ lr=5.0e-04
Epoch 0040 â”‚ loss=2.0054 â”‚ recon=1.246 â”‚ kl=0.696 â”‚ Î²=0.80 â”‚ lr=5.0e-04
Epoch 0050 â”‚ loss=1.9418 â”‚ recon=1.262 â”‚ kl=0.717 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0060 â”‚ loss=1.8972 â”‚ recon=1.745 â”‚ kl=0.671 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0070 â”‚ loss=1.8519 â”‚ recon=1.398 â”‚ kl=0.630 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0080 â”‚ loss=1.8769 â”‚ recon=1.356 â”‚ kl=0.675 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0090 â”‚ loss=1.8124 â”‚ recon=1.158 â”‚ kl=0.623 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0100 â”‚ loss=1.8337 â”‚ recon=0.934 â”‚ kl=0.644 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0110 â”‚ loss=1.8195 â”‚ recon=0.956 â”‚ kl=0.655 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0120 â”‚ loss=1.7955 â”‚ recon=0.961 â”‚ kl=0.644 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0130 â”‚ loss=1.8211 â”‚ recon=0.912 â”‚ kl=0.675 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0140 â”‚ loss=1.7632 â”‚ recon=1.044 â”‚ kl=0.657 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0150 â”‚ loss=1.7634 â”‚ recon=0.599 â”‚ kl=0.701 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0160 â”‚ loss=1.7736 â”‚ recon=0.962 â”‚ kl=0.691 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0170 â”‚ loss=1.7501 â”‚ recon=0.871 â”‚ kl=0.655 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0180 â”‚ loss=1.7387 â”‚ recon=1.023 â”‚ kl=0.696 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0190 â”‚ loss=1.6981 â”‚ recon=1.211 â”‚ kl=0.662 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0200 â”‚ loss=1.7192 â”‚ recon=1.054 â”‚ kl=0.691 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0210 â”‚ loss=1.7135 â”‚ recon=0.984 â”‚ kl=0.651 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0220 â”‚ loss=1.6691 â”‚ recon=0.975 â”‚ kl=0.658 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0230 â”‚ loss=1.6927 â”‚ recon=0.639 â”‚ kl=0.711 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0240 â”‚ loss=1.6913 â”‚ recon=1.139 â”‚ kl=0.672 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0250 â”‚ loss=1.6639 â”‚ recon=0.768 â”‚ kl=0.645 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0260 â”‚ loss=1.6947 â”‚ recon=1.009 â”‚ kl=0.700 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0270 â”‚ loss=1.6810 â”‚ recon=0.982 â”‚ kl=0.671 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0280 â”‚ loss=1.6580 â”‚ recon=0.798 â”‚ kl=0.645 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0290 â”‚ loss=1.6939 â”‚ recon=0.796 â”‚ kl=0.687 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0300 â”‚ loss=1.6972 â”‚ recon=0.966 â”‚ kl=0.708 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0310 â”‚ loss=1.6778 â”‚ recon=1.154 â”‚ kl=0.660 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0320 â”‚ loss=1.6901 â”‚ recon=0.895 â”‚ kl=0.647 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0330 â”‚ loss=1.6725 â”‚ recon=0.854 â”‚ kl=0.688 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0340 â”‚ loss=1.6411 â”‚ recon=1.068 â”‚ kl=0.677 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0350 â”‚ loss=1.6782 â”‚ recon=0.826 â”‚ kl=0.686 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0360 â”‚ loss=1.6846 â”‚ recon=0.980 â”‚ kl=0.701 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0370 â”‚ loss=1.6509 â”‚ recon=1.009 â”‚ kl=0.663 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0380 â”‚ loss=1.6536 â”‚ recon=0.829 â”‚ kl=0.671 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0390 â”‚ loss=1.6394 â”‚ recon=0.979 â”‚ kl=0.668 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0400 â”‚ loss=1.6465 â”‚ recon=1.173 â”‚ kl=0.692 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0410 â”‚ loss=1.6302 â”‚ recon=0.853 â”‚ kl=0.659 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0420 â”‚ loss=1.6364 â”‚ recon=1.054 â”‚ kl=0.680 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0430 â”‚ loss=1.6379 â”‚ recon=0.894 â”‚ kl=0.680 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0440 â”‚ loss=1.6461 â”‚ recon=0.850 â”‚ kl=0.689 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0450 â”‚ loss=1.6204 â”‚ recon=1.001 â”‚ kl=0.655 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0460 â”‚ loss=1.6561 â”‚ recon=0.967 â”‚ kl=0.687 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0470 â”‚ loss=1.6288 â”‚ recon=1.137 â”‚ kl=0.713 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0480 â”‚ loss=1.6013 â”‚ recon=0.826 â”‚ kl=0.651 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0490 â”‚ loss=1.6303 â”‚ recon=0.746 â”‚ kl=0.667 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0500 â”‚ loss=1.6298 â”‚ recon=0.663 â”‚ kl=0.706 â”‚ Î²=1.00 â”‚ lr=2.5e-04
âœ…  Model saved to ./experiments/gat_hd64_ld32_nr3_ep500_nf3.0_dl1/model.pt
ðŸ“ˆ  Loss curve saved to vgae_loss.png
Novel: 100.00% (Baseline), 100.00% (VGAE)
Unique: 100.00% (Baseline), 99.50% (VGAE)
Novel and Unique: 100.00% (Baseline), 99.50% (VGAE)
Computing node-level statistics...
Computing node-level statistics...
Computing node-level statistics...

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 24883115: <graph_generations> in cluster <dcc> Done

Job <graph_generations> was submitted from host <n-62-20-1> by user <s185927> in cluster <dcc> at Sun May  4 10:42:31 2025
Job was executed on host(s) <8*n-62-20-11>, in queue <gpuv100>, as user <s185927> in cluster <dcc> at Sun May  4 10:42:31 2025
</zhome/e3/3/139772> was used as the home directory.
</zhome/e3/3/139772/Desktop/AML/AML/Module_3/AML-Project-3> was used as the working directory.
Started at Sun May  4 10:42:31 2025
Terminated at Sun May  4 11:06:19 2025
Results reported at Sun May  4 11:06:19 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J graph_generations
#BSUB -q gpuv100
#BSUB -n 8
#BSUB -o logs/%J.out
#BSUB -e logs/%J.err
#BSUB -gpu "num=1:mode=exclusive_process"
#BSUB -W 5:00
#BSUB -R "span[hosts=1]"
#BSUB -R "rusage[mem=5GB]"
# end of BSUB options

module load cuda/11.8

source ~/Desktop/AML/aml_new/bin/activate

python -u src/main.py --mode 'train' --epochs 500 --lr 5e-4 --hidden_dim 64 --latent_dim 32 --num_enc_MP_rounds 3 --decoder gat  --neg_factor 3 --dec_layers 1 --heads 4 



------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   1420.00 sec.
    Max Memory :                                 680 MB
    Average Memory :                             659.09 MB
    Total Requested Memory :                     40960.00 MB
    Delta Memory :                               40280.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                11
    Run time :                                   1544 sec.
    Turnaround time :                            1428 sec.

The output (if any) is above this job summary.



PS:

Read file <logs/24883115.err> for stderr output of this job.

