Experiment arguments: {'mode': 'train', 'epochs': 500, 'lr': 0.0005, 'beta': 5, 'neg_factor': 3.0, 'device': 'cuda', 'hidden_dim': 32, 'latent_dim': 16, 'num_enc_MP_rounds': 3, 'dec_layers': 2, 'heads': 4, 'decoder': 'gat', 'checkpoint': './experiments/gat_hd32_ld16_nr3_ep500_nf3.0_dl2/model.pt', 'log_dir': './experiments/gat_hd32_ld16_nr3_ep500_nf3.0_dl2/logs', 'fig_dir': './experiments/gat_hd32_ld16_nr3_ep500_nf3.0_dl2/figures'}
Loaded 171 training hashes.
cuda
Device: cuda
Training VGAE model...
Epoch 0001 │ loss=2.6936 │ recon=1.518 │ kl=0.716 │ β=0.02 │ lr=5.0e-04
Epoch 0002 │ loss=1.4545 │ recon=1.441 │ kl=1.028 │ β=0.04 │ lr=5.0e-04
Epoch 0003 │ loss=1.4561 │ recon=1.358 │ kl=1.091 │ β=0.06 │ lr=5.0e-04
Epoch 0004 │ loss=1.4751 │ recon=1.463 │ kl=1.107 │ β=0.08 │ lr=5.0e-04
Epoch 0005 │ loss=1.4873 │ recon=1.430 │ kl=1.068 │ β=0.10 │ lr=5.0e-04
Epoch 0006 │ loss=1.4929 │ recon=1.359 │ kl=1.026 │ β=0.12 │ lr=5.0e-04
Epoch 0007 │ loss=1.4551 │ recon=1.359 │ kl=1.039 │ β=0.14 │ lr=5.0e-04
Epoch 0008 │ loss=1.3956 │ recon=1.231 │ kl=0.990 │ β=0.16 │ lr=5.0e-04
Epoch 0009 │ loss=1.3758 │ recon=1.233 │ kl=0.956 │ β=0.18 │ lr=5.0e-04
Epoch 0010 │ loss=1.3761 │ recon=1.310 │ kl=0.894 │ β=0.20 │ lr=5.0e-04
Epoch 0011 │ loss=1.3776 │ recon=1.113 │ kl=0.858 │ β=0.22 │ lr=5.0e-04
Epoch 0012 │ loss=1.3869 │ recon=1.240 │ kl=0.804 │ β=0.24 │ lr=5.0e-04
Epoch 0013 │ loss=1.3846 │ recon=1.208 │ kl=0.805 │ β=0.26 │ lr=5.0e-04
Epoch 0014 │ loss=1.3803 │ recon=1.166 │ kl=0.778 │ β=0.28 │ lr=5.0e-04
Epoch 0015 │ loss=1.3752 │ recon=1.260 │ kl=0.749 │ β=0.30 │ lr=5.0e-04
Epoch 0016 │ loss=1.3636 │ recon=1.078 │ kl=0.762 │ β=0.32 │ lr=5.0e-04
Epoch 0017 │ loss=1.3692 │ recon=1.094 │ kl=0.747 │ β=0.34 │ lr=5.0e-04
Epoch 0018 │ loss=1.3506 │ recon=1.121 │ kl=0.715 │ β=0.36 │ lr=5.0e-04
Epoch 0019 │ loss=1.3674 │ recon=1.009 │ kl=0.738 │ β=0.38 │ lr=5.0e-04
Epoch 0020 │ loss=1.3804 │ recon=1.182 │ kl=0.654 │ β=0.40 │ lr=5.0e-04
Epoch 0021 │ loss=1.3937 │ recon=1.094 │ kl=0.671 │ β=0.42 │ lr=5.0e-04
Epoch 0022 │ loss=1.3807 │ recon=1.039 │ kl=0.650 │ β=0.44 │ lr=5.0e-04
Epoch 0023 │ loss=1.4035 │ recon=1.082 │ kl=0.662 │ β=0.46 │ lr=5.0e-04
Epoch 0024 │ loss=1.3872 │ recon=0.930 │ kl=0.647 │ β=0.48 │ lr=5.0e-04
Epoch 0025 │ loss=1.3929 │ recon=1.277 │ kl=0.619 │ β=0.50 │ lr=5.0e-04
Epoch 0026 │ loss=1.4007 │ recon=1.142 │ kl=0.590 │ β=0.52 │ lr=5.0e-04
Epoch 0027 │ loss=1.3970 │ recon=1.204 │ kl=0.612 │ β=0.54 │ lr=5.0e-04
Epoch 0028 │ loss=1.4128 │ recon=1.129 │ kl=0.578 │ β=0.56 │ lr=5.0e-04
Epoch 0029 │ loss=1.4164 │ recon=1.015 │ kl=0.590 │ β=0.58 │ lr=5.0e-04
Epoch 0030 │ loss=1.4302 │ recon=1.010 │ kl=0.550 │ β=0.60 │ lr=5.0e-04
Epoch 0031 │ loss=1.4335 │ recon=1.053 │ kl=0.560 │ β=0.62 │ lr=5.0e-04
Epoch 0032 │ loss=1.4399 │ recon=1.066 │ kl=0.527 │ β=0.64 │ lr=5.0e-04
Epoch 0033 │ loss=1.4355 │ recon=1.145 │ kl=0.528 │ β=0.66 │ lr=5.0e-04
Epoch 0034 │ loss=1.4387 │ recon=1.121 │ kl=0.524 │ β=0.68 │ lr=5.0e-04
Epoch 0035 │ loss=1.4625 │ recon=1.199 │ kl=0.526 │ β=0.70 │ lr=5.0e-04
Epoch 0036 │ loss=1.4360 │ recon=1.139 │ kl=0.485 │ β=0.72 │ lr=5.0e-04
Epoch 0037 │ loss=1.4643 │ recon=1.223 │ kl=0.510 │ β=0.74 │ lr=5.0e-04
Epoch 0038 │ loss=1.4661 │ recon=0.947 │ kl=0.486 │ β=0.76 │ lr=5.0e-04
Epoch 0039 │ loss=1.4776 │ recon=1.052 │ kl=0.492 │ β=0.78 │ lr=5.0e-04
Epoch 0040 │ loss=1.4759 │ recon=1.238 │ kl=0.462 │ β=0.80 │ lr=5.0e-04
Epoch 0041 │ loss=1.5075 │ recon=1.165 │ kl=0.460 │ β=0.82 │ lr=5.0e-04
Epoch 0042 │ loss=1.5002 │ recon=1.005 │ kl=0.479 │ β=0.84 │ lr=5.0e-04
Epoch 0043 │ loss=1.4853 │ recon=1.177 │ kl=0.450 │ β=0.86 │ lr=5.0e-04
Epoch 0044 │ loss=1.4941 │ recon=1.287 │ kl=0.452 │ β=0.88 │ lr=5.0e-04
Epoch 0045 │ loss=1.5044 │ recon=1.167 │ kl=0.447 │ β=0.90 │ lr=5.0e-04
Epoch 0046 │ loss=1.4921 │ recon=1.008 │ kl=0.420 │ β=0.92 │ lr=5.0e-04
Epoch 0047 │ loss=1.5078 │ recon=1.196 │ kl=0.427 │ β=0.94 │ lr=5.0e-04
Epoch 0048 │ loss=1.5170 │ recon=1.083 │ kl=0.449 │ β=0.96 │ lr=5.0e-04
Epoch 0049 │ loss=1.5453 │ recon=1.200 │ kl=0.432 │ β=0.98 │ lr=5.0e-04
Epoch 0050 │ loss=1.5310 │ recon=1.086 │ kl=0.416 │ β=1.00 │ lr=5.0e-04
Epoch 0051 │ loss=1.5333 │ recon=1.224 │ kl=0.419 │ β=1.00 │ lr=5.0e-04
Epoch 0052 │ loss=1.5425 │ recon=1.049 │ kl=0.418 │ β=1.00 │ lr=5.0e-04
Epoch 0053 │ loss=1.5211 │ recon=1.034 │ kl=0.407 │ β=1.00 │ lr=5.0e-04
Epoch 0054 │ loss=1.5142 │ recon=1.190 │ kl=0.416 │ β=1.00 │ lr=5.0e-04
Epoch 0055 │ loss=1.5258 │ recon=1.063 │ kl=0.396 │ β=1.00 │ lr=5.0e-04
Epoch 0056 │ loss=1.5156 │ recon=1.309 │ kl=0.411 │ β=1.00 │ lr=5.0e-04
Epoch 0057 │ loss=1.5222 │ recon=1.201 │ kl=0.427 │ β=1.00 │ lr=5.0e-04
Epoch 0058 │ loss=1.5158 │ recon=1.015 │ kl=0.386 │ β=1.00 │ lr=5.0e-04
Epoch 0059 │ loss=1.5258 │ recon=1.245 │ kl=0.401 │ β=1.00 │ lr=5.0e-04
Epoch 0060 │ loss=1.4981 │ recon=1.054 │ kl=0.388 │ β=1.00 │ lr=5.0e-04
Epoch 0061 │ loss=1.5194 │ recon=1.094 │ kl=0.417 │ β=1.00 │ lr=5.0e-04
Epoch 0062 │ loss=1.4998 │ recon=1.269 │ kl=0.381 │ β=1.00 │ lr=5.0e-04
Epoch 0063 │ loss=1.5223 │ recon=1.475 │ kl=0.397 │ β=1.00 │ lr=5.0e-04
Epoch 0064 │ loss=1.4999 │ recon=1.159 │ kl=0.398 │ β=1.00 │ lr=5.0e-04
Epoch 0065 │ loss=1.5022 │ recon=1.064 │ kl=0.402 │ β=1.00 │ lr=5.0e-04
Epoch 0066 │ loss=1.5060 │ recon=1.227 │ kl=0.393 │ β=1.00 │ lr=5.0e-04
Epoch 0067 │ loss=1.5194 │ recon=1.093 │ kl=0.398 │ β=1.00 │ lr=5.0e-04
Epoch 0068 │ loss=1.5017 │ recon=1.116 │ kl=0.397 │ β=1.00 │ lr=5.0e-04
Epoch 0069 │ loss=1.4992 │ recon=1.232 │ kl=0.401 │ β=1.00 │ lr=2.5e-04
Epoch 0070 │ loss=1.4947 │ recon=0.826 │ kl=0.410 │ β=1.00 │ lr=2.5e-04
Epoch 0071 │ loss=1.4798 │ recon=1.139 │ kl=0.411 │ β=1.00 │ lr=2.5e-04
Epoch 0072 │ loss=1.4776 │ recon=1.052 │ kl=0.381 │ β=1.00 │ lr=2.5e-04
Epoch 0073 │ loss=1.4783 │ recon=1.210 │ kl=0.397 │ β=1.00 │ lr=2.5e-04
Epoch 0074 │ loss=1.4905 │ recon=1.152 │ kl=0.396 │ β=1.00 │ lr=2.5e-04
Epoch 0075 │ loss=1.4743 │ recon=1.067 │ kl=0.379 │ β=1.00 │ lr=2.5e-04
Epoch 0076 │ loss=1.4786 │ recon=0.903 │ kl=0.392 │ β=1.00 │ lr=2.5e-04
Epoch 0077 │ loss=1.4834 │ recon=1.396 │ kl=0.378 │ β=1.00 │ lr=2.5e-04
Epoch 0078 │ loss=1.4914 │ recon=1.204 │ kl=0.406 │ β=1.00 │ lr=2.5e-04
Epoch 0079 │ loss=1.4876 │ recon=1.167 │ kl=0.395 │ β=1.00 │ lr=2.5e-04
Epoch 0080 │ loss=1.4779 │ recon=1.248 │ kl=0.394 │ β=1.00 │ lr=2.5e-04
Epoch 0081 │ loss=1.4749 │ recon=1.016 │ kl=0.398 │ β=1.00 │ lr=2.5e-04
Epoch 0082 │ loss=1.4678 │ recon=1.321 │ kl=0.377 │ β=1.00 │ lr=2.5e-04
Epoch 0083 │ loss=1.5042 │ recon=0.967 │ kl=0.393 │ β=1.00 │ lr=2.5e-04
Epoch 0084 │ loss=1.4878 │ recon=0.972 │ kl=0.399 │ β=1.00 │ lr=2.5e-04
Epoch 0085 │ loss=1.4722 │ recon=1.033 │ kl=0.390 │ β=1.00 │ lr=2.5e-04
Epoch 0086 │ loss=1.4598 │ recon=1.273 │ kl=0.386 │ β=1.00 │ lr=2.5e-04
Epoch 0087 │ loss=1.4726 │ recon=1.239 │ kl=0.386 │ β=1.00 │ lr=2.5e-04
Epoch 0088 │ loss=1.4699 │ recon=1.038 │ kl=0.396 │ β=1.00 │ lr=2.5e-04
Epoch 0089 │ loss=1.4963 │ recon=1.209 │ kl=0.399 │ β=1.00 │ lr=2.5e-04
Epoch 0090 │ loss=1.4728 │ recon=0.945 │ kl=0.384 │ β=1.00 │ lr=2.5e-04
Epoch 0091 │ loss=1.4760 │ recon=1.171 │ kl=0.390 │ β=1.00 │ lr=2.5e-04
Epoch 0092 │ loss=1.4573 │ recon=0.963 │ kl=0.384 │ β=1.00 │ lr=2.5e-04
Epoch 0093 │ loss=1.5033 │ recon=1.073 │ kl=0.401 │ β=1.00 │ lr=2.5e-04
Epoch 0094 │ loss=1.4537 │ recon=1.528 │ kl=0.373 │ β=1.00 │ lr=2.5e-04
Epoch 0095 │ loss=1.4517 │ recon=1.179 │ kl=0.374 │ β=1.00 │ lr=2.5e-04
Epoch 0096 │ loss=1.4848 │ recon=1.040 │ kl=0.393 │ β=1.00 │ lr=2.5e-04
Epoch 0097 │ loss=1.4425 │ recon=0.866 │ kl=0.386 │ β=1.00 │ lr=2.5e-04
Epoch 0098 │ loss=1.4772 │ recon=0.990 │ kl=0.389 │ β=1.00 │ lr=2.5e-04
Epoch 0099 │ loss=1.4739 │ recon=1.078 │ kl=0.390 │ β=1.00 │ lr=2.5e-04
Epoch 0100 │ loss=1.4806 │ recon=1.179 │ kl=0.386 │ β=1.00 │ lr=2.5e-04
Epoch 0101 │ loss=1.4763 │ recon=1.125 │ kl=0.401 │ β=1.00 │ lr=2.5e-04
Epoch 0102 │ loss=1.5014 │ recon=0.834 │ kl=0.414 │ β=1.00 │ lr=2.5e-04
Epoch 0103 │ loss=1.4697 │ recon=1.130 │ kl=0.376 │ β=1.00 │ lr=2.5e-04
Epoch 0104 │ loss=1.4825 │ recon=1.315 │ kl=0.387 │ β=1.00 │ lr=2.5e-04
Epoch 0105 │ loss=1.4736 │ recon=1.129 │ kl=0.384 │ β=1.00 │ lr=2.5e-04
Epoch 0106 │ loss=1.4698 │ recon=1.286 │ kl=0.399 │ β=1.00 │ lr=2.5e-04
Epoch 0107 │ loss=1.4795 │ recon=1.096 │ kl=0.392 │ β=1.00 │ lr=2.5e-04
Epoch 0108 │ loss=1.4603 │ recon=0.824 │ kl=0.386 │ β=1.00 │ lr=2.5e-04
Epoch 0109 │ loss=1.4661 │ recon=0.994 │ kl=0.392 │ β=1.00 │ lr=2.5e-04
Epoch 0110 │ loss=1.4723 │ recon=1.269 │ kl=0.375 │ β=1.00 │ lr=2.5e-04
Epoch 0111 │ loss=1.4479 │ recon=1.244 │ kl=0.380 │ β=1.00 │ lr=2.5e-04
Epoch 0112 │ loss=1.4673 │ recon=0.944 │ kl=0.397 │ β=1.00 │ lr=2.5e-04
Epoch 0113 │ loss=1.4545 │ recon=1.283 │ kl=0.386 │ β=1.00 │ lr=2.5e-04
Epoch 0114 │ loss=1.4719 │ recon=1.145 │ kl=0.382 │ β=1.00 │ lr=2.5e-04
Epoch 0115 │ loss=1.4651 │ recon=1.024 │ kl=0.391 │ β=1.00 │ lr=2.5e-04
Epoch 0116 │ loss=1.4479 │ recon=1.176 │ kl=0.384 │ β=1.00 │ lr=2.5e-04
Epoch 0117 │ loss=1.4890 │ recon=1.288 │ kl=0.391 │ β=1.00 │ lr=2.5e-04
Epoch 0118 │ loss=1.4558 │ recon=1.077 │ kl=0.386 │ β=1.00 │ lr=2.5e-04
⏹️  Early stopping at epoch 118 (no improvement for 100 epochs).
✅  Model saved to ./experiments/gat_hd32_ld16_nr3_ep500_nf3.0_dl2/model.pt
📈  Loss curve saved to vgae_loss.png
Novel: 100.00% (Baseline), 100.00% (VGAE)
Unique: 100.00% (Baseline), 98.70% (VGAE)
Novel and Unique: 100.00% (Baseline), 98.70% (VGAE)
Computing node-level statistics...
Computing node-level statistics...
Computing node-level statistics...

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 24864426: <graph_generations> in cluster <dcc> Done

Job <graph_generations> was submitted from host <n-62-20-9> by user <s185927> in cluster <dcc> at Fri May  2 16:27:08 2025
Job was executed on host(s) <8*n-62-20-2>, in queue <gpuv100>, as user <s185927> in cluster <dcc> at Fri May  2 16:29:16 2025
</zhome/e3/3/139772> was used as the home directory.
</zhome/e3/3/139772/Desktop/AML/AML/Module_3/AML-Project-3> was used as the working directory.
Started at Fri May  2 16:29:16 2025
Terminated at Fri May  2 16:35:11 2025
Results reported at Fri May  2 16:35:11 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J graph_generations
#BSUB -q gpuv100
#BSUB -n 8
#BSUB -o logs/%J.out
#BSUB -e logs/%J.err
#BSUB -gpu "num=1:mode=exclusive_process"
#BSUB -W 5:00
#BSUB -R "span[hosts=1]"
#BSUB -R "rusage[mem=5GB]"
# end of BSUB options

module load cuda/11.8

source ~/Desktop/AML/aml_new/bin/activate

python -u src/main.py --mode 'train' --epochs 500 --lr 5e-4 --hidden_dim 32 --latent_dim 16 --num_enc_MP_rounds 3 --decoder gat  --neg_factor 3 --dec_layers 2 --heads 4 



------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   349.52 sec.
    Max Memory :                                 768 MB
    Average Memory :                             732.20 MB
    Total Requested Memory :                     40960.00 MB
    Delta Memory :                               40192.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   356 sec.
    Turnaround time :                            483 sec.

The output (if any) is above this job summary.



PS:

Read file <logs/24864426.err> for stderr output of this job.

