Experiment arguments: {'mode': 'train', 'epochs': 500, 'lr': 0.0005, 'beta': 5, 'neg_factor': 3.0, 'device': 'cuda', 'hidden_dim': 32, 'latent_dim': 16, 'num_enc_MP_rounds': 3, 'dec_layers': 2, 'heads': 4, 'decoder': 'gat', 'checkpoint': './experiments/gat_hd32_ld16_nr3_ep500_nf3.0_dl2/model.pt', 'log_dir': './experiments/gat_hd32_ld16_nr3_ep500_nf3.0_dl2/logs', 'fig_dir': './experiments/gat_hd32_ld16_nr3_ep500_nf3.0_dl2/figures'}
Loaded 171 training hashes.
cuda
Device: cuda
Training VGAE model...
Epoch 0001 â”‚ loss=2.6936 â”‚ recon=1.518 â”‚ kl=0.716 â”‚ Î²=0.02 â”‚ lr=5.0e-04
Epoch 0002 â”‚ loss=1.4545 â”‚ recon=1.441 â”‚ kl=1.028 â”‚ Î²=0.04 â”‚ lr=5.0e-04
Epoch 0003 â”‚ loss=1.4561 â”‚ recon=1.358 â”‚ kl=1.091 â”‚ Î²=0.06 â”‚ lr=5.0e-04
Epoch 0004 â”‚ loss=1.4751 â”‚ recon=1.463 â”‚ kl=1.107 â”‚ Î²=0.08 â”‚ lr=5.0e-04
Epoch 0005 â”‚ loss=1.4873 â”‚ recon=1.430 â”‚ kl=1.068 â”‚ Î²=0.10 â”‚ lr=5.0e-04
Epoch 0006 â”‚ loss=1.4929 â”‚ recon=1.359 â”‚ kl=1.026 â”‚ Î²=0.12 â”‚ lr=5.0e-04
Epoch 0007 â”‚ loss=1.4551 â”‚ recon=1.359 â”‚ kl=1.039 â”‚ Î²=0.14 â”‚ lr=5.0e-04
Epoch 0008 â”‚ loss=1.3956 â”‚ recon=1.231 â”‚ kl=0.990 â”‚ Î²=0.16 â”‚ lr=5.0e-04
Epoch 0009 â”‚ loss=1.3758 â”‚ recon=1.233 â”‚ kl=0.956 â”‚ Î²=0.18 â”‚ lr=5.0e-04
Epoch 0010 â”‚ loss=1.3761 â”‚ recon=1.310 â”‚ kl=0.894 â”‚ Î²=0.20 â”‚ lr=5.0e-04
Epoch 0011 â”‚ loss=1.3776 â”‚ recon=1.113 â”‚ kl=0.858 â”‚ Î²=0.22 â”‚ lr=5.0e-04
Epoch 0012 â”‚ loss=1.3869 â”‚ recon=1.240 â”‚ kl=0.804 â”‚ Î²=0.24 â”‚ lr=5.0e-04
Epoch 0013 â”‚ loss=1.3846 â”‚ recon=1.208 â”‚ kl=0.805 â”‚ Î²=0.26 â”‚ lr=5.0e-04
Epoch 0014 â”‚ loss=1.3803 â”‚ recon=1.166 â”‚ kl=0.778 â”‚ Î²=0.28 â”‚ lr=5.0e-04
Epoch 0015 â”‚ loss=1.3752 â”‚ recon=1.260 â”‚ kl=0.749 â”‚ Î²=0.30 â”‚ lr=5.0e-04
Epoch 0016 â”‚ loss=1.3636 â”‚ recon=1.078 â”‚ kl=0.762 â”‚ Î²=0.32 â”‚ lr=5.0e-04
Epoch 0017 â”‚ loss=1.3692 â”‚ recon=1.094 â”‚ kl=0.747 â”‚ Î²=0.34 â”‚ lr=5.0e-04
Epoch 0018 â”‚ loss=1.3506 â”‚ recon=1.121 â”‚ kl=0.715 â”‚ Î²=0.36 â”‚ lr=5.0e-04
Epoch 0019 â”‚ loss=1.3674 â”‚ recon=1.009 â”‚ kl=0.738 â”‚ Î²=0.38 â”‚ lr=5.0e-04
Epoch 0020 â”‚ loss=1.3804 â”‚ recon=1.182 â”‚ kl=0.654 â”‚ Î²=0.40 â”‚ lr=5.0e-04
Epoch 0021 â”‚ loss=1.3937 â”‚ recon=1.094 â”‚ kl=0.671 â”‚ Î²=0.42 â”‚ lr=5.0e-04
Epoch 0022 â”‚ loss=1.3807 â”‚ recon=1.039 â”‚ kl=0.650 â”‚ Î²=0.44 â”‚ lr=5.0e-04
Epoch 0023 â”‚ loss=1.4035 â”‚ recon=1.082 â”‚ kl=0.662 â”‚ Î²=0.46 â”‚ lr=5.0e-04
Epoch 0024 â”‚ loss=1.3872 â”‚ recon=0.930 â”‚ kl=0.647 â”‚ Î²=0.48 â”‚ lr=5.0e-04
Epoch 0025 â”‚ loss=1.3929 â”‚ recon=1.277 â”‚ kl=0.619 â”‚ Î²=0.50 â”‚ lr=5.0e-04
Epoch 0026 â”‚ loss=1.4007 â”‚ recon=1.142 â”‚ kl=0.590 â”‚ Î²=0.52 â”‚ lr=5.0e-04
Epoch 0027 â”‚ loss=1.3970 â”‚ recon=1.204 â”‚ kl=0.612 â”‚ Î²=0.54 â”‚ lr=5.0e-04
Epoch 0028 â”‚ loss=1.4128 â”‚ recon=1.129 â”‚ kl=0.578 â”‚ Î²=0.56 â”‚ lr=5.0e-04
Epoch 0029 â”‚ loss=1.4164 â”‚ recon=1.015 â”‚ kl=0.590 â”‚ Î²=0.58 â”‚ lr=5.0e-04
Epoch 0030 â”‚ loss=1.4302 â”‚ recon=1.010 â”‚ kl=0.550 â”‚ Î²=0.60 â”‚ lr=5.0e-04
Epoch 0031 â”‚ loss=1.4335 â”‚ recon=1.053 â”‚ kl=0.560 â”‚ Î²=0.62 â”‚ lr=5.0e-04
Epoch 0032 â”‚ loss=1.4399 â”‚ recon=1.066 â”‚ kl=0.527 â”‚ Î²=0.64 â”‚ lr=5.0e-04
Epoch 0033 â”‚ loss=1.4355 â”‚ recon=1.145 â”‚ kl=0.528 â”‚ Î²=0.66 â”‚ lr=5.0e-04
Epoch 0034 â”‚ loss=1.4387 â”‚ recon=1.121 â”‚ kl=0.524 â”‚ Î²=0.68 â”‚ lr=5.0e-04
Epoch 0035 â”‚ loss=1.4625 â”‚ recon=1.199 â”‚ kl=0.526 â”‚ Î²=0.70 â”‚ lr=5.0e-04
Epoch 0036 â”‚ loss=1.4360 â”‚ recon=1.139 â”‚ kl=0.485 â”‚ Î²=0.72 â”‚ lr=5.0e-04
Epoch 0037 â”‚ loss=1.4643 â”‚ recon=1.223 â”‚ kl=0.510 â”‚ Î²=0.74 â”‚ lr=5.0e-04
Epoch 0038 â”‚ loss=1.4661 â”‚ recon=0.947 â”‚ kl=0.486 â”‚ Î²=0.76 â”‚ lr=5.0e-04
Epoch 0039 â”‚ loss=1.4776 â”‚ recon=1.052 â”‚ kl=0.492 â”‚ Î²=0.78 â”‚ lr=5.0e-04
Epoch 0040 â”‚ loss=1.4759 â”‚ recon=1.238 â”‚ kl=0.462 â”‚ Î²=0.80 â”‚ lr=5.0e-04
Epoch 0041 â”‚ loss=1.5075 â”‚ recon=1.165 â”‚ kl=0.460 â”‚ Î²=0.82 â”‚ lr=5.0e-04
Epoch 0042 â”‚ loss=1.5002 â”‚ recon=1.005 â”‚ kl=0.479 â”‚ Î²=0.84 â”‚ lr=5.0e-04
Epoch 0043 â”‚ loss=1.4853 â”‚ recon=1.177 â”‚ kl=0.450 â”‚ Î²=0.86 â”‚ lr=5.0e-04
Epoch 0044 â”‚ loss=1.4941 â”‚ recon=1.287 â”‚ kl=0.452 â”‚ Î²=0.88 â”‚ lr=5.0e-04
Epoch 0045 â”‚ loss=1.5044 â”‚ recon=1.167 â”‚ kl=0.447 â”‚ Î²=0.90 â”‚ lr=5.0e-04
Epoch 0046 â”‚ loss=1.4921 â”‚ recon=1.008 â”‚ kl=0.420 â”‚ Î²=0.92 â”‚ lr=5.0e-04
Epoch 0047 â”‚ loss=1.5078 â”‚ recon=1.196 â”‚ kl=0.427 â”‚ Î²=0.94 â”‚ lr=5.0e-04
Epoch 0048 â”‚ loss=1.5170 â”‚ recon=1.083 â”‚ kl=0.449 â”‚ Î²=0.96 â”‚ lr=5.0e-04
Epoch 0049 â”‚ loss=1.5453 â”‚ recon=1.200 â”‚ kl=0.432 â”‚ Î²=0.98 â”‚ lr=5.0e-04
Epoch 0050 â”‚ loss=1.5310 â”‚ recon=1.086 â”‚ kl=0.416 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0051 â”‚ loss=1.5333 â”‚ recon=1.224 â”‚ kl=0.419 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0052 â”‚ loss=1.5425 â”‚ recon=1.049 â”‚ kl=0.418 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0053 â”‚ loss=1.5211 â”‚ recon=1.034 â”‚ kl=0.407 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0054 â”‚ loss=1.5142 â”‚ recon=1.190 â”‚ kl=0.416 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0055 â”‚ loss=1.5258 â”‚ recon=1.063 â”‚ kl=0.396 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0056 â”‚ loss=1.5156 â”‚ recon=1.309 â”‚ kl=0.411 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0057 â”‚ loss=1.5222 â”‚ recon=1.201 â”‚ kl=0.427 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0058 â”‚ loss=1.5158 â”‚ recon=1.015 â”‚ kl=0.386 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0059 â”‚ loss=1.5258 â”‚ recon=1.245 â”‚ kl=0.401 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0060 â”‚ loss=1.4981 â”‚ recon=1.054 â”‚ kl=0.388 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0061 â”‚ loss=1.5194 â”‚ recon=1.094 â”‚ kl=0.417 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0062 â”‚ loss=1.4998 â”‚ recon=1.269 â”‚ kl=0.381 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0063 â”‚ loss=1.5223 â”‚ recon=1.475 â”‚ kl=0.397 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0064 â”‚ loss=1.4999 â”‚ recon=1.159 â”‚ kl=0.398 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0065 â”‚ loss=1.5022 â”‚ recon=1.064 â”‚ kl=0.402 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0066 â”‚ loss=1.5060 â”‚ recon=1.227 â”‚ kl=0.393 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0067 â”‚ loss=1.5194 â”‚ recon=1.093 â”‚ kl=0.398 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0068 â”‚ loss=1.5017 â”‚ recon=1.116 â”‚ kl=0.397 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0069 â”‚ loss=1.4992 â”‚ recon=1.232 â”‚ kl=0.401 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0070 â”‚ loss=1.4947 â”‚ recon=0.826 â”‚ kl=0.410 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0071 â”‚ loss=1.4798 â”‚ recon=1.139 â”‚ kl=0.411 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0072 â”‚ loss=1.4776 â”‚ recon=1.052 â”‚ kl=0.381 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0073 â”‚ loss=1.4783 â”‚ recon=1.210 â”‚ kl=0.397 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0074 â”‚ loss=1.4905 â”‚ recon=1.152 â”‚ kl=0.396 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0075 â”‚ loss=1.4743 â”‚ recon=1.067 â”‚ kl=0.379 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0076 â”‚ loss=1.4786 â”‚ recon=0.903 â”‚ kl=0.392 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0077 â”‚ loss=1.4834 â”‚ recon=1.396 â”‚ kl=0.378 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0078 â”‚ loss=1.4914 â”‚ recon=1.204 â”‚ kl=0.406 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0079 â”‚ loss=1.4876 â”‚ recon=1.167 â”‚ kl=0.395 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0080 â”‚ loss=1.4779 â”‚ recon=1.248 â”‚ kl=0.394 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0081 â”‚ loss=1.4749 â”‚ recon=1.016 â”‚ kl=0.398 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0082 â”‚ loss=1.4678 â”‚ recon=1.321 â”‚ kl=0.377 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0083 â”‚ loss=1.5042 â”‚ recon=0.967 â”‚ kl=0.393 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0084 â”‚ loss=1.4878 â”‚ recon=0.972 â”‚ kl=0.399 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0085 â”‚ loss=1.4722 â”‚ recon=1.033 â”‚ kl=0.390 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0086 â”‚ loss=1.4598 â”‚ recon=1.273 â”‚ kl=0.386 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0087 â”‚ loss=1.4726 â”‚ recon=1.239 â”‚ kl=0.386 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0088 â”‚ loss=1.4699 â”‚ recon=1.038 â”‚ kl=0.396 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0089 â”‚ loss=1.4963 â”‚ recon=1.209 â”‚ kl=0.399 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0090 â”‚ loss=1.4728 â”‚ recon=0.945 â”‚ kl=0.384 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0091 â”‚ loss=1.4760 â”‚ recon=1.171 â”‚ kl=0.390 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0092 â”‚ loss=1.4573 â”‚ recon=0.963 â”‚ kl=0.384 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0093 â”‚ loss=1.5033 â”‚ recon=1.073 â”‚ kl=0.401 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0094 â”‚ loss=1.4537 â”‚ recon=1.528 â”‚ kl=0.373 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0095 â”‚ loss=1.4517 â”‚ recon=1.179 â”‚ kl=0.374 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0096 â”‚ loss=1.4848 â”‚ recon=1.040 â”‚ kl=0.393 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0097 â”‚ loss=1.4425 â”‚ recon=0.866 â”‚ kl=0.386 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0098 â”‚ loss=1.4772 â”‚ recon=0.990 â”‚ kl=0.389 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0099 â”‚ loss=1.4739 â”‚ recon=1.078 â”‚ kl=0.390 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0100 â”‚ loss=1.4806 â”‚ recon=1.179 â”‚ kl=0.386 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0101 â”‚ loss=1.4763 â”‚ recon=1.125 â”‚ kl=0.401 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0102 â”‚ loss=1.5014 â”‚ recon=0.834 â”‚ kl=0.414 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0103 â”‚ loss=1.4697 â”‚ recon=1.130 â”‚ kl=0.376 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0104 â”‚ loss=1.4825 â”‚ recon=1.315 â”‚ kl=0.387 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0105 â”‚ loss=1.4736 â”‚ recon=1.129 â”‚ kl=0.384 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0106 â”‚ loss=1.4698 â”‚ recon=1.286 â”‚ kl=0.399 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0107 â”‚ loss=1.4795 â”‚ recon=1.096 â”‚ kl=0.392 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0108 â”‚ loss=1.4603 â”‚ recon=0.824 â”‚ kl=0.386 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0109 â”‚ loss=1.4661 â”‚ recon=0.994 â”‚ kl=0.392 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0110 â”‚ loss=1.4723 â”‚ recon=1.269 â”‚ kl=0.375 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0111 â”‚ loss=1.4479 â”‚ recon=1.244 â”‚ kl=0.380 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0112 â”‚ loss=1.4673 â”‚ recon=0.944 â”‚ kl=0.397 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0113 â”‚ loss=1.4545 â”‚ recon=1.283 â”‚ kl=0.386 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0114 â”‚ loss=1.4719 â”‚ recon=1.145 â”‚ kl=0.382 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0115 â”‚ loss=1.4651 â”‚ recon=1.024 â”‚ kl=0.391 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0116 â”‚ loss=1.4479 â”‚ recon=1.176 â”‚ kl=0.384 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0117 â”‚ loss=1.4890 â”‚ recon=1.288 â”‚ kl=0.391 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0118 â”‚ loss=1.4558 â”‚ recon=1.077 â”‚ kl=0.386 â”‚ Î²=1.00 â”‚ lr=2.5e-04
â¹ï¸  Early stopping at epoch 118 (no improvement for 100 epochs).
âœ…  Model saved to ./experiments/gat_hd32_ld16_nr3_ep500_nf3.0_dl2/model.pt
ğŸ“ˆ  Loss curve saved to vgae_loss.png
Novel: 100.00% (Baseline), 100.00% (VGAE)
Unique: 100.00% (Baseline), 98.70% (VGAE)
Novel and Unique: 100.00% (Baseline), 98.70% (VGAE)
Computing node-level statistics...
Computing node-level statistics...
Computing node-level statistics...

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 24864426: <graph_generations> in cluster <dcc> Done

Job <graph_generations> was submitted from host <n-62-20-9> by user <s185927> in cluster <dcc> at Fri May  2 16:27:08 2025
Job was executed on host(s) <8*n-62-20-2>, in queue <gpuv100>, as user <s185927> in cluster <dcc> at Fri May  2 16:29:16 2025
</zhome/e3/3/139772> was used as the home directory.
</zhome/e3/3/139772/Desktop/AML/AML/Module_3/AML-Project-3> was used as the working directory.
Started at Fri May  2 16:29:16 2025
Terminated at Fri May  2 16:35:11 2025
Results reported at Fri May  2 16:35:11 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J graph_generations
#BSUB -q gpuv100
#BSUB -n 8
#BSUB -o logs/%J.out
#BSUB -e logs/%J.err
#BSUB -gpu "num=1:mode=exclusive_process"
#BSUB -W 5:00
#BSUB -R "span[hosts=1]"
#BSUB -R "rusage[mem=5GB]"
# end of BSUB options

module load cuda/11.8

source ~/Desktop/AML/aml_new/bin/activate

python -u src/main.py --mode 'train' --epochs 500 --lr 5e-4 --hidden_dim 32 --latent_dim 16 --num_enc_MP_rounds 3 --decoder gat  --neg_factor 3 --dec_layers 2 --heads 4 



------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   349.52 sec.
    Max Memory :                                 768 MB
    Average Memory :                             732.20 MB
    Total Requested Memory :                     40960.00 MB
    Delta Memory :                               40192.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   356 sec.
    Turnaround time :                            483 sec.

The output (if any) is above this job summary.



PS:

Read file <logs/24864426.err> for stderr output of this job.

