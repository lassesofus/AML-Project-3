Experiment arguments: {'mode': 'train', 'epochs': 500, 'beta': 1.0, 'neg_factor': 1.0, 'device': None, 'hidden_dim': 64, 'latent_dim': 32, 'num_rounds': 5, 'decoder': 'gnn', 'checkpoint': './experiments/gnn_hd64_ld32_nr5_ep500_b1.0_nf1.0/model.pt', 'log_dir': './experiments/gnn_hd64_ld32_nr5_ep500_b1.0_nf1.0/logs', 'fig_dir': './experiments/gnn_hd64_ld32_nr5_ep500_b1.0_nf1.0/figures'}
Loaded 171 training hashes.
Epoch 0001 â”‚ loss=2.4256 â”‚ lr=1.0e-03
Epoch 0005 â”‚ loss=1.5366 â”‚ lr=1.0e-03
Epoch 0010 â”‚ loss=1.4791 â”‚ lr=1.0e-03
Epoch 0015 â”‚ loss=1.4502 â”‚ lr=1.0e-03
Epoch 0020 â”‚ loss=1.4397 â”‚ lr=1.0e-03
Epoch 0025 â”‚ loss=1.4271 â”‚ lr=1.0e-03
Epoch 0030 â”‚ loss=1.4197 â”‚ lr=1.0e-03
Epoch 0035 â”‚ loss=1.4167 â”‚ lr=1.0e-03
Epoch 0040 â”‚ loss=1.4142 â”‚ lr=1.0e-03
Epoch 0045 â”‚ loss=1.4064 â”‚ lr=1.0e-03
Epoch 0050 â”‚ loss=1.4036 â”‚ lr=1.0e-03
Epoch 0055 â”‚ loss=1.3999 â”‚ lr=1.0e-03
Epoch 0060 â”‚ loss=1.4059 â”‚ lr=1.0e-03
Epoch 0065 â”‚ loss=1.2571 â”‚ lr=1.0e-03
Epoch 0070 â”‚ loss=1.2274 â”‚ lr=1.0e-03
Epoch 0075 â”‚ loss=1.1934 â”‚ lr=1.0e-03
Epoch 0080 â”‚ loss=1.1382 â”‚ lr=1.0e-03
Epoch 0085 â”‚ loss=1.0969 â”‚ lr=1.0e-03
Epoch 0090 â”‚ loss=1.0568 â”‚ lr=1.0e-03
Epoch 0095 â”‚ loss=1.0298 â”‚ lr=1.0e-03
Epoch 0100 â”‚ loss=1.0328 â”‚ lr=5.0e-04
Epoch 0105 â”‚ loss=1.0014 â”‚ lr=5.0e-04
Epoch 0110 â”‚ loss=0.9980 â”‚ lr=5.0e-04
Epoch 0115 â”‚ loss=1.0041 â”‚ lr=5.0e-04
Epoch 0120 â”‚ loss=1.0031 â”‚ lr=5.0e-04
Epoch 0125 â”‚ loss=0.9923 â”‚ lr=5.0e-04
Epoch 0130 â”‚ loss=0.9938 â”‚ lr=5.0e-04
Epoch 0135 â”‚ loss=0.9956 â”‚ lr=5.0e-04
Epoch 0140 â”‚ loss=0.9882 â”‚ lr=5.0e-04
Epoch 0145 â”‚ loss=0.9826 â”‚ lr=5.0e-04
Epoch 0150 â”‚ loss=0.9892 â”‚ lr=5.0e-04
Epoch 0155 â”‚ loss=0.9771 â”‚ lr=5.0e-04
Epoch 0160 â”‚ loss=0.9750 â”‚ lr=5.0e-04
Epoch 0165 â”‚ loss=0.9791 â”‚ lr=5.0e-04
Epoch 0170 â”‚ loss=0.9713 â”‚ lr=5.0e-04
Epoch 0175 â”‚ loss=0.9751 â”‚ lr=5.0e-04
Epoch 0180 â”‚ loss=0.9798 â”‚ lr=5.0e-04
Epoch 0185 â”‚ loss=0.9762 â”‚ lr=5.0e-04
Epoch 0190 â”‚ loss=0.9644 â”‚ lr=5.0e-04
Epoch 0195 â”‚ loss=0.9716 â”‚ lr=5.0e-04
Epoch 0200 â”‚ loss=0.9697 â”‚ lr=2.5e-04
Epoch 0205 â”‚ loss=0.9543 â”‚ lr=2.5e-04
Epoch 0210 â”‚ loss=0.9544 â”‚ lr=2.5e-04
Epoch 0215 â”‚ loss=0.9602 â”‚ lr=2.5e-04
Epoch 0220 â”‚ loss=0.9600 â”‚ lr=2.5e-04
Epoch 0225 â”‚ loss=0.9697 â”‚ lr=2.5e-04
Epoch 0230 â”‚ loss=0.9545 â”‚ lr=2.5e-04
Epoch 0235 â”‚ loss=0.9622 â”‚ lr=2.5e-04
Epoch 0240 â”‚ loss=0.9495 â”‚ lr=2.5e-04
Epoch 0245 â”‚ loss=0.9618 â”‚ lr=2.5e-04
Epoch 0250 â”‚ loss=0.9472 â”‚ lr=2.5e-04
Epoch 0255 â”‚ loss=0.9574 â”‚ lr=2.5e-04
Epoch 0260 â”‚ loss=0.9656 â”‚ lr=2.5e-04
Epoch 0265 â”‚ loss=0.9657 â”‚ lr=2.5e-04
Epoch 0270 â”‚ loss=0.9388 â”‚ lr=2.5e-04
Epoch 0275 â”‚ loss=0.9544 â”‚ lr=2.5e-04
Epoch 0280 â”‚ loss=0.9641 â”‚ lr=2.5e-04
Epoch 0285 â”‚ loss=0.9561 â”‚ lr=2.5e-04
Epoch 0290 â”‚ loss=0.9526 â”‚ lr=2.5e-04
Epoch 0295 â”‚ loss=0.9627 â”‚ lr=2.5e-04
Epoch 0300 â”‚ loss=0.9471 â”‚ lr=1.3e-04
Epoch 0305 â”‚ loss=0.9547 â”‚ lr=1.3e-04
Epoch 0310 â”‚ loss=0.9469 â”‚ lr=1.3e-04
Epoch 0315 â”‚ loss=0.9529 â”‚ lr=1.3e-04
Epoch 0320 â”‚ loss=0.9557 â”‚ lr=1.3e-04
Epoch 0325 â”‚ loss=0.9539 â”‚ lr=1.3e-04
Epoch 0330 â”‚ loss=0.9462 â”‚ lr=1.3e-04
Epoch 0335 â”‚ loss=0.9535 â”‚ lr=1.3e-04
Epoch 0340 â”‚ loss=0.9568 â”‚ lr=1.3e-04
Epoch 0345 â”‚ loss=0.9535 â”‚ lr=1.3e-04
Epoch 0350 â”‚ loss=0.9478 â”‚ lr=1.3e-04
Epoch 0355 â”‚ loss=0.9576 â”‚ lr=1.3e-04
Epoch 0360 â”‚ loss=0.9415 â”‚ lr=1.3e-04
Epoch 0365 â”‚ loss=0.9520 â”‚ lr=1.3e-04
Epoch 0370 â”‚ loss=0.9404 â”‚ lr=1.3e-04
Epoch 0375 â”‚ loss=0.9550 â”‚ lr=1.3e-04
Epoch 0380 â”‚ loss=0.9613 â”‚ lr=1.3e-04
Epoch 0385 â”‚ loss=0.9575 â”‚ lr=1.3e-04
Epoch 0390 â”‚ loss=0.9463 â”‚ lr=1.3e-04
Epoch 0395 â”‚ loss=0.9482 â”‚ lr=1.3e-04
Epoch 0400 â”‚ loss=0.9683 â”‚ lr=6.3e-05
Epoch 0405 â”‚ loss=0.9528 â”‚ lr=6.3e-05
Epoch 0410 â”‚ loss=0.9542 â”‚ lr=6.3e-05
Epoch 0415 â”‚ loss=0.9553 â”‚ lr=6.3e-05
Epoch 0420 â”‚ loss=0.9570 â”‚ lr=6.3e-05
Epoch 0425 â”‚ loss=0.9456 â”‚ lr=6.3e-05
Epoch 0430 â”‚ loss=0.9386 â”‚ lr=6.3e-05
Epoch 0435 â”‚ loss=0.9560 â”‚ lr=6.3e-05
Epoch 0440 â”‚ loss=0.9461 â”‚ lr=6.3e-05
Epoch 0445 â”‚ loss=0.9647 â”‚ lr=6.3e-05
Epoch 0450 â”‚ loss=0.9360 â”‚ lr=6.3e-05
Epoch 0455 â”‚ loss=0.9492 â”‚ lr=6.3e-05
Epoch 0460 â”‚ loss=0.9485 â”‚ lr=6.3e-05
Epoch 0465 â”‚ loss=0.9523 â”‚ lr=6.3e-05
Epoch 0470 â”‚ loss=0.9474 â”‚ lr=6.3e-05
Epoch 0475 â”‚ loss=0.9343 â”‚ lr=6.3e-05
Epoch 0480 â”‚ loss=0.9494 â”‚ lr=6.3e-05
Epoch 0485 â”‚ loss=0.9502 â”‚ lr=6.3e-05
Epoch 0490 â”‚ loss=0.9431 â”‚ lr=6.3e-05
Epoch 0495 â”‚ loss=0.9402 â”‚ lr=6.3e-05
Epoch 0500 â”‚ loss=0.9570 â”‚ lr=3.1e-05
âœ…  Model saved to ./experiments/gnn_hd64_ld32_nr5_ep500_b1.0_nf1.0/model.pt
ðŸ“ˆ  Loss curve saved to vgae_loss.png
Novel: 100.00% (Baseline), 100.00% (VGAE)
Unique: 100.00% (Baseline), 100.00% (VGAE)
Novel and Unique: 100.00% (Baseline), 100.00% (VGAE)
Computing node-level statistics...
Computing node-level statistics...
Computing node-level statistics...
