Experiment arguments: {'mode': 'train', 'epochs': 500, 'lr': 0.0005, 'beta': 5, 'neg_factor': 3.0, 'device': 'cuda', 'hidden_dim': 64, 'latent_dim': 32, 'num_enc_MP_rounds': 3, 'dec_layers': 1, 'heads': 4, 'decoder': 'gat', 'checkpoint': './experiments/gat_hd64_ld32_nr3_ep500_nf3.0_dl1/model.pt', 'log_dir': './experiments/gat_hd64_ld32_nr3_ep500_nf3.0_dl1/logs', 'fig_dir': './experiments/gat_hd64_ld32_nr3_ep500_nf3.0_dl1/figures'}
Loaded 171 training hashes.
cuda
Device: cuda
Training VGAE model...
Epoch 0001 â”‚ loss=2.9420 â”‚ recon=1.371 â”‚ kl=2.113 â”‚ Î²=0.02 â”‚ lr=5.0e-04
Epoch 0002 â”‚ loss=1.4951 â”‚ recon=1.414 â”‚ kl=2.331 â”‚ Î²=0.04 â”‚ lr=5.0e-04
Epoch 0003 â”‚ loss=1.5044 â”‚ recon=1.264 â”‚ kl=2.262 â”‚ Î²=0.06 â”‚ lr=5.0e-04
Epoch 0004 â”‚ loss=1.4240 â”‚ recon=1.221 â”‚ kl=2.225 â”‚ Î²=0.08 â”‚ lr=5.0e-04
Epoch 0005 â”‚ loss=1.4162 â”‚ recon=1.325 â”‚ kl=2.038 â”‚ Î²=0.10 â”‚ lr=5.0e-04
Epoch 0006 â”‚ loss=1.4317 â”‚ recon=1.283 â”‚ kl=1.816 â”‚ Î²=0.12 â”‚ lr=5.0e-04
Epoch 0007 â”‚ loss=1.4533 â”‚ recon=1.108 â”‚ kl=1.760 â”‚ Î²=0.14 â”‚ lr=5.0e-04
Epoch 0008 â”‚ loss=1.4769 â”‚ recon=1.142 â”‚ kl=1.728 â”‚ Î²=0.16 â”‚ lr=5.0e-04
Epoch 0009 â”‚ loss=1.4727 â”‚ recon=1.112 â”‚ kl=1.683 â”‚ Î²=0.18 â”‚ lr=5.0e-04
Epoch 0010 â”‚ loss=1.4742 â”‚ recon=1.211 â”‚ kl=1.562 â”‚ Î²=0.20 â”‚ lr=5.0e-04
Epoch 0011 â”‚ loss=1.4675 â”‚ recon=1.152 â”‚ kl=1.603 â”‚ Î²=0.22 â”‚ lr=5.0e-04
Epoch 0012 â”‚ loss=1.4696 â”‚ recon=1.173 â”‚ kl=1.477 â”‚ Î²=0.24 â”‚ lr=5.0e-04
Epoch 0013 â”‚ loss=1.4843 â”‚ recon=1.192 â”‚ kl=1.475 â”‚ Î²=0.26 â”‚ lr=5.0e-04
Epoch 0014 â”‚ loss=1.4899 â”‚ recon=1.179 â”‚ kl=1.374 â”‚ Î²=0.28 â”‚ lr=5.0e-04
Epoch 0015 â”‚ loss=1.5271 â”‚ recon=1.137 â”‚ kl=1.316 â”‚ Î²=0.30 â”‚ lr=5.0e-04
Epoch 0016 â”‚ loss=1.5190 â”‚ recon=1.102 â”‚ kl=1.325 â”‚ Î²=0.32 â”‚ lr=5.0e-04
Epoch 0017 â”‚ loss=1.5575 â”‚ recon=1.141 â”‚ kl=1.341 â”‚ Î²=0.34 â”‚ lr=5.0e-04
Epoch 0018 â”‚ loss=1.5556 â”‚ recon=1.387 â”‚ kl=1.271 â”‚ Î²=0.36 â”‚ lr=5.0e-04
Epoch 0019 â”‚ loss=1.5574 â”‚ recon=1.144 â”‚ kl=1.176 â”‚ Î²=0.38 â”‚ lr=5.0e-04
Epoch 0020 â”‚ loss=1.5713 â”‚ recon=1.167 â”‚ kl=1.208 â”‚ Î²=0.40 â”‚ lr=5.0e-04
Epoch 0021 â”‚ loss=1.5920 â”‚ recon=1.187 â”‚ kl=1.172 â”‚ Î²=0.42 â”‚ lr=5.0e-04
Epoch 0022 â”‚ loss=1.6225 â”‚ recon=1.287 â”‚ kl=1.175 â”‚ Î²=0.44 â”‚ lr=5.0e-04
Epoch 0023 â”‚ loss=1.6285 â”‚ recon=1.048 â”‚ kl=1.046 â”‚ Î²=0.46 â”‚ lr=5.0e-04
Epoch 0024 â”‚ loss=1.6216 â”‚ recon=1.029 â”‚ kl=1.068 â”‚ Î²=0.48 â”‚ lr=5.0e-04
Epoch 0025 â”‚ loss=1.6175 â”‚ recon=1.086 â”‚ kl=0.989 â”‚ Î²=0.50 â”‚ lr=5.0e-04
Epoch 0026 â”‚ loss=1.6418 â”‚ recon=1.084 â”‚ kl=1.035 â”‚ Î²=0.52 â”‚ lr=5.0e-04
Epoch 0027 â”‚ loss=1.6693 â”‚ recon=1.028 â”‚ kl=1.025 â”‚ Î²=0.54 â”‚ lr=5.0e-04
Epoch 0028 â”‚ loss=1.6713 â”‚ recon=1.307 â”‚ kl=0.960 â”‚ Î²=0.56 â”‚ lr=5.0e-04
Epoch 0029 â”‚ loss=1.6805 â”‚ recon=1.183 â”‚ kl=0.961 â”‚ Î²=0.58 â”‚ lr=5.0e-04
Epoch 0030 â”‚ loss=1.7075 â”‚ recon=0.936 â”‚ kl=0.946 â”‚ Î²=0.60 â”‚ lr=5.0e-04
Epoch 0031 â”‚ loss=1.7185 â”‚ recon=1.233 â”‚ kl=0.897 â”‚ Î²=0.62 â”‚ lr=5.0e-04
Epoch 0032 â”‚ loss=1.7063 â”‚ recon=1.028 â”‚ kl=0.884 â”‚ Î²=0.64 â”‚ lr=5.0e-04
Epoch 0033 â”‚ loss=1.7368 â”‚ recon=1.250 â”‚ kl=0.906 â”‚ Î²=0.66 â”‚ lr=5.0e-04
Epoch 0034 â”‚ loss=1.7304 â”‚ recon=0.957 â”‚ kl=0.880 â”‚ Î²=0.68 â”‚ lr=5.0e-04
Epoch 0035 â”‚ loss=1.7367 â”‚ recon=1.022 â”‚ kl=0.856 â”‚ Î²=0.70 â”‚ lr=5.0e-04
Epoch 0036 â”‚ loss=1.7530 â”‚ recon=1.244 â”‚ kl=0.865 â”‚ Î²=0.72 â”‚ lr=5.0e-04
Epoch 0037 â”‚ loss=1.7672 â”‚ recon=1.070 â”‚ kl=0.869 â”‚ Î²=0.74 â”‚ lr=5.0e-04
Epoch 0038 â”‚ loss=1.7874 â”‚ recon=0.786 â”‚ kl=0.859 â”‚ Î²=0.76 â”‚ lr=5.0e-04
Epoch 0039 â”‚ loss=1.8166 â”‚ recon=1.147 â”‚ kl=0.830 â”‚ Î²=0.78 â”‚ lr=5.0e-04
Epoch 0040 â”‚ loss=1.8259 â”‚ recon=1.218 â”‚ kl=0.791 â”‚ Î²=0.80 â”‚ lr=5.0e-04
Epoch 0041 â”‚ loss=1.8149 â”‚ recon=1.141 â”‚ kl=0.814 â”‚ Î²=0.82 â”‚ lr=5.0e-04
Epoch 0042 â”‚ loss=1.8350 â”‚ recon=0.837 â”‚ kl=0.839 â”‚ Î²=0.84 â”‚ lr=5.0e-04
Epoch 0043 â”‚ loss=1.8369 â”‚ recon=1.447 â”‚ kl=0.831 â”‚ Î²=0.86 â”‚ lr=5.0e-04
Epoch 0044 â”‚ loss=1.8408 â”‚ recon=1.162 â”‚ kl=0.788 â”‚ Î²=0.88 â”‚ lr=5.0e-04
Epoch 0045 â”‚ loss=1.8734 â”‚ recon=1.708 â”‚ kl=0.742 â”‚ Î²=0.90 â”‚ lr=5.0e-04
Epoch 0046 â”‚ loss=1.8444 â”‚ recon=1.077 â”‚ kl=0.749 â”‚ Î²=0.92 â”‚ lr=5.0e-04
Epoch 0047 â”‚ loss=1.8982 â”‚ recon=1.027 â”‚ kl=0.762 â”‚ Î²=0.94 â”‚ lr=5.0e-04
Epoch 0048 â”‚ loss=1.8998 â”‚ recon=1.435 â”‚ kl=0.733 â”‚ Î²=0.96 â”‚ lr=5.0e-04
Epoch 0049 â”‚ loss=1.8927 â”‚ recon=1.057 â”‚ kl=0.718 â”‚ Î²=0.98 â”‚ lr=5.0e-04
Epoch 0050 â”‚ loss=1.9063 â”‚ recon=1.086 â”‚ kl=0.748 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0051 â”‚ loss=1.8835 â”‚ recon=1.265 â”‚ kl=0.754 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0052 â”‚ loss=1.8938 â”‚ recon=1.150 â”‚ kl=0.716 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0053 â”‚ loss=1.9377 â”‚ recon=1.077 â”‚ kl=0.738 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0054 â”‚ loss=1.8961 â”‚ recon=1.260 â”‚ kl=0.700 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0055 â”‚ loss=1.8970 â”‚ recon=1.519 â”‚ kl=0.690 â”‚ Î²=1.00 â”‚ lr=5.0e-04
â¹ï¸  Early stopping at epoch 55 (no improvement for 50 epochs).
âœ…  Model saved to ./experiments/gat_hd64_ld32_nr3_ep500_nf3.0_dl1/model.pt
ğŸ“ˆ  Loss curve saved to vgae_loss.png
Novel: 100.00% (Baseline), 100.00% (VGAE)
Unique: 100.00% (Baseline), 98.70% (VGAE)
Novel and Unique: 100.00% (Baseline), 98.70% (VGAE)
Computing node-level statistics...
Computing node-level statistics...
Computing node-level statistics...

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 24864295: <graph_generations> in cluster <dcc> Done

Job <graph_generations> was submitted from host <n-62-20-9> by user <s185927> in cluster <dcc> at Fri May  2 16:02:40 2025
Job was executed on host(s) <8*n-62-20-10>, in queue <gpuv100>, as user <s185927> in cluster <dcc> at Fri May  2 16:02:50 2025
</zhome/e3/3/139772> was used as the home directory.
</zhome/e3/3/139772/Desktop/AML/AML/Module_3/AML-Project-3> was used as the working directory.
Started at Fri May  2 16:02:50 2025
Terminated at Fri May  2 16:05:49 2025
Results reported at Fri May  2 16:05:49 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J graph_generations
#BSUB -q gpuv100
#BSUB -n 8
#BSUB -o logs/%J.out
#BSUB -e logs/%J.err
#BSUB -gpu "num=1:mode=exclusive_process"
#BSUB -W 5:00
#BSUB -R "span[hosts=1]"
#BSUB -R "rusage[mem=5GB]"
# end of BSUB options

module load cuda/11.8

source ~/Desktop/AML/aml_new/bin/activate

python -u src/main.py --mode 'train' --epochs 500 --lr 5e-4 --hidden_dim 64 --latent_dim 32 --num_enc_MP_rounds 3 --decoder gat  --neg_factor 3 --dec_layers 1 --heads 4 



------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   166.00 sec.
    Max Memory :                                 729 MB
    Average Memory :                             648.00 MB
    Total Requested Memory :                     40960.00 MB
    Delta Memory :                               40231.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                11
    Run time :                                   181 sec.
    Turnaround time :                            189 sec.

The output (if any) is above this job summary.



PS:

Read file <logs/24864295.err> for stderr output of this job.

