Experiment arguments: {'mode': 'train', 'epochs': 500, 'lr': 0.0005, 'beta': 5, 'neg_factor': 3.0, 'device': 'cuda', 'hidden_dim': 64, 'latent_dim': 32, 'num_enc_MP_rounds': 3, 'dec_layers': 1, 'heads': 4, 'decoder': 'gat', 'checkpoint': './experiments/gat_hd64_ld32_nr3_ep500_nf3.0_dl1/model.pt', 'log_dir': './experiments/gat_hd64_ld32_nr3_ep500_nf3.0_dl1/logs', 'fig_dir': './experiments/gat_hd64_ld32_nr3_ep500_nf3.0_dl1/figures'}
Loaded 171 training hashes.
cuda
Device: cuda
Training VGAE model...
Epoch 0001 â”‚ loss=2.9420 â”‚ recon=1.371 â”‚ kl=2.113 â”‚ Î²=0.02 â”‚ lr=5.0e-04
Epoch 0002 â”‚ loss=1.4951 â”‚ recon=1.414 â”‚ kl=2.331 â”‚ Î²=0.04 â”‚ lr=5.0e-04
Epoch 0003 â”‚ loss=1.5044 â”‚ recon=1.264 â”‚ kl=2.262 â”‚ Î²=0.06 â”‚ lr=5.0e-04
Epoch 0004 â”‚ loss=1.4240 â”‚ recon=1.221 â”‚ kl=2.225 â”‚ Î²=0.08 â”‚ lr=5.0e-04
Epoch 0005 â”‚ loss=1.4162 â”‚ recon=1.325 â”‚ kl=2.038 â”‚ Î²=0.10 â”‚ lr=5.0e-04
Epoch 0006 â”‚ loss=1.4317 â”‚ recon=1.283 â”‚ kl=1.816 â”‚ Î²=0.12 â”‚ lr=5.0e-04
Epoch 0007 â”‚ loss=1.4533 â”‚ recon=1.108 â”‚ kl=1.760 â”‚ Î²=0.14 â”‚ lr=5.0e-04
Epoch 0008 â”‚ loss=1.4769 â”‚ recon=1.142 â”‚ kl=1.728 â”‚ Î²=0.16 â”‚ lr=5.0e-04
Epoch 0009 â”‚ loss=1.4727 â”‚ recon=1.112 â”‚ kl=1.683 â”‚ Î²=0.18 â”‚ lr=5.0e-04
Epoch 0010 â”‚ loss=1.4742 â”‚ recon=1.211 â”‚ kl=1.562 â”‚ Î²=0.20 â”‚ lr=5.0e-04
Epoch 0011 â”‚ loss=1.4675 â”‚ recon=1.152 â”‚ kl=1.603 â”‚ Î²=0.22 â”‚ lr=5.0e-04
Epoch 0012 â”‚ loss=1.4696 â”‚ recon=1.173 â”‚ kl=1.477 â”‚ Î²=0.24 â”‚ lr=5.0e-04
Epoch 0013 â”‚ loss=1.4843 â”‚ recon=1.192 â”‚ kl=1.475 â”‚ Î²=0.26 â”‚ lr=5.0e-04
Epoch 0014 â”‚ loss=1.4899 â”‚ recon=1.179 â”‚ kl=1.374 â”‚ Î²=0.28 â”‚ lr=5.0e-04
Epoch 0015 â”‚ loss=1.5272 â”‚ recon=1.137 â”‚ kl=1.317 â”‚ Î²=0.30 â”‚ lr=5.0e-04
Epoch 0016 â”‚ loss=1.5191 â”‚ recon=1.103 â”‚ kl=1.325 â”‚ Î²=0.32 â”‚ lr=5.0e-04
Epoch 0017 â”‚ loss=1.5577 â”‚ recon=1.140 â”‚ kl=1.341 â”‚ Î²=0.34 â”‚ lr=5.0e-04
Epoch 0018 â”‚ loss=1.5564 â”‚ recon=1.388 â”‚ kl=1.275 â”‚ Î²=0.36 â”‚ lr=5.0e-04
Epoch 0019 â”‚ loss=1.5571 â”‚ recon=1.148 â”‚ kl=1.174 â”‚ Î²=0.38 â”‚ lr=5.0e-04
Epoch 0020 â”‚ loss=1.5734 â”‚ recon=1.168 â”‚ kl=1.205 â”‚ Î²=0.40 â”‚ lr=5.0e-04
Epoch 0021 â”‚ loss=1.5936 â”‚ recon=1.189 â”‚ kl=1.168 â”‚ Î²=0.42 â”‚ lr=5.0e-04
Epoch 0022 â”‚ loss=1.6248 â”‚ recon=1.284 â”‚ kl=1.174 â”‚ Î²=0.44 â”‚ lr=5.0e-04
Epoch 0023 â”‚ loss=1.6303 â”‚ recon=1.048 â”‚ kl=1.042 â”‚ Î²=0.46 â”‚ lr=5.0e-04
Epoch 0024 â”‚ loss=1.6230 â”‚ recon=1.021 â”‚ kl=1.071 â”‚ Î²=0.48 â”‚ lr=5.0e-04
Epoch 0025 â”‚ loss=1.6188 â”‚ recon=1.091 â”‚ kl=0.987 â”‚ Î²=0.50 â”‚ lr=5.0e-04
Epoch 0026 â”‚ loss=1.6409 â”‚ recon=1.082 â”‚ kl=1.039 â”‚ Î²=0.52 â”‚ lr=5.0e-04
Epoch 0027 â”‚ loss=1.6690 â”‚ recon=1.023 â”‚ kl=1.028 â”‚ Î²=0.54 â”‚ lr=5.0e-04
Epoch 0028 â”‚ loss=1.6714 â”‚ recon=1.305 â”‚ kl=0.965 â”‚ Î²=0.56 â”‚ lr=5.0e-04
Epoch 0029 â”‚ loss=1.6809 â”‚ recon=1.176 â”‚ kl=0.965 â”‚ Î²=0.58 â”‚ lr=5.0e-04
Epoch 0030 â”‚ loss=1.7079 â”‚ recon=0.953 â”‚ kl=0.941 â”‚ Î²=0.60 â”‚ lr=5.0e-04
Epoch 0031 â”‚ loss=1.7181 â”‚ recon=1.231 â”‚ kl=0.894 â”‚ Î²=0.62 â”‚ lr=5.0e-04
Epoch 0032 â”‚ loss=1.7067 â”‚ recon=1.022 â”‚ kl=0.892 â”‚ Î²=0.64 â”‚ lr=5.0e-04
Epoch 0033 â”‚ loss=1.7349 â”‚ recon=1.254 â”‚ kl=0.893 â”‚ Î²=0.66 â”‚ lr=5.0e-04
Epoch 0034 â”‚ loss=1.7305 â”‚ recon=0.954 â”‚ kl=0.873 â”‚ Î²=0.68 â”‚ lr=5.0e-04
Epoch 0035 â”‚ loss=1.7380 â”‚ recon=1.021 â”‚ kl=0.855 â”‚ Î²=0.70 â”‚ lr=5.0e-04
Epoch 0036 â”‚ loss=1.7546 â”‚ recon=1.244 â”‚ kl=0.867 â”‚ Î²=0.72 â”‚ lr=5.0e-04
Epoch 0037 â”‚ loss=1.7668 â”‚ recon=1.079 â”‚ kl=0.865 â”‚ Î²=0.74 â”‚ lr=5.0e-04
Epoch 0038 â”‚ loss=1.7880 â”‚ recon=0.784 â”‚ kl=0.853 â”‚ Î²=0.76 â”‚ lr=5.0e-04
Epoch 0039 â”‚ loss=1.8158 â”‚ recon=1.139 â”‚ kl=0.825 â”‚ Î²=0.78 â”‚ lr=5.0e-04
Epoch 0040 â”‚ loss=1.8260 â”‚ recon=1.227 â”‚ kl=0.788 â”‚ Î²=0.80 â”‚ lr=5.0e-04
Epoch 0041 â”‚ loss=1.8161 â”‚ recon=1.147 â”‚ kl=0.805 â”‚ Î²=0.82 â”‚ lr=5.0e-04
Epoch 0042 â”‚ loss=1.8345 â”‚ recon=0.848 â”‚ kl=0.829 â”‚ Î²=0.84 â”‚ lr=5.0e-04
Epoch 0043 â”‚ loss=1.8387 â”‚ recon=1.460 â”‚ kl=0.819 â”‚ Î²=0.86 â”‚ lr=5.0e-04
Epoch 0044 â”‚ loss=1.8394 â”‚ recon=1.173 â”‚ kl=0.786 â”‚ Î²=0.88 â”‚ lr=5.0e-04
Epoch 0045 â”‚ loss=1.8734 â”‚ recon=1.710 â”‚ kl=0.745 â”‚ Î²=0.90 â”‚ lr=5.0e-04
Epoch 0046 â”‚ loss=1.8428 â”‚ recon=1.091 â”‚ kl=0.748 â”‚ Î²=0.92 â”‚ lr=5.0e-04
Epoch 0047 â”‚ loss=1.8981 â”‚ recon=1.027 â”‚ kl=0.762 â”‚ Î²=0.94 â”‚ lr=5.0e-04
Epoch 0048 â”‚ loss=1.8984 â”‚ recon=1.414 â”‚ kl=0.727 â”‚ Î²=0.96 â”‚ lr=5.0e-04
Epoch 0049 â”‚ loss=1.8939 â”‚ recon=1.067 â”‚ kl=0.713 â”‚ Î²=0.98 â”‚ lr=5.0e-04
Epoch 0050 â”‚ loss=1.9067 â”‚ recon=1.081 â”‚ kl=0.741 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0051 â”‚ loss=1.8850 â”‚ recon=1.275 â”‚ kl=0.752 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0052 â”‚ loss=1.8910 â”‚ recon=1.092 â”‚ kl=0.720 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0053 â”‚ loss=1.9384 â”‚ recon=1.102 â”‚ kl=0.733 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0054 â”‚ loss=1.8967 â”‚ recon=1.249 â”‚ kl=0.701 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0055 â”‚ loss=1.8966 â”‚ recon=1.523 â”‚ kl=0.693 â”‚ Î²=1.00 â”‚ lr=5.0e-04
Epoch 0056 â”‚ loss=1.8759 â”‚ recon=1.538 â”‚ kl=0.692 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0057 â”‚ loss=1.8905 â”‚ recon=1.179 â”‚ kl=0.693 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0058 â”‚ loss=1.8581 â”‚ recon=1.432 â”‚ kl=0.716 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0059 â”‚ loss=1.8756 â”‚ recon=1.000 â”‚ kl=0.697 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0060 â”‚ loss=1.8703 â”‚ recon=1.521 â”‚ kl=0.681 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0061 â”‚ loss=1.8564 â”‚ recon=1.103 â”‚ kl=0.679 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0062 â”‚ loss=1.8574 â”‚ recon=1.737 â”‚ kl=0.692 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0063 â”‚ loss=1.8791 â”‚ recon=1.492 â”‚ kl=0.686 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0064 â”‚ loss=1.8861 â”‚ recon=0.924 â”‚ kl=0.693 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0065 â”‚ loss=1.8897 â”‚ recon=0.991 â”‚ kl=0.693 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0066 â”‚ loss=1.8512 â”‚ recon=0.973 â”‚ kl=0.699 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0067 â”‚ loss=1.8563 â”‚ recon=0.924 â”‚ kl=0.693 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0068 â”‚ loss=1.8701 â”‚ recon=1.234 â”‚ kl=0.717 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0069 â”‚ loss=1.8778 â”‚ recon=1.205 â”‚ kl=0.720 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0070 â”‚ loss=1.8307 â”‚ recon=1.236 â”‚ kl=0.663 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0071 â”‚ loss=1.8301 â”‚ recon=0.802 â”‚ kl=0.688 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0072 â”‚ loss=1.8567 â”‚ recon=1.038 â”‚ kl=0.711 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0073 â”‚ loss=1.8252 â”‚ recon=1.416 â”‚ kl=0.680 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0074 â”‚ loss=1.8635 â”‚ recon=0.999 â”‚ kl=0.699 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0075 â”‚ loss=1.8369 â”‚ recon=1.260 â”‚ kl=0.686 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0076 â”‚ loss=1.8348 â”‚ recon=1.210 â”‚ kl=0.697 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0077 â”‚ loss=1.8425 â”‚ recon=1.248 â”‚ kl=0.670 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0078 â”‚ loss=1.8282 â”‚ recon=1.044 â”‚ kl=0.705 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0079 â”‚ loss=1.8475 â”‚ recon=1.269 â”‚ kl=0.666 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0080 â”‚ loss=1.8828 â”‚ recon=1.296 â”‚ kl=0.742 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0081 â”‚ loss=1.8574 â”‚ recon=0.935 â”‚ kl=0.743 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0082 â”‚ loss=1.8336 â”‚ recon=1.227 â”‚ kl=0.686 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0083 â”‚ loss=1.8314 â”‚ recon=1.150 â”‚ kl=0.655 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0084 â”‚ loss=1.8281 â”‚ recon=0.889 â”‚ kl=0.667 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0085 â”‚ loss=1.8403 â”‚ recon=0.927 â”‚ kl=0.683 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0086 â”‚ loss=1.8054 â”‚ recon=1.155 â”‚ kl=0.685 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0087 â”‚ loss=1.8208 â”‚ recon=1.007 â”‚ kl=0.642 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0088 â”‚ loss=1.8175 â”‚ recon=1.086 â”‚ kl=0.702 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0089 â”‚ loss=1.8227 â”‚ recon=0.934 â”‚ kl=0.688 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0090 â”‚ loss=1.8242 â”‚ recon=1.089 â”‚ kl=0.667 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0091 â”‚ loss=1.8280 â”‚ recon=1.213 â”‚ kl=0.694 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0092 â”‚ loss=1.8167 â”‚ recon=1.133 â”‚ kl=0.673 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0093 â”‚ loss=1.8308 â”‚ recon=1.229 â”‚ kl=0.667 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0094 â”‚ loss=1.8206 â”‚ recon=1.031 â”‚ kl=0.675 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0095 â”‚ loss=1.8200 â”‚ recon=1.468 â”‚ kl=0.656 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0096 â”‚ loss=1.8484 â”‚ recon=1.206 â”‚ kl=0.694 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0097 â”‚ loss=1.8314 â”‚ recon=1.125 â”‚ kl=0.696 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0098 â”‚ loss=1.8272 â”‚ recon=0.710 â”‚ kl=0.699 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0099 â”‚ loss=1.8206 â”‚ recon=1.399 â”‚ kl=0.692 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0100 â”‚ loss=1.8280 â”‚ recon=0.796 â”‚ kl=0.666 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0101 â”‚ loss=1.8104 â”‚ recon=1.188 â”‚ kl=0.687 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0102 â”‚ loss=1.8007 â”‚ recon=1.359 â”‚ kl=0.669 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0103 â”‚ loss=1.8184 â”‚ recon=1.189 â”‚ kl=0.672 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0104 â”‚ loss=1.8016 â”‚ recon=1.035 â”‚ kl=0.676 â”‚ Î²=1.00 â”‚ lr=2.5e-04
Epoch 0105 â”‚ loss=1.8079 â”‚ recon=1.298 â”‚ kl=0.703 â”‚ Î²=1.00 â”‚ lr=2.5e-04
â¹ï¸  Early stopping at epoch 105 (no improvement for 100 epochs).
âœ…  Model saved to ./experiments/gat_hd64_ld32_nr3_ep500_nf3.0_dl1/model.pt
ğŸ“ˆ  Loss curve saved to vgae_loss.png
Novel: 100.00% (Baseline), 100.00% (VGAE)
Unique: 100.00% (Baseline), 99.40% (VGAE)
Novel and Unique: 100.00% (Baseline), 99.40% (VGAE)
Computing node-level statistics...
Computing node-level statistics...
Computing node-level statistics...

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 24864380: <graph_generations> in cluster <dcc> Done

Job <graph_generations> was submitted from host <n-62-20-9> by user <s185927> in cluster <dcc> at Fri May  2 16:13:07 2025
Job was executed on host(s) <8*n-62-20-2>, in queue <gpuv100>, as user <s185927> in cluster <dcc> at Fri May  2 16:20:00 2025
</zhome/e3/3/139772> was used as the home directory.
</zhome/e3/3/139772/Desktop/AML/AML/Module_3/AML-Project-3> was used as the working directory.
Started at Fri May  2 16:20:00 2025
Terminated at Fri May  2 16:24:54 2025
Results reported at Fri May  2 16:24:54 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J graph_generations
#BSUB -q gpuv100
#BSUB -n 8
#BSUB -o logs/%J.out
#BSUB -e logs/%J.err
#BSUB -gpu "num=1:mode=exclusive_process"
#BSUB -W 5:00
#BSUB -R "span[hosts=1]"
#BSUB -R "rusage[mem=5GB]"
# end of BSUB options

module load cuda/11.8

source ~/Desktop/AML/aml_new/bin/activate

python -u src/main.py --mode 'train' --epochs 500 --lr 5e-4 --hidden_dim 64 --latent_dim 32 --num_enc_MP_rounds 3 --decoder gat  --neg_factor 3 --dec_layers 1 --heads 4 



------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   282.31 sec.
    Max Memory :                                 694 MB
    Average Memory :                             618.50 MB
    Total Requested Memory :                     40960.00 MB
    Delta Memory :                               40266.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   295 sec.
    Turnaround time :                            707 sec.

The output (if any) is above this job summary.



PS:

Read file <logs/24864380.err> for stderr output of this job.

