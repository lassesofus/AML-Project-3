Experiment arguments: {'mode': 'train', 'epochs': 500, 'lr': 0.0005, 'beta': 5, 'neg_factor': 3.0, 'device': 'cuda', 'hidden_dim': 64, 'latent_dim': 32, 'num_enc_MP_rounds': 3, 'dec_layers': 1, 'heads': 4, 'decoder': 'gat', 'checkpoint': './experiments/gat_hd64_ld32_nr3_ep500_nf3.0_dl1/model.pt', 'log_dir': './experiments/gat_hd64_ld32_nr3_ep500_nf3.0_dl1/logs', 'fig_dir': './experiments/gat_hd64_ld32_nr3_ep500_nf3.0_dl1/figures'}
Loaded 171 training hashes.
cuda
Device: cuda
Training VGAE model...
Epoch 0001 │ loss=2.9420 │ recon=1.371 │ kl=2.113 │ β=0.02 │ lr=5.0e-04
Epoch 0002 │ loss=1.4951 │ recon=1.414 │ kl=2.331 │ β=0.04 │ lr=5.0e-04
Epoch 0003 │ loss=1.5044 │ recon=1.264 │ kl=2.262 │ β=0.06 │ lr=5.0e-04
Epoch 0004 │ loss=1.4240 │ recon=1.221 │ kl=2.225 │ β=0.08 │ lr=5.0e-04
Epoch 0005 │ loss=1.4162 │ recon=1.325 │ kl=2.038 │ β=0.10 │ lr=5.0e-04
Epoch 0006 │ loss=1.4317 │ recon=1.283 │ kl=1.816 │ β=0.12 │ lr=5.0e-04
Epoch 0007 │ loss=1.4533 │ recon=1.108 │ kl=1.760 │ β=0.14 │ lr=5.0e-04
Epoch 0008 │ loss=1.4769 │ recon=1.142 │ kl=1.728 │ β=0.16 │ lr=5.0e-04
Epoch 0009 │ loss=1.4727 │ recon=1.112 │ kl=1.683 │ β=0.18 │ lr=5.0e-04
Epoch 0010 │ loss=1.4742 │ recon=1.211 │ kl=1.562 │ β=0.20 │ lr=5.0e-04
Epoch 0011 │ loss=1.4675 │ recon=1.152 │ kl=1.603 │ β=0.22 │ lr=5.0e-04
Epoch 0012 │ loss=1.4696 │ recon=1.173 │ kl=1.477 │ β=0.24 │ lr=5.0e-04
Epoch 0013 │ loss=1.4843 │ recon=1.192 │ kl=1.475 │ β=0.26 │ lr=5.0e-04
Epoch 0014 │ loss=1.4899 │ recon=1.179 │ kl=1.374 │ β=0.28 │ lr=5.0e-04
Epoch 0015 │ loss=1.5272 │ recon=1.137 │ kl=1.317 │ β=0.30 │ lr=5.0e-04
Epoch 0016 │ loss=1.5191 │ recon=1.103 │ kl=1.325 │ β=0.32 │ lr=5.0e-04
Epoch 0017 │ loss=1.5577 │ recon=1.140 │ kl=1.341 │ β=0.34 │ lr=5.0e-04
Epoch 0018 │ loss=1.5564 │ recon=1.388 │ kl=1.275 │ β=0.36 │ lr=5.0e-04
Epoch 0019 │ loss=1.5571 │ recon=1.148 │ kl=1.174 │ β=0.38 │ lr=5.0e-04
Epoch 0020 │ loss=1.5734 │ recon=1.168 │ kl=1.205 │ β=0.40 │ lr=5.0e-04
Epoch 0021 │ loss=1.5936 │ recon=1.189 │ kl=1.168 │ β=0.42 │ lr=5.0e-04
Epoch 0022 │ loss=1.6248 │ recon=1.284 │ kl=1.174 │ β=0.44 │ lr=5.0e-04
Epoch 0023 │ loss=1.6303 │ recon=1.048 │ kl=1.042 │ β=0.46 │ lr=5.0e-04
Epoch 0024 │ loss=1.6230 │ recon=1.021 │ kl=1.071 │ β=0.48 │ lr=5.0e-04
Epoch 0025 │ loss=1.6188 │ recon=1.091 │ kl=0.987 │ β=0.50 │ lr=5.0e-04
Epoch 0026 │ loss=1.6409 │ recon=1.082 │ kl=1.039 │ β=0.52 │ lr=5.0e-04
Epoch 0027 │ loss=1.6690 │ recon=1.023 │ kl=1.028 │ β=0.54 │ lr=5.0e-04
Epoch 0028 │ loss=1.6714 │ recon=1.305 │ kl=0.965 │ β=0.56 │ lr=5.0e-04
Epoch 0029 │ loss=1.6809 │ recon=1.176 │ kl=0.965 │ β=0.58 │ lr=5.0e-04
Epoch 0030 │ loss=1.7079 │ recon=0.953 │ kl=0.941 │ β=0.60 │ lr=5.0e-04
Epoch 0031 │ loss=1.7181 │ recon=1.231 │ kl=0.894 │ β=0.62 │ lr=5.0e-04
Epoch 0032 │ loss=1.7067 │ recon=1.022 │ kl=0.892 │ β=0.64 │ lr=5.0e-04
Epoch 0033 │ loss=1.7349 │ recon=1.254 │ kl=0.893 │ β=0.66 │ lr=5.0e-04
Epoch 0034 │ loss=1.7305 │ recon=0.954 │ kl=0.873 │ β=0.68 │ lr=5.0e-04
Epoch 0035 │ loss=1.7380 │ recon=1.021 │ kl=0.855 │ β=0.70 │ lr=5.0e-04
Epoch 0036 │ loss=1.7546 │ recon=1.244 │ kl=0.867 │ β=0.72 │ lr=5.0e-04
Epoch 0037 │ loss=1.7668 │ recon=1.079 │ kl=0.865 │ β=0.74 │ lr=5.0e-04
Epoch 0038 │ loss=1.7880 │ recon=0.784 │ kl=0.853 │ β=0.76 │ lr=5.0e-04
Epoch 0039 │ loss=1.8158 │ recon=1.139 │ kl=0.825 │ β=0.78 │ lr=5.0e-04
Epoch 0040 │ loss=1.8260 │ recon=1.227 │ kl=0.788 │ β=0.80 │ lr=5.0e-04
Epoch 0041 │ loss=1.8161 │ recon=1.147 │ kl=0.805 │ β=0.82 │ lr=5.0e-04
Epoch 0042 │ loss=1.8345 │ recon=0.848 │ kl=0.829 │ β=0.84 │ lr=5.0e-04
Epoch 0043 │ loss=1.8387 │ recon=1.460 │ kl=0.819 │ β=0.86 │ lr=5.0e-04
Epoch 0044 │ loss=1.8394 │ recon=1.173 │ kl=0.786 │ β=0.88 │ lr=5.0e-04
Epoch 0045 │ loss=1.8734 │ recon=1.710 │ kl=0.745 │ β=0.90 │ lr=5.0e-04
Epoch 0046 │ loss=1.8428 │ recon=1.091 │ kl=0.748 │ β=0.92 │ lr=5.0e-04
Epoch 0047 │ loss=1.8981 │ recon=1.027 │ kl=0.762 │ β=0.94 │ lr=5.0e-04
Epoch 0048 │ loss=1.8984 │ recon=1.414 │ kl=0.727 │ β=0.96 │ lr=5.0e-04
Epoch 0049 │ loss=1.8939 │ recon=1.067 │ kl=0.713 │ β=0.98 │ lr=5.0e-04
Epoch 0050 │ loss=1.9067 │ recon=1.081 │ kl=0.741 │ β=1.00 │ lr=5.0e-04
Epoch 0051 │ loss=1.8850 │ recon=1.275 │ kl=0.752 │ β=1.00 │ lr=5.0e-04
Epoch 0052 │ loss=1.8910 │ recon=1.092 │ kl=0.720 │ β=1.00 │ lr=5.0e-04
Epoch 0053 │ loss=1.9384 │ recon=1.102 │ kl=0.733 │ β=1.00 │ lr=5.0e-04
Epoch 0054 │ loss=1.8967 │ recon=1.249 │ kl=0.701 │ β=1.00 │ lr=5.0e-04
Epoch 0055 │ loss=1.8966 │ recon=1.523 │ kl=0.693 │ β=1.00 │ lr=5.0e-04
Epoch 0056 │ loss=1.8759 │ recon=1.538 │ kl=0.692 │ β=1.00 │ lr=2.5e-04
Epoch 0057 │ loss=1.8905 │ recon=1.179 │ kl=0.693 │ β=1.00 │ lr=2.5e-04
Epoch 0058 │ loss=1.8581 │ recon=1.432 │ kl=0.716 │ β=1.00 │ lr=2.5e-04
Epoch 0059 │ loss=1.8756 │ recon=1.000 │ kl=0.697 │ β=1.00 │ lr=2.5e-04
Epoch 0060 │ loss=1.8703 │ recon=1.521 │ kl=0.681 │ β=1.00 │ lr=2.5e-04
Epoch 0061 │ loss=1.8564 │ recon=1.103 │ kl=0.679 │ β=1.00 │ lr=2.5e-04
Epoch 0062 │ loss=1.8574 │ recon=1.737 │ kl=0.692 │ β=1.00 │ lr=2.5e-04
Epoch 0063 │ loss=1.8791 │ recon=1.492 │ kl=0.686 │ β=1.00 │ lr=2.5e-04
Epoch 0064 │ loss=1.8861 │ recon=0.924 │ kl=0.693 │ β=1.00 │ lr=2.5e-04
Epoch 0065 │ loss=1.8897 │ recon=0.991 │ kl=0.693 │ β=1.00 │ lr=2.5e-04
Epoch 0066 │ loss=1.8512 │ recon=0.973 │ kl=0.699 │ β=1.00 │ lr=2.5e-04
Epoch 0067 │ loss=1.8563 │ recon=0.924 │ kl=0.693 │ β=1.00 │ lr=2.5e-04
Epoch 0068 │ loss=1.8701 │ recon=1.234 │ kl=0.717 │ β=1.00 │ lr=2.5e-04
Epoch 0069 │ loss=1.8778 │ recon=1.205 │ kl=0.720 │ β=1.00 │ lr=2.5e-04
Epoch 0070 │ loss=1.8307 │ recon=1.236 │ kl=0.663 │ β=1.00 │ lr=2.5e-04
Epoch 0071 │ loss=1.8301 │ recon=0.802 │ kl=0.688 │ β=1.00 │ lr=2.5e-04
Epoch 0072 │ loss=1.8567 │ recon=1.038 │ kl=0.711 │ β=1.00 │ lr=2.5e-04
Epoch 0073 │ loss=1.8252 │ recon=1.416 │ kl=0.680 │ β=1.00 │ lr=2.5e-04
Epoch 0074 │ loss=1.8635 │ recon=0.999 │ kl=0.699 │ β=1.00 │ lr=2.5e-04
Epoch 0075 │ loss=1.8369 │ recon=1.260 │ kl=0.686 │ β=1.00 │ lr=2.5e-04
Epoch 0076 │ loss=1.8348 │ recon=1.210 │ kl=0.697 │ β=1.00 │ lr=2.5e-04
Epoch 0077 │ loss=1.8425 │ recon=1.248 │ kl=0.670 │ β=1.00 │ lr=2.5e-04
Epoch 0078 │ loss=1.8282 │ recon=1.044 │ kl=0.705 │ β=1.00 │ lr=2.5e-04
Epoch 0079 │ loss=1.8475 │ recon=1.269 │ kl=0.666 │ β=1.00 │ lr=2.5e-04
Epoch 0080 │ loss=1.8828 │ recon=1.296 │ kl=0.742 │ β=1.00 │ lr=2.5e-04
Epoch 0081 │ loss=1.8574 │ recon=0.935 │ kl=0.743 │ β=1.00 │ lr=2.5e-04
Epoch 0082 │ loss=1.8336 │ recon=1.227 │ kl=0.686 │ β=1.00 │ lr=2.5e-04
Epoch 0083 │ loss=1.8314 │ recon=1.150 │ kl=0.655 │ β=1.00 │ lr=2.5e-04
Epoch 0084 │ loss=1.8281 │ recon=0.889 │ kl=0.667 │ β=1.00 │ lr=2.5e-04
Epoch 0085 │ loss=1.8403 │ recon=0.927 │ kl=0.683 │ β=1.00 │ lr=2.5e-04
Epoch 0086 │ loss=1.8054 │ recon=1.155 │ kl=0.685 │ β=1.00 │ lr=2.5e-04
Epoch 0087 │ loss=1.8208 │ recon=1.007 │ kl=0.642 │ β=1.00 │ lr=2.5e-04
Epoch 0088 │ loss=1.8175 │ recon=1.086 │ kl=0.702 │ β=1.00 │ lr=2.5e-04
Epoch 0089 │ loss=1.8227 │ recon=0.934 │ kl=0.688 │ β=1.00 │ lr=2.5e-04
Epoch 0090 │ loss=1.8242 │ recon=1.089 │ kl=0.667 │ β=1.00 │ lr=2.5e-04
Epoch 0091 │ loss=1.8280 │ recon=1.213 │ kl=0.694 │ β=1.00 │ lr=2.5e-04
Epoch 0092 │ loss=1.8167 │ recon=1.133 │ kl=0.673 │ β=1.00 │ lr=2.5e-04
Epoch 0093 │ loss=1.8308 │ recon=1.229 │ kl=0.667 │ β=1.00 │ lr=2.5e-04
Epoch 0094 │ loss=1.8206 │ recon=1.031 │ kl=0.675 │ β=1.00 │ lr=2.5e-04
Epoch 0095 │ loss=1.8200 │ recon=1.468 │ kl=0.656 │ β=1.00 │ lr=2.5e-04
Epoch 0096 │ loss=1.8484 │ recon=1.206 │ kl=0.694 │ β=1.00 │ lr=2.5e-04
Epoch 0097 │ loss=1.8314 │ recon=1.125 │ kl=0.696 │ β=1.00 │ lr=2.5e-04
Epoch 0098 │ loss=1.8272 │ recon=0.710 │ kl=0.699 │ β=1.00 │ lr=2.5e-04
Epoch 0099 │ loss=1.8206 │ recon=1.399 │ kl=0.692 │ β=1.00 │ lr=2.5e-04
Epoch 0100 │ loss=1.8280 │ recon=0.796 │ kl=0.666 │ β=1.00 │ lr=2.5e-04
Epoch 0101 │ loss=1.8104 │ recon=1.188 │ kl=0.687 │ β=1.00 │ lr=2.5e-04
Epoch 0102 │ loss=1.8007 │ recon=1.359 │ kl=0.669 │ β=1.00 │ lr=2.5e-04
Epoch 0103 │ loss=1.8184 │ recon=1.189 │ kl=0.672 │ β=1.00 │ lr=2.5e-04
Epoch 0104 │ loss=1.8016 │ recon=1.035 │ kl=0.676 │ β=1.00 │ lr=2.5e-04
Epoch 0105 │ loss=1.8079 │ recon=1.298 │ kl=0.703 │ β=1.00 │ lr=2.5e-04
⏹️  Early stopping at epoch 105 (no improvement for 100 epochs).
✅  Model saved to ./experiments/gat_hd64_ld32_nr3_ep500_nf3.0_dl1/model.pt
📈  Loss curve saved to vgae_loss.png
Novel: 100.00% (Baseline), 100.00% (VGAE)
Unique: 100.00% (Baseline), 99.40% (VGAE)
Novel and Unique: 100.00% (Baseline), 99.40% (VGAE)
Computing node-level statistics...
Computing node-level statistics...
Computing node-level statistics...

------------------------------------------------------------
Sender: LSF System <lsfadmin@hpc.dtu.dk>
Subject: Job 24864380: <graph_generations> in cluster <dcc> Done

Job <graph_generations> was submitted from host <n-62-20-9> by user <s185927> in cluster <dcc> at Fri May  2 16:13:07 2025
Job was executed on host(s) <8*n-62-20-2>, in queue <gpuv100>, as user <s185927> in cluster <dcc> at Fri May  2 16:20:00 2025
</zhome/e3/3/139772> was used as the home directory.
</zhome/e3/3/139772/Desktop/AML/AML/Module_3/AML-Project-3> was used as the working directory.
Started at Fri May  2 16:20:00 2025
Terminated at Fri May  2 16:24:54 2025
Results reported at Fri May  2 16:24:54 2025

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
#!/bin/bash
#BSUB -J graph_generations
#BSUB -q gpuv100
#BSUB -n 8
#BSUB -o logs/%J.out
#BSUB -e logs/%J.err
#BSUB -gpu "num=1:mode=exclusive_process"
#BSUB -W 5:00
#BSUB -R "span[hosts=1]"
#BSUB -R "rusage[mem=5GB]"
# end of BSUB options

module load cuda/11.8

source ~/Desktop/AML/aml_new/bin/activate

python -u src/main.py --mode 'train' --epochs 500 --lr 5e-4 --hidden_dim 64 --latent_dim 32 --num_enc_MP_rounds 3 --decoder gat  --neg_factor 3 --dec_layers 1 --heads 4 



------------------------------------------------------------

Successfully completed.

Resource usage summary:

    CPU time :                                   282.31 sec.
    Max Memory :                                 694 MB
    Average Memory :                             618.50 MB
    Total Requested Memory :                     40960.00 MB
    Delta Memory :                               40266.00 MB
    Max Swap :                                   -
    Max Processes :                              4
    Max Threads :                                9
    Run time :                                   295 sec.
    Turnaround time :                            707 sec.

The output (if any) is above this job summary.



PS:

Read file <logs/24864380.err> for stderr output of this job.

