Experiment arguments: {'mode': 'train', 'epochs': 500, 'beta': 2.0, 'neg_factor': 2.0, 'device': None, 'hidden_dim': 64, 'latent_dim': 32, 'num_rounds': 3, 'gnn_layers': 1, 'decoder': 'gnn', 'checkpoint': './experiments/gnn_hd64_ld32_nr3_ep500_b2.0_nf2.0/model.pt', 'log_dir': './experiments/gnn_hd64_ld32_nr3_ep500_b2.0_nf2.0/logs', 'fig_dir': './experiments/gnn_hd64_ld32_nr3_ep500_b2.0_nf2.0/figures'}
Loaded 171 training hashes.
Epoch 0001 â”‚ loss=3.4328 â”‚ lr=1.0e-03
Epoch 0005 â”‚ loss=2.7758 â”‚ lr=1.0e-03
Epoch 0010 â”‚ loss=2.5763 â”‚ lr=1.0e-03
Epoch 0015 â”‚ loss=2.4745 â”‚ lr=1.0e-03
Epoch 0020 â”‚ loss=2.4289 â”‚ lr=1.0e-03
Epoch 0025 â”‚ loss=2.3694 â”‚ lr=1.0e-03
Epoch 0030 â”‚ loss=2.4005 â”‚ lr=1.0e-03
Epoch 0035 â”‚ loss=2.3469 â”‚ lr=1.0e-03
Epoch 0040 â”‚ loss=2.3778 â”‚ lr=1.0e-03
Epoch 0045 â”‚ loss=2.3737 â”‚ lr=1.0e-03
Epoch 0050 â”‚ loss=2.3379 â”‚ lr=1.0e-03
Epoch 0055 â”‚ loss=2.3208 â”‚ lr=1.0e-03
Epoch 0060 â”‚ loss=2.3389 â”‚ lr=1.0e-03
Epoch 0065 â”‚ loss=2.3246 â”‚ lr=1.0e-03
Epoch 0070 â”‚ loss=2.2771 â”‚ lr=1.0e-03
Epoch 0075 â”‚ loss=2.2809 â”‚ lr=1.0e-03
Epoch 0080 â”‚ loss=2.3078 â”‚ lr=1.0e-03
Epoch 0085 â”‚ loss=2.2786 â”‚ lr=1.0e-03
Epoch 0090 â”‚ loss=2.2500 â”‚ lr=1.0e-03
Epoch 0095 â”‚ loss=2.2509 â”‚ lr=1.0e-03
Epoch 0100 â”‚ loss=2.2722 â”‚ lr=5.0e-04
Epoch 0105 â”‚ loss=2.1805 â”‚ lr=5.0e-04
Epoch 0110 â”‚ loss=2.2530 â”‚ lr=5.0e-04
Epoch 0115 â”‚ loss=2.2296 â”‚ lr=5.0e-04
Epoch 0120 â”‚ loss=2.2293 â”‚ lr=5.0e-04
Epoch 0125 â”‚ loss=2.2036 â”‚ lr=5.0e-04
Epoch 0130 â”‚ loss=2.2122 â”‚ lr=5.0e-04
Epoch 0135 â”‚ loss=2.2049 â”‚ lr=5.0e-04
Epoch 0140 â”‚ loss=2.2171 â”‚ lr=5.0e-04
Epoch 0145 â”‚ loss=2.2173 â”‚ lr=5.0e-04
Epoch 0150 â”‚ loss=2.1960 â”‚ lr=5.0e-04
Epoch 0155 â”‚ loss=2.2203 â”‚ lr=5.0e-04
Epoch 0160 â”‚ loss=2.2543 â”‚ lr=5.0e-04
Epoch 0165 â”‚ loss=2.2144 â”‚ lr=5.0e-04
Epoch 0170 â”‚ loss=2.2250 â”‚ lr=5.0e-04
Epoch 0175 â”‚ loss=2.1815 â”‚ lr=5.0e-04
Epoch 0180 â”‚ loss=2.2155 â”‚ lr=5.0e-04
Epoch 0185 â”‚ loss=2.2158 â”‚ lr=5.0e-04
Epoch 0190 â”‚ loss=2.1925 â”‚ lr=5.0e-04
Epoch 0195 â”‚ loss=2.2053 â”‚ lr=5.0e-04
Epoch 0200 â”‚ loss=2.1740 â”‚ lr=2.5e-04
Epoch 0205 â”‚ loss=2.2292 â”‚ lr=2.5e-04
Epoch 0210 â”‚ loss=2.1673 â”‚ lr=2.5e-04
Epoch 0215 â”‚ loss=2.2038 â”‚ lr=2.5e-04
Epoch 0220 â”‚ loss=2.1725 â”‚ lr=2.5e-04
Epoch 0225 â”‚ loss=2.1619 â”‚ lr=2.5e-04
Epoch 0230 â”‚ loss=2.1713 â”‚ lr=2.5e-04
Epoch 0235 â”‚ loss=2.2168 â”‚ lr=2.5e-04
Epoch 0240 â”‚ loss=2.1759 â”‚ lr=2.5e-04
Epoch 0245 â”‚ loss=2.1548 â”‚ lr=2.5e-04
Epoch 0250 â”‚ loss=2.1525 â”‚ lr=2.5e-04
Epoch 0255 â”‚ loss=2.1810 â”‚ lr=2.5e-04
Epoch 0260 â”‚ loss=2.1720 â”‚ lr=2.5e-04
Epoch 0265 â”‚ loss=2.1871 â”‚ lr=2.5e-04
Epoch 0270 â”‚ loss=2.1634 â”‚ lr=2.5e-04
Epoch 0275 â”‚ loss=2.1475 â”‚ lr=2.5e-04
Epoch 0280 â”‚ loss=2.1452 â”‚ lr=2.5e-04
Epoch 0285 â”‚ loss=2.1673 â”‚ lr=2.5e-04
Epoch 0290 â”‚ loss=2.1958 â”‚ lr=2.5e-04
Epoch 0295 â”‚ loss=2.1752 â”‚ lr=2.5e-04
Epoch 0300 â”‚ loss=2.2053 â”‚ lr=1.3e-04
Epoch 0305 â”‚ loss=2.1552 â”‚ lr=1.3e-04
Epoch 0310 â”‚ loss=2.1900 â”‚ lr=1.3e-04
Epoch 0315 â”‚ loss=2.1868 â”‚ lr=1.3e-04
Epoch 0320 â”‚ loss=2.1853 â”‚ lr=1.3e-04
Epoch 0325 â”‚ loss=2.1770 â”‚ lr=1.3e-04
Epoch 0330 â”‚ loss=2.1735 â”‚ lr=1.3e-04
Epoch 0335 â”‚ loss=2.1463 â”‚ lr=1.3e-04
Epoch 0340 â”‚ loss=2.2035 â”‚ lr=1.3e-04
Epoch 0345 â”‚ loss=2.1744 â”‚ lr=1.3e-04
Epoch 0350 â”‚ loss=2.2067 â”‚ lr=1.3e-04
Epoch 0355 â”‚ loss=2.1763 â”‚ lr=1.3e-04
Epoch 0360 â”‚ loss=2.1924 â”‚ lr=1.3e-04
Epoch 0365 â”‚ loss=2.1345 â”‚ lr=1.3e-04
Epoch 0370 â”‚ loss=2.1517 â”‚ lr=1.3e-04
Epoch 0375 â”‚ loss=2.1523 â”‚ lr=1.3e-04
Epoch 0380 â”‚ loss=2.1893 â”‚ lr=1.3e-04
Epoch 0385 â”‚ loss=2.1422 â”‚ lr=1.3e-04
Epoch 0390 â”‚ loss=2.1714 â”‚ lr=1.3e-04
Epoch 0395 â”‚ loss=2.1764 â”‚ lr=1.3e-04
Epoch 0400 â”‚ loss=2.1597 â”‚ lr=6.3e-05
Epoch 0405 â”‚ loss=2.1534 â”‚ lr=6.3e-05
Epoch 0410 â”‚ loss=2.1610 â”‚ lr=6.3e-05
Epoch 0415 â”‚ loss=2.1673 â”‚ lr=6.3e-05
Epoch 0420 â”‚ loss=2.2031 â”‚ lr=6.3e-05
Epoch 0425 â”‚ loss=2.1510 â”‚ lr=6.3e-05
Epoch 0430 â”‚ loss=2.1726 â”‚ lr=6.3e-05
Epoch 0435 â”‚ loss=2.1520 â”‚ lr=6.3e-05
Epoch 0440 â”‚ loss=2.1914 â”‚ lr=6.3e-05
Epoch 0445 â”‚ loss=2.2015 â”‚ lr=6.3e-05
Epoch 0450 â”‚ loss=2.1602 â”‚ lr=6.3e-05
Epoch 0455 â”‚ loss=2.1483 â”‚ lr=6.3e-05
Epoch 0460 â”‚ loss=2.1465 â”‚ lr=6.3e-05
Epoch 0465 â”‚ loss=2.1524 â”‚ lr=6.3e-05
Epoch 0470 â”‚ loss=2.1540 â”‚ lr=6.3e-05
Epoch 0475 â”‚ loss=2.1490 â”‚ lr=6.3e-05
Epoch 0480 â”‚ loss=2.1294 â”‚ lr=6.3e-05
Epoch 0485 â”‚ loss=2.1459 â”‚ lr=6.3e-05
Epoch 0490 â”‚ loss=2.1493 â”‚ lr=6.3e-05
Epoch 0495 â”‚ loss=2.1759 â”‚ lr=6.3e-05
Epoch 0500 â”‚ loss=2.1672 â”‚ lr=3.1e-05
âœ…  Model saved to ./experiments/gnn_hd64_ld32_nr3_ep500_b2.0_nf2.0/model.pt
ðŸ“ˆ  Loss curve saved to vgae_loss.png
Novel: 100.00% (Baseline), 100.00% (VGAE)
Unique: 100.00% (Baseline), 100.00% (VGAE)
Novel and Unique: 100.00% (Baseline), 100.00% (VGAE)
Computing node-level statistics...
Computing node-level statistics...
Computing node-level statistics...
