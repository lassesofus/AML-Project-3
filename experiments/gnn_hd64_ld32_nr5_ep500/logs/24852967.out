Loaded 171 training hashes.
Epoch 0001 â”‚ loss=2.4256 â”‚ lr=1.0e-03
Epoch 0005 â”‚ loss=1.5366 â”‚ lr=1.0e-03
Epoch 0010 â”‚ loss=1.4791 â”‚ lr=1.0e-03
Epoch 0015 â”‚ loss=1.4502 â”‚ lr=1.0e-03
Epoch 0020 â”‚ loss=1.4397 â”‚ lr=1.0e-03
Epoch 0025 â”‚ loss=1.4271 â”‚ lr=1.0e-03
Epoch 0030 â”‚ loss=1.4197 â”‚ lr=1.0e-03
Epoch 0035 â”‚ loss=1.4166 â”‚ lr=1.0e-03
Epoch 0040 â”‚ loss=1.4141 â”‚ lr=1.0e-03
Epoch 0045 â”‚ loss=1.4064 â”‚ lr=1.0e-03
Epoch 0050 â”‚ loss=1.4031 â”‚ lr=1.0e-03
Epoch 0055 â”‚ loss=1.2554 â”‚ lr=1.0e-03
Epoch 0060 â”‚ loss=1.2211 â”‚ lr=1.0e-03
Epoch 0065 â”‚ loss=1.2152 â”‚ lr=1.0e-03
Epoch 0070 â”‚ loss=1.2096 â”‚ lr=1.0e-03
Epoch 0075 â”‚ loss=1.1889 â”‚ lr=1.0e-03
Epoch 0080 â”‚ loss=1.1730 â”‚ lr=1.0e-03
Epoch 0085 â”‚ loss=1.1196 â”‚ lr=1.0e-03
Epoch 0090 â”‚ loss=1.1262 â”‚ lr=1.0e-03
Epoch 0095 â”‚ loss=1.1119 â”‚ lr=1.0e-03
Epoch 0100 â”‚ loss=1.1128 â”‚ lr=5.0e-04
Epoch 0105 â”‚ loss=1.1123 â”‚ lr=5.0e-04
Epoch 0110 â”‚ loss=1.0996 â”‚ lr=5.0e-04
Epoch 0115 â”‚ loss=1.1028 â”‚ lr=5.0e-04
Epoch 0120 â”‚ loss=1.1004 â”‚ lr=5.0e-04
Epoch 0125 â”‚ loss=1.1057 â”‚ lr=5.0e-04
Epoch 0130 â”‚ loss=1.1061 â”‚ lr=5.0e-04
Epoch 0135 â”‚ loss=1.1035 â”‚ lr=5.0e-04
Epoch 0140 â”‚ loss=1.1110 â”‚ lr=5.0e-04
Epoch 0145 â”‚ loss=1.0959 â”‚ lr=5.0e-04
Epoch 0150 â”‚ loss=1.1039 â”‚ lr=5.0e-04
Epoch 0155 â”‚ loss=1.0975 â”‚ lr=5.0e-04
Epoch 0160 â”‚ loss=1.0948 â”‚ lr=5.0e-04
Epoch 0165 â”‚ loss=1.1001 â”‚ lr=5.0e-04
Epoch 0170 â”‚ loss=1.0917 â”‚ lr=5.0e-04
Epoch 0175 â”‚ loss=1.0969 â”‚ lr=5.0e-04
Epoch 0180 â”‚ loss=1.1036 â”‚ lr=5.0e-04
Epoch 0185 â”‚ loss=1.1021 â”‚ lr=5.0e-04
Epoch 0190 â”‚ loss=1.0919 â”‚ lr=5.0e-04
Epoch 0195 â”‚ loss=1.1067 â”‚ lr=5.0e-04
Epoch 0200 â”‚ loss=1.1018 â”‚ lr=2.5e-04
Epoch 0205 â”‚ loss=1.0805 â”‚ lr=2.5e-04
Epoch 0210 â”‚ loss=1.0564 â”‚ lr=2.5e-04
Epoch 0215 â”‚ loss=1.0723 â”‚ lr=2.5e-04
Epoch 0220 â”‚ loss=1.0725 â”‚ lr=2.5e-04
Epoch 0225 â”‚ loss=1.0702 â”‚ lr=2.5e-04
Epoch 0230 â”‚ loss=1.0408 â”‚ lr=2.5e-04
Epoch 0235 â”‚ loss=1.0421 â”‚ lr=2.5e-04
Epoch 0240 â”‚ loss=1.0207 â”‚ lr=2.5e-04
Epoch 0245 â”‚ loss=1.0211 â”‚ lr=2.5e-04
Epoch 0250 â”‚ loss=1.0176 â”‚ lr=2.5e-04
Epoch 0255 â”‚ loss=1.0089 â”‚ lr=2.5e-04
Epoch 0260 â”‚ loss=1.0022 â”‚ lr=2.5e-04
Epoch 0265 â”‚ loss=1.0152 â”‚ lr=2.5e-04
Epoch 0270 â”‚ loss=0.9809 â”‚ lr=2.5e-04
Epoch 0275 â”‚ loss=0.9966 â”‚ lr=2.5e-04
Epoch 0280 â”‚ loss=0.9976 â”‚ lr=2.5e-04
Epoch 0285 â”‚ loss=0.9952 â”‚ lr=2.5e-04
Epoch 0290 â”‚ loss=0.9832 â”‚ lr=2.5e-04
Epoch 0295 â”‚ loss=0.9946 â”‚ lr=2.5e-04
Epoch 0300 â”‚ loss=0.9745 â”‚ lr=1.3e-04
Epoch 0305 â”‚ loss=0.9838 â”‚ lr=1.3e-04
Epoch 0310 â”‚ loss=0.9787 â”‚ lr=1.3e-04
Epoch 0315 â”‚ loss=0.9737 â”‚ lr=1.3e-04
Epoch 0320 â”‚ loss=0.9821 â”‚ lr=1.3e-04
Epoch 0325 â”‚ loss=0.9778 â”‚ lr=1.3e-04
Epoch 0330 â”‚ loss=0.9728 â”‚ lr=1.3e-04
Epoch 0335 â”‚ loss=0.9781 â”‚ lr=1.3e-04
Epoch 0340 â”‚ loss=0.9820 â”‚ lr=1.3e-04
Epoch 0345 â”‚ loss=0.9809 â”‚ lr=1.3e-04
Epoch 0350 â”‚ loss=0.9713 â”‚ lr=1.3e-04
Epoch 0355 â”‚ loss=0.9854 â”‚ lr=1.3e-04
Epoch 0360 â”‚ loss=0.9611 â”‚ lr=1.3e-04
Epoch 0365 â”‚ loss=0.9741 â”‚ lr=1.3e-04
Epoch 0370 â”‚ loss=0.9576 â”‚ lr=1.3e-04
Epoch 0375 â”‚ loss=0.9699 â”‚ lr=1.3e-04
Epoch 0380 â”‚ loss=0.9773 â”‚ lr=1.3e-04
Epoch 0385 â”‚ loss=0.9756 â”‚ lr=1.3e-04
Epoch 0390 â”‚ loss=0.9640 â”‚ lr=1.3e-04
Epoch 0395 â”‚ loss=0.9620 â”‚ lr=1.3e-04
Epoch 0400 â”‚ loss=0.9888 â”‚ lr=6.3e-05
Epoch 0405 â”‚ loss=0.9680 â”‚ lr=6.3e-05
Epoch 0410 â”‚ loss=0.9722 â”‚ lr=6.3e-05
Epoch 0415 â”‚ loss=0.9712 â”‚ lr=6.3e-05
Epoch 0420 â”‚ loss=0.9719 â”‚ lr=6.3e-05
Epoch 0425 â”‚ loss=0.9692 â”‚ lr=6.3e-05
Epoch 0430 â”‚ loss=0.9558 â”‚ lr=6.3e-05
Epoch 0435 â”‚ loss=0.9704 â”‚ lr=6.3e-05
Epoch 0440 â”‚ loss=0.9620 â”‚ lr=6.3e-05
Epoch 0445 â”‚ loss=0.9812 â”‚ lr=6.3e-05
Epoch 0450 â”‚ loss=0.9532 â”‚ lr=6.3e-05
Epoch 0455 â”‚ loss=0.9624 â”‚ lr=6.3e-05
Epoch 0460 â”‚ loss=0.9661 â”‚ lr=6.3e-05
Epoch 0465 â”‚ loss=0.9644 â”‚ lr=6.3e-05
Epoch 0470 â”‚ loss=0.9536 â”‚ lr=6.3e-05
Epoch 0475 â”‚ loss=0.9532 â”‚ lr=6.3e-05
Epoch 0480 â”‚ loss=0.9679 â”‚ lr=6.3e-05
Epoch 0485 â”‚ loss=0.9614 â”‚ lr=6.3e-05
Epoch 0490 â”‚ loss=0.9501 â”‚ lr=6.3e-05
Epoch 0495 â”‚ loss=0.9543 â”‚ lr=6.3e-05
Epoch 0500 â”‚ loss=0.9719 â”‚ lr=3.1e-05
âœ…  Model saved to ./experiments/gnn_hd64_ld32_nr5_ep500/model.pt
ðŸ“ˆ  Loss curve saved to vgae_loss.png
Novel: 100.00% (Baseline), 100.00% (VGAE)
Unique: 100.00% (Baseline), 100.00% (VGAE)
Novel and Unique: 100.00% (Baseline), 100.00% (VGAE)
Computing node-level statistics...
Computing node-level statistics...
Computing node-level statistics...
